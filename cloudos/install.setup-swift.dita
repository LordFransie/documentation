<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic7517">
<title>Setup Swift Storage Network (Optional)</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./cloudos/install.setup-swift.md-->
 <!--permalink: /cloudos/install/setup-swift/--></p>
<p>Swift is the OpenStack(R) object/blob storage service for cloud computing. You can use Swift to store lots of data efficiently, safely, and cheaply.</p>
<p>
<b>Important</b>: Swift is an optional service. However, if you want to use it in your cloud, you must manually setup Swift as described 
in this topic <b>before performing</b> the steps in <xref href="../cloudos/install.complete-admin-node-installation.dita" >Complete Admin Node Installation</xref>. 
After that, the network settings in the HP Cloud OS Operational Dashboard are read only. Once the install process is triggered, 
no changes can be made. If you want to make changes, you need to start over by re-installing the Admin Node.</p>
<ul>
<li>
<xref type="section" href="#topic7517/swift-requirements">Swift Requirements</xref>
</li>
<li>
<xref type="section" href="#topic7517/zeroing-disks">Zeroing Disks</xref>
</li>
<li>
<xref type="section" href="#topic7517/setup-network-configuration-for-swift">Setup Network Configuration for Swift</xref>
</li>
<li>
<xref type="section" href="#topic7517/preview-of-swift-deployment-steps">Preview of Swift Deployment Steps</xref>
</li>
<li>
<xref type="section" href="#topic7517/swift-roles">Swift Roles</xref>
</li>
<li>
<xref type="section" href="#topic7517/next-step">Next Step</xref>
</li>
</ul>
<section id="swift-requirements"> <title>Swift Requirements</title>
<p>The Swift service requires the following:</p>
<ul>
<li>
<p>A single controller node, which you'll later assign to the following Swift roles:</p>

<ul>
<li>ring-compute</li>
<li>proxy</li>
<li>dispersion</li>
</ul>
</li>
<li>
<p>Between 2-5 storage nodes. On each storage node, Swift needs a dedicated disk that is independent of the OS disk. 
This dedicated disk must have the first and last megabyte of the disk zeroed out; this is required even for new disks. See the next section, 
<xref type="section" href="#topic7517/zeroing-disks">Zeroing Disks</xref>.</p>
</li>
</ul>
</section>
<section id="zeroing-disks"> <title>Zeroing Disks</title>
<p>To show the start and end megabyte, you can use the following on each Swift storage node:</p>
<p>Show start and end:</p>
<codeblock>
dd if=/dev/sdb bs=1M count=1 | hd <!--A BR tag was used here in the original source.--><required-cleanup remap="nobr">
<ph>size=$(fdisk -l  /dev/sdb | grep Disk | grep bytes | cut -f 5 -d" ") &amp;&amp; echo $size</ph>
</required-cleanup>
<!--A BR tag was used here in the original source.-->
skip=$(python -c "print ($size / (1024*1024)) - 1") &amp;&amp; echo $skip <!--A BR tag was used here in the original source.-->
dd if=/dev/sdb bs=1M skip=$skip count=1 2&gt;/dev/null | hd
</codeblock>
<p>In the start and end data, look for a display of all zeros, similar to this example:</p>
<codeblock>
<required-cleanup remap="nobr">
<ph>00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|</ph>
</required-cleanup>
<!--A BR tag was used here in the original source.-->
* <!--A BR tag was used here in the original source.-->
1+0 records in <!--A BR tag was used here in the original source.-->
1+0 records out
</codeblock>
<p>If the results show something like the following, the disk is not zeroed:</p>
<codeblock>
<required-cleanup remap="nobr">
<ph>000fbe00  a2 a0 d0 eb e5 b9 33 44  87 c0 68 b6 b7 26 99 c7  |......3D..h..&amp;..|</ph>
</required-cleanup>
</codeblock>
<p>To zero out the first and last meg, use this:</p>
<codeblock>
dd if=/dev/zero of=/dev/sdb bs=1M count=1 <!--A BR tag was used here in the original source.--><required-cleanup remap="nobr">
<ph>size=$(fdisk -l  /dev/sdb | grep Disk | grep bytes | cut -f 5 -d" ") &amp;&amp; echo $size <!--A BR tag was used here in the original source.-->
seek=$(python -c "print ($size / (1024*1024)) - 1") &amp;&amp; echo $seek</ph>
</required-cleanup>
<!--A BR tag was used here in the original source.-->
dd if=/dev/zero of=/dev/sdb seek=$seek bs=1M </codeblock>
</section>
<section id="setup-network-configuration-for-swift"> <title>Setup Network Configuration for Swift</title>
<p>Here are the network configuration steps:</p>
<ol>
<li>
<p>Add a new network named "storage"</p>
</li>
<li>
<p>Assign Logical Interface "intf1"</p>
</li>
<li>
<p>Enter "192.168.125.0" as the subnet</p>
</li>
<li>
<p>Enter "255.255.255.0" as the Netmask</p>
</li>
<li>
<p>Enter "200" as the VLAN id</p>
</li>
<li>
<p>Enter "192.168.125.1" as the router address</p>
</li>
<li>
<p>Next, click on the down arrow to the right of the storage network and select the Edit "Address Ranges" item.</p>
</li>
<li>
<p>In the Edit Address Ranges dialog, click "Add Range" and specify the range as:</p>
</li>
</ol>
<ul>
<li>Node Type: host </li>
<li>IPV4 Start Addr: 192.168.125.81</li>
<li>IPV4 End Addr: 192.168.125.160</li>
</ul>
<p>Keep all the other default values.</p>
</section>
<section id="preview-of-swift-deployment-steps"> <title>Preview of Swift Deployment Steps</title>
<p>After creating the cloud and defining your compute regions (see this later topic, <xref href="../cloudos/install.create-cloud.dita" >Create a Cloud</xref>, 
you can deploy Swift in your cloud by following these steps:</p>
<ol>
<li>
<p>Launch the HP Cloud OS Installation Dashboard. Its URL is http://192.168.124.10:3000.  <b>Note:</b> In the current release, the dashboards use pre-defined login credentials. These are not published in the web-hosted documentation. To get the pre-defined login credentials, refer to the readme file included in the same ZIP that contained the HP Cloud OS ISO. If you have not already done so, see the ZIP on the <xref href="https://cloudos.hpwsportal.com" scope="external" format="html" >HP Helion Distribution Network</xref>.</p>
</li>
<li>
<p>The Swift install module is pre-installed with a default proposal created (without assigned nodes).</p>
</li>
<li>
<p>From the UI's top menu, select Barclamps &gt; Cloud Infrastructure &gt; Hp Cos Swift 100.</p>
</li>
<li>
<p>Click the Edit button for the pre-existing Swift proposal.</p>
</li>
<li>
<p>Assign the controller node to the Swift ring-compute, proxy and dispersion roles. For background information, see the next section, <xref type="section" href="#topic7517/swift-roles">Swift Roles</xref>.</p>
</li>
<li>
<p>Assign between 2-5 nodes to the Swift storage role.</p>
</li>
<li>
<p>All the other defaults should be adequate.</p>
</li>
<li>
<p>Click the Apply button to deploy Swift in your cloud.</p>
</li>
</ol>
</section>
<section id="swift-roles"> <title>Swift Roles</title>
</section>
<section id="proxy"> <title>Proxy</title>
<p>The proxy server is responsible for tying together the Swift storage architecture. For each request sent to the proxy server, 
it will look up the location of the account, container, or object in the ring (see below) and route the request accordingly. The 
proxy server also exposes the public API.</p>
<p>A large number of failures are also handled in the proxy server. For example, if a server is unavailable for an object PUT, 
it will ask the ring for a handoff server and route there instead.</p>
<p>When objects are streamed to (or from) an object server, they are streamed directly through the proxy server to (or from) the user.  The 
proxy server does not spool them.</p>
</section>
<section id="ring-compute"> <title>Ring-compute</title>
<p>In a Swift environment, a ring represents a mapping between the names of entities stored on disk and their physical location. 
There are separate rings for accounts, containers, and objects. When other components need to perform any operation on an object, 
container, or account, they need to interact with the appropriate ring to determine its location in the cluster.</p>
<p>The Ring maintains this mapping using zones, devices, partitions, and replicas. Each partition in the ring is replicated, by default, 
three times across the cluster.  The locations for a partition are stored in the mapping maintained by the ring. The ring is 
also responsible for determining which devices are used for handoff in failure scenarios.</p>
<p>Data can be isolated with the concept of zones in the ring. Each replica of a partition is guaranteed to reside in a different zone. 
A zone could represent a drive, a server, a cabinet, a switch, or even a datacenter. The partitions of the ring are equally divided 
among all the devices in the Swift installation. When partitions need to be moved around (for example if a device is added to the cluster), 
the ring ensures that a minimum number of partitions are moved at a time, and only one replica of a partition is moved at a time.</p>
<p>Weights can be used to balance the distribution of partitions on drives across the cluster. This can be useful, for example, 
when different sized drives are used in a cluster. The ring is used by the proxy server and several background processes (like replication).</p>
</section>
<section id="dispersion"> <title>Dispersion</title>
<p>The Swift dispersion role is used for cluster health monitoring.</p>
<p>There is a swift-dispersion-report tool that you can use to measure the overall cluster health. This is accomplished by checking if a 
set of deliberately distributed containers and objects are currently in their proper places within the cluster.</p>
<p>For instance, a common deployment has three replicas of each object. The health of that object can be measured by checking if each 
replica is in its proper place. If only two of the three is in place, the object's health can be said to be at 66.66%.</p>
<p>A single object's health, especially an older object, usually reflects the health of that entire partition the object is in. 
If you make enough objects on a distinct percentage of the partitions in the cluster, you can get a valid estimate of the overall 
cluster health. In practice, about 1% partition coverage seems to balance well between accuracy and the amount of time 
it takes to gather results.</p>
</section>
<section id="next-step"> <title>Next Step</title>
<p>Proceed to the next topic, <xref href="../cloudos/install.complete-admin-node-installation.dita" >Complete Admin Node Installation (Required)</xref>.</p>
<p>
  <xref type="section" href="#topic7517/top"> Return to Top </xref>
</p>
</section>
</body>
</topic>
