---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Disaster Recovery"
permalink: /helion/openstack/1.1/disaster/recovery/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Paul F

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!-- <p style="font-size: small;"> <a href="/helion/openstack/1.1/">&#9664; PREV | <a href="/helion/openstack/1.1/">&#9650; UP</a> | <a href="/helion/openstack/1.1/faq/">NEXT &#9654; </a></p> -->

# HP Helion OpenStack&#174; 1.1: Disaster Recovery

Disaster Recovery (DR) and Business Continuity Planning (BCP) plans are standard requirements for information security management systems (ISMSs) and compliance activities. These plans must be documented and periodically tested to make sure they work. For OpenStack deployments, key areas include the management security domain, and any identified single points of failure (SPOFs).

##Recovering from a Cloud (OS) Failure

This section discusses recovering a cloud deployment after a disaster.
The procedures in this section explain how to recover your cloud after a disaster. 

###A disaster recovery scenario

This section presents an example of a cloud deployment disaster and the recovery steps taken to restore the cloud.

A disaster typically affects several components of your architecture (for example, a disk crash, a network loss, a loss of power). In this example, the following components are configured:

1. A cloud controller `(nova-api`, `nova-objectstore`, `nova-network`)

2. A compute node (`nova-compute`)

3. A Storage Area Network (SAN) used by OpenStack Block Storage (`cinder-volumes`)

The worst case disaster scenario for a cloud is a loss of power, in which case all components, disks, network, are affected. Before a power loss:

* From the SAN to the cloud controller, we have an active iSCSI session (used for the `cinder-volumes` LVM's VG).

* From the cloud controller to the compute node, we also have active iSCSI sessions (managed by `cinder-volume`).

* For every volume, an iSCSI session is made (so 14 EBS volumes equals 14 sessions).

* From the cloud controller to the compute node we also have iptables/ebtables rules, which allow access from the cloud controller to the running instance.

* And from the cloud controller to the compute node, the current state of the "running" instances and their volume attachments (mount point, volume ID, volume status, and so on) which are saved into database.


After the power loss occurs and all hardware components restart with power restoration:

* The iSCSI session from the SAN to the cloud, no longer exists.

* The iSCSI sessions from the cloud controller to the compute node, no longer exist.

* The iptables and ebtables are recreated from the cloud controller to the compute node, since at boot, nova-network reapplies configurations.

* Instances from the cloud controller are in a shutdown state (because they are no longer running).

* Data in the database was not updated at all since Compute could not have anticipated the crash.

**Note**: Instances won't be lost, because no `destroy` or `terminate` command was invoked, so the files for the instances remain on the compute node.

To recover from this disaster, perform these tasks in the following order:

####To perform disaster recovery

1.	Get the instance-to-volume relationship

	You must determine the current relationship from a volume to its instance, because you will re-create the attachment.

	You can find this relationship by running `nova volume-list`. Note that the **nova** client includes the ability to get volume information from OpenStack Block Storage (Cinder).

2.	Update the database

	Update the database to clean the stalled state. Use these queries to clean up the database for each volume:

		mysql> use cinder;
		mysql> update volumes set mountpoint=NULL;
		mysql> update volumes set status="available" where status <>"error_deleting";
		mysql> update volumes set attach_status="detached";
		mysql> update volumes set instance_id=0;

	You can then run `nova volume-list` commands to list all volumes.

3.	Restart instances

	Restart the instances using the `nova reboot INSTANCE` command.

	At this stage, depending on your image, some instances completely reboot and become reachable, while others stop on the "plymouth" stage.

4.	DO NOT reboot a second time
	
	Do not reboot instances that are stopped at this point. The instance state depends on whether you added an `/etc/fstab` entry for a specific volume. Images built with the cloud-init package remain in a pending state, while others skip the missing volume and start. The idea of that stage is only to ask Compute to reboot every instance, so the stored state is preserved. For more information about `cloud-ini`t, see [help.ubuntu.com/community/CloudInit](https://help.ubuntu.com/community/CloudInit).

5.	Reattach volumes
	
	After restart and Compute has restored the correct status, you can reattach the volumes to their respective instances using the `nova volume-attach` command. The following snippet uses a file of listed volumes to reattach them:

		#!/bin/bash
		while read line; do
		volume=`echo $line | $CUT -f 1 -d " "`
		instance=`echo $line | $CUT -f 2 -d " "`
		mount_point=`echo $line | $CUT -f 3 -d " "`
		echo "ATTACHING VOLUME FOR INSTANCE - $instance"
		nova volume-attach $instance $volume $mount_point
		sleep 2
		done < $volumes_tmp_file

	At this stage, instances that were pending on the boot sequence (plymouth) automatically continue their boot, and restart normally, while the ones that booted see the volume.

6. SSH into instances

	If some services depend on the volume, or if a volume has an entry into fstab, simply restart the instance. This restart needs to be made from the instance itself, not through **nova**.

	SSH into the instance and perform a reboot:

		# shutdown -r now

By completing this procedure, you can successfully recover your cloud.

###Tips

Follow these guidelines:

* Use the `errors=remount` parameter in the `fstab` file, which prevents data corruption.
The system locks any write to the disk if it detects an I/O error. This configuration option should be added into the `cinder-volume` server (the one which performs the iSCSI connection to the SAN), but also into the instances' `fstab` file.

* Do not add the entry for the SAN's disks to the `cinder-volume fstab` file.
Some systems hang on this step, which means you could lose access to your cloud-controller. To re-run the session manually, before performing the mount, enter:

		# iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l

* For your instances, if you have the whole `/home/` directory on the disk, leave a user's directory with the user's bash files and the `authorized_keys` file (instead of emptying the `/home` directory and mapping the disk on it).

This enables you to connect to the instance, even without the volume attached, if you allow only connections through public keys.

###Script the DRP

You can download a bash script (available from HP customer support) which:

1.	Creates an array for instances and their attached volumes.

2.	Updates the MySQL database.

3.	Restarts all instances using `euca2ools`.

4.	Make the volume attachment.

5.	Performs an SSH connection into every instance using Compute credentials.

The **test mode** allows you to perform this whole sequence for only one instance.

To reproduce the power loss, connect to the compute node which runs that same instance and close the iSCSI session. Do not detach the volume using the `nova volume-detach` command. Instead, manually close the iSCSI session. The following example command uses an iSCSI session with the number 15:
		
		# iscsiadm -m session -u -r 15

**Important**: Use the `â€“r` flag. Otherwise, you close ALL sessions.
For additional Disaster Recovery Plan (DRP) information, see [http://en.wikipedia.org/wiki/Disaster_Recovery_Plan.](http://en.wikipedia.org/wiki/Disaster_Recovery_Plan)

##Recover failed compute nodes

If a cloud compute node fails (due to a hardware malfunction for example), you can evacuate instances to make them available again. You can optionally include the target host on the `evacuate` command. If you omit the host, the scheduler determines the target host.

To preserve user data on the server disk, you must configure shared storage on the target host. Also, you must validate that the current VM host is down; otherwise, the evacuation fails with an error.

1.	To list hosts and find a different host for the evacuated instance, run:

		$ nova host-list

2.	Evacuate the instance. You can pass the instance password to the command by using the` --password <pwd>` option. If you do not specify a password, one is generated and printed after the command finishes successfully. The following command evacuates a server without shared storage from a host that is down, to the specified host_b:

		$ nova evacuate evacuated_server_name host_b
 
	The instance is booted from a new disk, but preserves its configuration including its ID, name, uid, IP address, and so on. The command returns a password:

		+-----------+--------------+
		| Property  |    Value     |
		+-----------+--------------+
		| adminPass | kRAJpErnT4xZ |
		+-----------+--------------+

3.	To preserve the user disk data on the evacuated server, deploy OpenStack Compute with a shared file system. To configure your system, see [Configure migrations](http://docs.openstack.org/admin-guide-cloud/content/section_configuring-compute-migrations.html). In the following example, the password remains unchanged:

		$ nova evacuate evacuated_server_name host_b --on-shared-storage    

##Manual recovery

Use the following procedure for all hypervisors, except a KVM/libvirt compute node.
Review host information

1.	Identify the VMs on the affected hosts, using tools such as a combination of `nova list` and `nova show` or `euca-describe-instances`. For example, the following output displays information about instance `i-000015b9` that is running on node `np-rcc54`:

		$ euca-describe-instances
		i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60

2.	Review the status of the host by querying the Compute database. Important information is highlighted below. The following example converts an EC2 API instance ID into an OpenStack ID. If you used the nova commands, you can substitute the ID directly. You can find the credentials for your database in `/etc/nova.conf`.


		mysql> SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
		*************************** 1. row ***************************
		              created_at: 2012-06-19 00:48:11
		              updated_at: 2012-07-03 00:35:11
		              deleted_at: NULL
		...
		                      id: 5561
		...
		             power_state: 5
		                vm_state: shutoff
		...
		                hostname: at3-ui02
		                    host: np-rcc54
		...
		                    uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
		...
		              task_state: NULL
...

###Recover the VM

1. After you have determined the status of the VM on the failed host, decide to which compute host the affected VM should be moved. For example, run the following database command to move the VM to `np-rcc46`:

		mysql> UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';

2.	If you are using a hypervisor that relies on libvirt (such as KVM), it is a good idea to update the `libvirt.xml` file (found in `/var/lib/nova/instances/[instance ID]`). <br />The important changes to make are:

	* Change the `DHCPSERVER` value to the host IP address of the compute host that is now the VM's new home.

	* Update the VNC IP, if it isn't already updated, to: `0.0.0.0`.

3.	Reboot the VM using:

		$ nova reboot --hard 3f57699a-e773-4650-a443-b4b37eed5a06

The above database update and `nova reboot` command are typically all that are required to recover a VM from a failed host. However, if further problems occur, consider looking at recreating the network filter configuration using `virsh`, restarting the Compute services or updating the `vm_state` and `power_state` in the Compute database.

##Recover from, a UID/GID mismatch

When running OpenStack Compute, using a shared file system or an automated configuration tool, you could encounter a situation where some files on your compute node are using the wrong UID or GID. 

This causes a number of errors, such as being unable to do live migration or start virtual machines.

The following procedure runs on `nova-compute` hosts, based on the KVM hypervisor, and should restore the situation:

**To recover from a UID/GID mismatch**:

1.	Make sure you do not use numbers that are already used for some other user/group.

2.	Set the nova uid in `/etc/passwd` to the same number in all hosts (for example, 112).

3.	Set the `libvirt-qemu uid` in `/etc/passwd` to the same number in all hosts (for example, 119).

4.	Set the nova group in `/etc/group` file to the same number in all hosts (for example, 120).

5.	Set the `libvirtd` group in `/etc/group` file to the same number in all hosts (for example, 119).

6.	Stop the services on the compute node.

7.	Change all the files owned by user nova or by group nova. For example:

		# find / -uid 108 -exec chown nova {} \; # note the 108 here is the old nova uid before the change
		# find / -gid 120 -exec chgrp nova {} \;

8.	Repeat the steps for the `libvirt-qemu` owned files, if those need to change.

9.	Restart the services.

10.	Now you can run the find command to verify that all files are using the correct identifiers.

###Active/Passive

In an active/passive configuration, systems are set up to bring additional resources online to replace those that have failed. For example, OpenStack would write to the main database while maintaining a disaster recovery database that can be brought online in the event that the main database fails.

Typically, an active/passive installation for a stateless service would maintain a redundant instance that can be brought online when required. Requests may be handled using a virtual IP address to facilitate return to service with minimal reconfiguration required.

A typical active/passive installation for a stateful service maintains a replacement resource that can be brought online when required. A separate application (such as Pacemaker or Corosync) monitors these services, bringing the backup online as necessary.

##Recovering overcloud clusters

The following procedures for recovering overcloud clusters address these concerns:

* RabbitMQ loss of quorum handling.

* How to gracefully take nodes out of a MySQL cluster.

* How to gracefully take nodes out of RabbitMQ cluster.

* Power outage, what to do to bring all solutions back.

* How to re-create all the network/bridges when the KVM host is rebooted.

* How to find if the database in the overcloud is corrupted or not

The following procedures explain how to shut down specific servers for maintenance:

###KVM Host

####Shutdown

First shut down the seed then manually access the OS and shut down the host.

####Power On

Normally power on the KVM Host and then re-create the bridge/routers. For the steps supported in this release, please contact customer support.

####Considerations

During the time the Seed VM will also be shut down, the undercloud servers will not have access to the pxe-boot server. Do not shut down/reboot any of the servers during this period. Users will not be able to SSH to the undercloud and overcloud servers. This will not affect the overcloud usage.

###Seed

####Shutdown

Normally shut down the Seed VM and after that the KVM Host, to execute any necessary maintenance in the server.

####Power On

Normally power on the KVM Host and after that the Seed VM.

####Considerations

During the time the Seed VM will also be shut down, the undercloud servers will not have access to the pxe-boot server. Do not shut down/reboot any of the servers during this period. Users will not be able to SSH to the undercloud and overcloud servers. This will not affect the overcloud usage.

###UnderCloud

####Shutdown

Normally shut down the server, using the `nova stop` option in seed or manually access the OS and shut down the machine.

####Power On

Normally power on the server using `nova start` option in the seed or manually start the server using iLO.

####Considerations

During the time that the Seed VM is shut down, the overcloud servers will not have access to the pxe-boot server. Do not shut down/reboot any of the servers during this period. This will not affect the overcloud usage.

###Overcloud Controllers

####Shutdown

Normally shut down the server, using the `nova stop` option in the seed/undercloud or manually access the OS and shut down the machine.

####Power On

Normally power on the server using `nova start` option in the seed/undercloud or manually start the server using iLO.

####Considerations 

HP recommends that you do not shut down more than one controller at the same time or the High Availability functionality will not work. If only one server is shut down, the solution should not have any downtime. When the management controller is shut down, access to the VSA will be temporarily disabled, so the user can follow the steps to start cinder-volume in one of the other controller during the downtime period.

###Overcloud Compute Nodes

####Shutdown

Normally shut down the server, using the `nova stop` option in seed/undercloud or manually access the OS and shut down the machine.

####Power On

Normally power on the server using `nova start` option in the seed/undercloud or manually start the server using iLO. After power on process ends, the VM`s hosted in the node will be automatically started.

Considerations

To reduce the down time of the instances located in the compute node HP suggests migrating the instances to another server before shutting down the node. Is possible to shut down N compute nodes at the same without problem, just remember that capacity will be reduced.

###Overcloud Swift

####Shutdown

Normally shut down the server, using the `nova stop` option in seed/undercloud or manually access the OS and shut down the machine.

####Power On

Normally power on the server using `nova start` option in the seed/undercloud or manually start the server using iLO.

####Considerations

HP recommends that you do not shut down both Swift servers at the same time or the High Availability functionality will not work. If only one server is shut down, the solution should not have any downtime.

###Overcloud VSA

####Shutdown

Normally shut down the server, using the `nova stop` option in seed/undercloud or manually access the OS and shut down the machine.

####Power On

Normally power on the server using `nova start` option in the seed/undercloud or manually start the server using iLO.

####Considerations

HP recommends that you do not shut down both VSA servers at the same time or the High Availability functionality, depending on your configuration, may not work. In the case of only one VSA server, if the server is shut down, the solution will not have the ability to create/manage/access the volumes created during the downtime.

1. Recovering from a power outage:

	In the case the data center has lost power, when power is restored, the administrator needs to start servers in a specific order and execute specific manual commands to bring back the overcloud controller cluster.

	* Start the KVM Host.

		* Re-create the bridge configuration.

	* SSH to the KVM Host and start the Seed VM (`virsh` start seed).

		* Wait the VM boot up.

	* Start Undercloud Node (using nova start in Seed VM).

		* Wait the server start up and stabilize.

	* Start all overcloud controllers.

		* Wait for the server to start up and stabilize.

	* Execute the command `cat /mnt/state/var/lib/mysql/grastate.dat` in all controllers to find which one has the most updated database (seqno).

	For example:

		cat /mnt/state/var/lib/mysql/grastate.dat
		# GALERA saved state
		version: 2.1
		uuid: 98414b8c-661a-11e4-945b-5f36f0cd94d6
		seqno: 959325
	
	* In the controller with the highest seqno, execute the command
 
			/etc/init.d/mysql bootstrap-pxc
	
	* Connect to the other controllers and normally start MySQL 

			/etc/init.d/mysql start
	
	* When all nodes joined the cluster execute in all the overcloud nodes, one each time to restart all the services and finish the startup

			os-refresh-config Start Swift0 and 1 Nodes
	
	* Wait for the  server to start up and stabilize.
	* Start the VSA nodes.
		* Wait for the server to start up and stabilize.
	* After everything else is running, start the Compute nodes.
		* Wait for the server to start up and stabilize. The instances should start automatically.
	
2. Graceful shut down and starting the complete solution:

In the case of a graceful shutdown of the entire environment, the administrator needs to follow these steps, in this specific order:

1.	Shutdown sequence

	a. Stop the overcloud instances/compute node(s) using the `nova stop` option in seed or manually access the OS and shut down the machine.

	b.  Stop the overcloud VSA node(s) nodes using the `nova stop` option in seed or manually access  the OS and shut down the machine.

	c. Stop the overcloud Swift nodes using the `nova stop` option in seed or manually access the OS and shut down the machine.

	d. Stop the overcloud Management controller node using the `nova stop` option in seed or manually access the OS and shut down the machine.

	e. Stop the overcloud Controller 1 node using the `nova stop` option in seed or manually access the OS and shut down the machine.

	f. Stop the overcloud Controller 0 node using the `nova stop` option in seed or manually access the OS and shut down the machine.

	g. Stop the Undercloud node using the `nova stop` option in seed or manually access the OS and shut down the machine.

	h. Stop the Seed VM using `virsh shutdown` or manually access the OS and shut down the machine.

	i. Stop the KVM Host manually access the OS and shut down the machine


	**Important**

	The order that the overcloud controllers are stopped is really important, because based on that we will know the most updated database. 

2.	Start sequence

	a. Start the KVM Host
		
	* Re-create the bridge configuration.
		
	b.	Start the Seed VM using `virsh start`.

	c.	Start the undercloud node using `nova start` from the Seed VM.

	d. Start the overcloud Controller 0 node using `nova start` from the Seed VM or undercloud node.

	e. When the node starts, manually start MySQL with the bootstrap options (`/etc/init.d/mysql bootstrap-pxc`)

	f. Start the overcloud Controller 1 node using `nova start` from the Seed VM or undercloud node.

	g. Start the overcloud Management node using `nova start` from the Seed VM or undercloud node.

	h. Start the overcloud Swift nodes using `nova start` from the Seed VM or undercloud node.

	i. Start the VSA node(s) using `nova start` from the Seed VM or undercloud node.

	j. Start the overcloud Compute Node(s) using `nova start` from the Seed VM or undercloud node.

## Recovering from failed controllers ##
In the event that you need to recover a controller, or inadvertently delete a controller, refer to the following link for recovery details.

[http://docs.hpcloud.com//helion/openstack/1.1/removing/failedovercloud/#removecontroller1](http://docs.hpcloud.com//helion/openstack/1.1/removing/failedovercloud/#removecontroller1 "Recovering a failed or deleted controller")


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

