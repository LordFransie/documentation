---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Update Procedure"
permalink: /helion/openstack/1.1/updates/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Michael B, Paul F

---
<!--UNDER REVISION-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<!---
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p> -->



# HP Helion OpenStack&#174; 1.1: Update Procedure

Welcome to the Helion OpenStack 1.1 update instructions. These instructions apply to existing Helion OpenStack 1.1 installations and describe how you can update your Helion OpenStack cloud environment from 1.1 to a higher release. 

If you are running a version of Helion OpenStack prior to the 1.1 release, there is no update procedure. The process of changing a Helion OpenStack 1.0 or 1.01 release to a later release (such as 1.1) is described in the release notes for the respective release.

The process of updating Helion OpenStack 1.1 to a higher release consists of two steps:

1. [Update the undercloud](#uc-update-process)
2. [Update the overcloud](#oc-update-process)

These two procedures are described in the following sections.

**NOTE:** Nodes to be updated must be in a “good” state before running the update.

## Before you begin ##
Before you start your update, [[
and heat templates

##Helion 1.1 to 1.1.X Undercloud update process {#uc-update-process}

The procedure will guide you through updating the Undercloud by running the update_uc.sh shell script.

To update your Helion OpenStack undercloud from 1.1 to a higher release:

1.	Log on to the seed VM.

    	ssh root@<seed-ip>

2.	Download the update tarball onto the seed VM.

	    cd /root
	    mkdir helion-update-1.1-to-<version>
	    cd helion-update-1.1-to-<version>
	    wget http://<url>/helion_ee_<version>.tgz

3.	Extract the tarball and run the update script.

	    tar -xvzf helion_ee_<version>.tgz
	    cd tripleo/helion-update/undercloud_update
	    ./update_uc.sh ~/helion-update-1.1-to-<version>/tripleo

Your Helion OpenStack Undercloud is now updated. Confirm that update was successful by examining the ` /var/log/ansible/ansible.log`.  You should see:

	PLAY RECAP ********************************************************************
	10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
	10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
	10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
	10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
	10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
	10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
	10.23.67.148   : ok=60   changed=30   unreachable=0failed=0


##Helion 1.1 to 1.1.X Overcloud update process {#oc-update-process}

Updating the Overcloud using `update_oc.sh`

1. Log on to the seed

    	ssh root@<seed-ip>

2. Download the update tarball on the seed

   		 cd /root
    	mkdir helion-update-1.1-to-<version>
    	cd helion-update-1.1-to-<version>
    	wget http://<url>/helion_ee_<version>.tgz

3. Extract the tarball and run the update script

	    tar -xvzf helion_ee_<version>.tgz
	    cd tripleo/helion-update/overcloud_update
	    ./update_oc.sh ~/helion-update-1.1-to-<version>/tripleo

Your Helion OpenStack overcloud is now updated. Confirm that update was successful by examining the ` /var/log/ansible/ansible.log`.  You should see:

	PLAY RECAP ********************************************************************
	10.23.67.141   : ok=122  changed=74   unreachable=0failed=0
	10.23.67.143   : ok=37   changed=22   unreachable=0failed=0
	10.23.67.144   : ok=122  changed=74   unreachable=0failed=0
	10.23.67.145   : ok=103  changed=62   unreachable=0failed=0
	10.23.67.146   : ok=37   changed=22   unreachable=0failed=0
	10.23.67.147   : ok=60   changed=30   unreachable=0failed=0
	10.23.67.148   : ok=60   changed=30   unreachable=0failed=0


###Updating the overcloud manually

This procedure allows you to update your overcloud installation from Helion OpenStack 1.1 to a higher release by executing the update commands directly and not relying on the update scripts.

To update your overcloud:

1. Log on to the seed
 
    	ssh root@<seed-ip>

2. Download the update tarball on the seed
 
	    cd /root
	 	mkdir helion-update-1.1-to-1.1.X
		cd helion-update-1.1-to-1.1.X
		wget http://<url>/helion_ee_1.1.X.tgz

3. Untar the file

    	tar -xvzf helion_ce_1.1.X.tgz

4. Log onto the seed and change to the downloaded directory.

	    ssh root@<seed-ip>
	    cd helion-update-1.1-to-1.1.X

5. Source the undercloud credentials

	    export TRIPLEO_ROOT=~/tripleo
	    export TE_DATAFILE=~/tripleo/ce_env.json
	    source ~/tripleo/tripleo-incubator/undercloudrc

6. Download the image service (Glance) images locally.
 
	    OLD_BUILD_NO=$(glance image-show overcloud-compute-vmlinuz \
	                   | grep "Property 'build_no'" | awk '{print $5}')
	    mkdir build-$OLD_BUILD_NO
	    cd build-$OLD_BUILD_NO
	    for image in $(glance image-list | awk '{print $4}' | grep -);do
	        glance image-download --file ./$image $image;
	    done

7. Remove old images from Glance once saved locally. 

	    for image in $(glance image-list | awk '{print $4}' | grep -);do
	        glance image-delete  $image;done

8. Upload new images to Glance.

	    BUILD_NO=$(cat /root/helion-update-1.1-to-1.1.1/tripleo/ce_env.json  | grep build_number | awk '{print $2}')
	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-compute-$BUILD_NO.qcow2
	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-control-$BUILD_NO.qcow2
	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-swift-$BUILD_NO.qcow2
	    glance image-create --name bm-deploy-kernel --is-public True \
	      --disk-format aki < /root/helion-update-1.1-to-1.1.X/tripleo/images/deploy-ramdisk-ironic.kernel
	    glance image-create --name bm-deploy-ramdisk --is-public True \
	      --disk-format ari < /root/helion-update-1.1-to-1.1.X/tripleo/images/deploy-ramdisk-ironic.initramfs

9. Update image names and set build metadata.
  
	    cd /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/
	    ./set-sherpa-metadata overcloud-compute-$BUILD_NO
	    ./set-sherpa-metadata overcloud-control-$BUILD_NO
	    ./set-sherpa-metadata overcloud-swift-$BUILD_NO

10. Update the triple Ansible playbook.

	    mv /opt/stack/triple-ansible /opt/stack/triple-ansible-1.1
	    cp -r /root/helion-update-1.1-to-1.1.X/tripleo/helion-update/tripleo-ansible/ /opt/stack/

Refer to the Ansible update README for more details on running the play.

    source /opt/stack/venvs/ansible/bin/activate
    cd /opt/stack/tripleo-ansible/
    ./scripts/inject_nova_meta.bash
    ./scripts/populate_image_vars
    ansible-playbook -vvvv -M library/cloud -i plugins/inventory/heat.py -u heat-admin playbooks/pre-flight_check.yml
    ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py playbooks/update_cloud.yml

# Validating the update #
[[from email]]

# Troubleshooting an update with errors #
This section explains how to fix common problems  that might arise when performing an update to Helion.

## Retrying failed actions ##

In some cases, steps may fail because some components may still be initializing and not not yet be ready for
use.
In this event, you have two options to re-attempt or resume playbook executions.

1.  Use the Ansible `ansible-playbook` command with the  `--start-at-task="TASK NAME"` option. This command allows resumption of a playbook, when used with the `-l limit` option.
1. Use the Ansible `ansible-playbook` command with the `--step` option. This command allows you to confirm  each task before it is executed by Ansible.

## A node goes to ERROR state during rebuild ##


This can happen from time to time due to network errors or a temporary
overload of the Undercloud. In this case, the `nova list` return shows node in ERROR. To fix this:

- Make sure your hardware is in working order.
- Verify that approximately 20% of the disk space on the Ironic server node is free.
- Get the image ID of the machine in question using `nova show`

        nova show $node_id


- Manually rebuild by running:

        nova rebuild --preserve-ephemeral $node_id $image_id

## A node times out after rebuild ##


While rare, there is the possibility that something unexpected happened during a rebuild
and the host has failed to reboot. When this happens, you will get this error Message:

	`msg: Timeout waiting for the server to come up.. Please check manually`

To fix this problem, follow the steps detailed in: "A node goes to ERROR state during rebuild".

## MySQL CLI configuration file is missing ##


Should the post-rebuild restart fail, the cause might be that the MySQL CLI configuration file is missing. If you have this issue, attempts to access the MySQL CLI command return:

        ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)

To fix this problem:

- Verify that the MySQL CLI config file stored on the state drive is present and has content.  To display the contents, run:

        sudo cat /mnt/state/root/metadata.my.cnf


- If the file is empty, retrieve current metadata and update  the config files on disk by running:

        sudo os-collect-config --force --one --command=os-apply-config


- Verify that the MySQL CLI config file is present in the root user directory by running:

        sudo cat /root/.my.cnf


- If that file does not exist, or is empty, you have two options.

	1. Add the following to your MySQL CLI command line:    
    
    		--defaults-extra-file=/mnt/state/root/metadata.my.cnf


	1. Alternatively, copy the configuration from the state drive by entering:


			sudo cp -f /mnt/state/root/metadata.my.cnf /root/.my.cnf


## MySQL fails to start after retrying the update ##


If the update was aborted or failed during the Update sequence before a single MySQL controller was operational, MySQL will fail to start upon retrying. In this case, you will see the following error messages:


       * `msg: Starting MySQL (Percona XtraDB Cluster) database server: mysqld . . . . The server quit without updating PID file (/var/run/mysqld/mysqld.pid)`

       * `stderr: ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (111)`

       * `FATAL: all hosts have already failed  - aborting`

    * Update automatically aborts.

**WARNING:**

The command `/etc/init.d/mysql bootstrap-pxc`,  mentioned below, 
      should only ever be executed when an entire MySQL cluster is down, and
      then only on the last node to have been shut down.  Running this command
      on multiple nodes will cause the MySQL cluster to enter a split brain
      scenario effectively breaking the cluster which will result in
      unpredictable behavior.

To fix this problem:


- Use the `nova list` command to determine the IP address of the controller0 node, Then SSH into it by entering:

        ssh heat-admin@$IP


- Verify that MySQL is down by running (as root) the `mysql` client. It should fail:

        sudo mysql -e "SELECT 1"


- Attempt to restart MySQL in case another cluster node is online. This should fail in this error state. However, if it succeeds your cluster should again be operational and you can skip the next step:

        sudo /etc/init.d/mysql start

- Start MySQL in single node bootstrap mode by entering:

        sudo /etc/init.d/mysql bootstrap-pxc


## MySQL/Percona/Galera is out of sync ##

OpenStack is configured to store all of its states in a multi-node, synchronous replication Percona XtraDB Cluster database, which uses Galera for replication. This database must be in sync and have the full
complement of servers before updates can be performed safely.

The problem is update fails with errors about Galera and/or MySQL being `Out of Sync`.

To fix this issue: 

- Use the  `nova list` command to determine the IP address of controller0 node, Then SSH to it by entering:
      
        ssh heat-admin@$IP

- Verify that replication is out of sync by entering:

        sudo mysql -e "SHOW STATUS like 'wsrep_%'"

- Stop mysql by entering:

        sudo /etc/init.d/mysql stop


- Verify that MySQL is down by running (as root) the `mysql` client. It should fail:

        sudo mysql -e "SELECT 1"


- Start controller0 MySQL in single node bootstrap mode by entering:

        sudo /etc/init.d/mysql bootstrap-pxc


- On the remaining controller nodes observed to be having issues, get their IP addresses using the  `nova list`  command and login to them by entering:

        ssh heat-admin@$IP

- Verify that replication is out of sync by entering:

        sudo mysql -e "SHOW STATUS like 'wsrep_%'"


- Stop MySQL by entering:

        sudo /etc/init.d/mysql stop

- Verify that MySQL is down by running (as root) the `mysql` client. It should fail:

        sudo mysql -e "SELECT 1"

- Start MySQL so it attempts to connect to controller0 by entering:

        sudo /etc/init.d/mysql start

If restarting MySQL fails, then the database is most certainly out of sync and the MySQL error logs, located at `/var/log/mysql/error.log`, will need to be checked.  In this case, never attempt to restart MySQL with `sudo /etc/init.d/mysql bootstrap-pxc` as it will bootstrap the host as a single node cluster thus worsening what already appears to be a split-brain scenario. If you need help with this matter, contact HP support.

## MysQL "Node appears to be the last node in a cluster" error ##


This error occurs when one of the controller nodes does not have MySQL running.
The playbook has detected that the current node is the last running node,
although based on its sequence, it should not be the last node.  As a result the
error is thrown and update is aborted.

 The full message is: 

	`Galera Replication. Node appears to be the last node in a cluster; cannot safely proceed unless overridden via single_controller setting. See README.rst`

 To fix this problem:


- Run the `pre-flight_check.yml` playbook.  It will attempt to restart MySQL on each node in the `Ensuring MySQL is running` step.  If that step succeeds, you should be able to re-run the playbook and not encounter this `Node appears to be last node in a cluster` error.

- If pre-flight_check fails to restart MySQL, review the MySQL logs (`/var/log/mysql/error.log`) to determine why the other nodes are not restarting.

## SSH Connectivity is lost ##

Ansible uses SSH to communicate with remote nodes. In heavily loaded, single
host virtualized environments, SSH can lose connectivity.  It should be noted
that similar issues in a physical environment may indicate issues in the
underlying network infrastructure.

  When Ansible loses SSH connectivity causing an update attempt to fail, you will see the following output:

        fatal: [192.0.2.25] => SSH encountered an unknown error. The
        output was: OpenSSH_6.6.1, OpenSSL 1.0.1i-dev xx XXX xxxx
        debug1: Reading configuration data /etc/ssh/ssh_config debug1:
        /etc/ssh/ssh_config line 19: Applying options for * debug1:
        auto-mux: Trying existing master debug2: fd 3 setting
        O_NONBLOCK mux_client_hello_exchange: write packet: Broken
        pipe FATAL: all hosts have already failed  - aborting

 To fix this problem, you can generally re-run the playbook to complete the upgrade, unless SSH connectivity is lost while all MySQL nodes are down. (See 'MySQL fails to start upon retrying update' to correct this issue.)

Early Ubuntu Trusty kernel versions have known issues with KVM which severely impact SSH connectivity to instances. To avoid this issue, Test hosts should have a minimum kernel version of 3.13.0-36-generic.

The update steps, as root, are:

        apt-get update
        apt-get dist-upgrade
        reboot

If you continue to encounter this issue in a physical environment, check the network infrastructure for errors.

Similar error messages may occur with long running processes, such as database creation/upgrade steps.  These cases will generally have partial program execution log output immediately before the broken pipe message visible. Should this be the case, Ansible and OpenSSH may need to have their configuration files tuned to meet the needs of the environment.

 Consult the Ansible configuration file to see available connection settings ssh_args, timeout, and possibly pipelining. To see this file, enter:

        https://github.com/ansible/ansible/blob/release1.7.0/examples/ansible.cfg

 As Ansible uses OpenSSH, consult the `ssh_config` manual, in paricular the `ServerAliveInterval` and `ServerAliveCountMax` options.

## Postfix fails to reload ##


Occasionally the postfix mail transfer agent will fail to reload because it is not running when the system expects it to be running.

 Confirm that this is the case by examining  the `/var/log/upstart/os-collect-config.log` for an indication that `service postfix reload` failed.

 To fix this issue, start postfix by entering:

        sudo service postfix start

## Apache2 fails to start ##


Apache2 requires several self-signed SSL certificates to be properly configured but because of earlier failures in the setup process these certificaties may not have been configured correctly. If this is the case, you will see the following error message:

    * failed: [192.0.2.25] => (item=apache2) => {"failed": true, "item": "apache2"}
    * msg: start: Job failed to start

 You will also note the following symptoms:

- the Apache2 service fails to start
- the `/etc/ssl/certs/ssl-cert-snakeoil.pem` file is missing or empty.

To fix this problem, re-run `os-collect-config` to reassert the SSL certificates by entering:

        sudo os-collect-config --force --one

## RabbitMQ still running when restart is attempted ##


There are certain system states that cause RabbitMQ to ignore normal kill signals. In these cases, RabbitMQ continues to run. You will notice that you have this issue when your attempts to start `rabbitmq` fail because it is already running.

To fix this problem, find any processes running as `rabbitmq` on the server, and kill them, forcibly if need be.

## Instance reported with status as "SHUTOFF" and task_state as "powering on" ##

When Nova attempts to restart an instance when the Compute node is not ready,
it is possible that Nova can enter a confused state where it thinks that
an instance is starting when in fact the Compute node is doing nothing. You have reason to suspect that this is the case when:

- The command `nova list --all-tenants` reports instance(s) with STATUS of "SHUTOFF" and `task_state` is "powering on".
- The instance cannot be pinged.
- No instance appears to be running on the Compute node.
-  Nova hangs when retrieving logs or returns old logs from the previous boot.
-  A console session cannot be established.

To fix this problem:

- Log into a controller as root and enter: 

	`source stackrc`

- Execute `nova list --all-tenants` to obtain instance ID(s)
- Execute `nova show <instance-id>` on each suspected ID to identify suspected Compute nodes.
- Log into the suspected Compute node(s) and execute:
	 `os-collect-config --force --one`
- Return to the controller node that you were logged into previously, and using the instancce IDs obtained previously, take the following steps:

	1. Execute `nova reset-state --active <instance-id>`
	1. Execute `nova stop <instance-id>`
	1. Execute `nova start <instance-id>`
	
1. Once the above steps have been taken in order, you should see the instance status return to ACTIVE and the instance become accessible via the network.

## State drive `/mnt` is not mounted ##


In the rare event that an error occurred between the state drive being unmounted and the rebuild command being triggered, the `/mnt` volume on the instance upon which the rebuild command was executed will be in an unmounted state.

In this state, you will not be able to start MySQL and RabbitMQ. You will likely see these error messages:

    * Pre-flight check returns an error similar to::

        failed: [192.0.2.24] => {"changed": true, "cmd":
        "rabbitmqctl -n rabbit@$(hostname) status" stderr: Error:
        unable to connect to node
        'rabbit@overcloud-controller0-vahypr34iy2x': nodedown


Attempts to start  MySQL or RabbitMQ  return manually will fail and you will see:

        start: Job failed to start

Upgrade attempts return with an error indicating:

        TASK: [fail msg="Galera Replication, Node appears to be the last node in a cluster;  cannot safely proceed unless overriden via single_controller setting. See README.rst"]

If you run the `df` command, the return does not show a volume mounted as `/mnt`.

To fix this problem: 


- Execute the `os-collect` config which will re-mount the state drive. This command may fail without additional intervention. However it should mount the state drive which is all that is needed to proceed to the next step. To run `os-collect-config`, enter:

        sudo os-collect-config --force --one


- At this point, the `/mnt` volume should be visible in the output of the `df` command.
- Start MySQL by entering:

        sudo /etc/init.d/mysqld start


- If MySQL fails to start, and it has been verified that MySQL is not running on any controller nodes, then you will need to identify the Last node that MySQL was stopped on and consult the section "MySQL fails to start upon retrying update" for guidance on restarting the cluster.
- Start RabbitMQ by entering:

        service rabbitmq-server start


- If `rabbitmq-server` fails to start, then the cluster may be down. If this is the case, then the last node to be stopped will need to be identified and started before attempting to restart RabbitMQ on this node.
- At this point, re-execute the pre-flight check, and proceed.

## VMs do not shut down properly during upgrade ##


During the upgrade process, VMs on Compute nodes are shut down
gracefully. If the VMs do not shut down, this can cause the upgrade to stop. If this is the case, you will seea playbook run which ends with a message similar to:

        failed: [10.23.210.31] => {"failed": true} msg: The ephemeral
        storage of this system failed to be cleaned up properly and
        processes or files are still in use. The previous ansible play
        should have information to help troubleshoot this issue.


The output of the playbook run prior to this message contains a process listing and a listing of open files.

The state drive on the Compute node, `/mnt`, is still in use and cannot be unmounted. You can confirm this by entering:

        lsof -n | grep /mnt


To see which VMs are running, enter:

        virsh list


If `virsh list` fails, you may need to restart `libvirt-bin` or `libvirtd` depending on which process you are running. To restart, enter:

        service libvirt-bin restart

or

        service libvirtd restart

To fix this problem, you will have to intervene manually. You will need to determine why the VMs did not shut down properly, and resolve the issue.

You can forcely shutdown non-responsive VMs by entering:
	
 	`virsh destroy <id>`

Note that this can corrupt filesystems on the VM.

Resume the playbook run once the VMs have been shut down.

## Instances are inaccessible via network ##


Upon restarting, it is possible that the virtual machine is unreachable due to Open vSwitch not being ready for the virtual machine networking. If this is the case, you will not be able to ping instances after a restart.

To fix this problem:

- Log into a controller node and execute:

		`source /root/stackrc`


- Stop all virtual machines on a Compute node by entering:

		`nova hypervisor-servers <hostname>` 

and

		`nova stop <id>`


- Log into the Undercloud node and enter:

	 	`source /root/stackrc`

- Obtain a list of nodes by entering:

	 	`nova list`



- Execute `nova stop <id>` for the affected Compute node.

- Once the compute node has stopped, execute `nova start <id>` to reboot the Compute node.

## Online Upgrade fails with message saying glanceclient is not found ##

This problem occurs when you attempt to perform an online upgrade, However the playbook execution failed when you attempted to download the new image from Glance. You get a message that `glanceclient` was not found.

If you are attempting to execute the Ansible playbook on the seed or Undercloud node, source the Ansible virtual environment by entering:


      `source /opt/stack/venvs/ansible/bin/activate`



Once the Ansible virtual environment has been sourced, on the node from which you are attempting to execute Ansible, enter:

      `sudo pip install python-glanceclient` 

## Online upgrade of compute node failed ##

In the event that an online upgrade of a Compute node fails, you can recover the node utilizing a traditional rebuild.

The problem occurs when you perform an online upgrade. The result of which is that a Compute node cannot be logged into, or is otherwise in a non-working state.

To fix this issue, from the Undercloud enter: 

	`source /root/stackrc`


Identify the instance ID of the broken Compute node using the `nova list` command.
Then stop the instance by entering: 
 
	`nova stop <instance-id>` 


Return to the host from which you ran the upgrade and re-run the playbook without the `-e online_upgrade=True` option.

You can also utilize the `-e force_rebuild=True`  option to force the instance to rebuild.











































<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>


----
