---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Deploying and Configuring OVSvApp on ESX hosts"
permalink: /helion/openstack/1.1/install/ovsvapp/update/
product: commercial.ga
product-version1: HP Helion OpenStack 1.1
role1: Storage Administrator
role2: Storage Architect
authors: Michael B, 

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<p style="font-size: small;"> &#9664; <a href="/helion/openstack/1.1/install/esx/proxy/">Deploy vCenter ESX compute proxy | <a href="/helion/openstack/1.1/install/post-esx/">&#9650; Post-Installation for ESX Hypervisor | <a href="/helion/openstack/1.1/install/dnsaas/">Install DNS as a service (DNSaaS) &#9654</a></p> 

# HP Helion OpenStack&#174; 1.1 and 1.1.1: Updating OVSvApp on ESX hosts  
HP Virtual Cloud Networking (VCN) is an enhanced Networking Operations (Neutron) service module of HP Helion OpenStack that delivers network virtualization to orchestrate your data center infrastructure.

## Update OVSvApp from version 1.01 to version 1.1 {#update}

Use the following steps to update the OVSvApp.

Peform the update from the system where OVSvApp passwordless authentication is enabled. That is, from the system where `id_rsa` key is present for the public key (`id_rsa.pub`) that was used during OVSvApp 1.0 installation.


To update the OVSvApp:

1. On the system where OVSvApp passwordless authentication is enabled, install or update the Fabric python library (https://pypi.python.org/pypi/Fabric) using the following command: 

		pip install --upgrade fabric 

2. Download the `ovsvapp-patch-*.zip` and extract the files. 

	The download has `hp-ovsvapp` directory and `ovsvapp-patch-*.tar.gz` 

5. Edit the `ovs_vapp.ini` file in the downloaded `hp-ovsvapp` directory: 

	a. Enter the path to the SSH key file in the [vmconfig] section. 

	Use the same ssh key (id_rsa.pub) that was used for OVSvApp 1.0 installation.

	b. Optionally, enter the path to the `ovsvapp-patch-*.tar.gz` file in the `update_file_path` value in the [update] section.

		update_file_path=<path to ovsvapp-patch-*.tar.gz>

	c. If you want to be able to roll back the update to the previous version, set the `do_backup` value in the [update] section to `True`.

		do_backup=True

	d. Enter the starting name/prefix of `OVSvAPP` in the `ovsvapp_prefix=` if your existing OVSvApp version is v1.0.  

	e. Provide the Kibana Rabbit Host address and password in the [logger] section. 

6. Execute the following commands: 

		sudo su 
		cd /hp-ovsvapp/src/update #python updater.py

	**Note:** If you see the following error while updating the OVSvApp, it means that the system from where update is launched does not have the passwordless authentication enabled for OVSvApp. 

		Fatal error: Needed to prompt for a connection or sudo password (host: <ovs-vapp-ip>), but input would be ambiguous in parallel mode
		Aborting. 

	Re-run the update from the system where OVSvApp passwordless authentication is enabled.


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

## Update OVSvApp from version 1.1 to version 1.1.1 {#update}

Use the following steps from the seed cloud host to update the OVSvApp.

To update the OVSvApp:

1. Edit the `neutron.conf` file on all three overcloud controllers to disable Maintenance Mode for the OVSvApp by adding the following:

		enable_agent_monitor=False

	During the update, the OVSvApp might not be able to communicate with the controller. Setting this parameter to False will prevent the ESX host from being put into maintenance mode.

2. Restart the Networking service (Neutron) server on all 3 overcloud controllers to take into effect:

		service neutron-server restart

3. On the system where OVSvApp passwordless authentication is enabled, install or update the Fabric python library (https://pypi.python.org/pypi/Fabric) using the following command: 

		pip install --upgrade fabric 

4. From the seed cloud host, copy `/root/helion-update-1.1-to-<version>/tripleo/hp-ovsvapp/` folder from seed VM using the following command:

		scp r root@<seed-ip>:/root/helion-update-1.1-to<version>/tripleo/hp-ovsvapp/ 

5. Edit the `ovs_vapp.ini` file in the downloaded `hp-ovsvapp` directory: 

	a. Enter the path to the SSH key file in the [vmconfig] section. 

	Use the same ssh key (id_rsa.pub) that was used for OVSvApp 1.0 installation.

	b. Make sure the `update_file_path` value in the [update] section is blank.

	c. Makwe sure the `do_backup` value in the [update] section is blank.

	d. Makwe sure the `ovsvapp_prefix=` field is blank. 

	e. Provide the Kibana Rabbit Host address and password in the [logger] section. 

	f. Specify the IP address of the seed VM:

		[seed-ip]
		#Seed IP
		seed_ip=

6. Execute the following commands: 

		sudo su 
		cd /hp-ovsvapp/src/update #python updater.py

	**Note:** If you see the following error while updating the OVSvApp, it means that the system from where update is launched does not have the passwordless authentication enabled for OVSvApp. 

		Fatal error: Needed to prompt for a connection or sudo password (host: <ovs-vapp-ip>), but input would be ambiguous in parallel mode
		Aborting. 

	Re-run the update from the system where OVSvApp passwordless authentication is enabled.

7. Enable agent monitoring, which is active by default, by setting the following varable in the `neutron.conf` file on all three overcloud controllers.

		enable_agent_monitor=True

8. Restart the Networking service (Neutron) server on all 3 overcloud controllers to take into effect:

		service neutron-server restart


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>


<!-- Not needed?
1. Make sure that DRS is enabled on the cluster on which 1.01 version of OVSvApp will be installed:

	a. In the vSphere client, select the cluster in the vSphere Client inventory.

	b. Right-click and select **Edit Settings**.

	c. In the left panel, select General, and make sure **Turn On vSphere DRS** is selected.

	d. Click **OK**.

	**Note:** DRS safeguards tenant VM traffic from being black-holed.

2. Disable vMotion from vSwitch properties. This will prevent DRS from bringing back VMs on the host when the host is brought back from maintenance mode as in Step 4. 

	a. In the vSphere client, select the host in the vSphere Client inventory.

	b. On the **Configuration** tab, select **Networking**.  

	c. Click **Virtual Switch** to display the virtual switches for the host.
	
	d. Locate the virtual switch that has a VMkernel port group configured for vMotion, and click the **Properties** link.

	e. On the **Ports** tab, select the port group that is configured for vMotion and click **Edit**.

	f. On the **General** tab, clear the **Enabled** option for vMotion.

	g. Click **OK** to close the port group **Properties** dialog, and click **Close** to close the vSwitch **Properties** dialog. 

3. Place the ESX host on which the 1.01 version of OVSvApp will be installed into maintenance mode :

	In the vSphere Client, right click on the ESX host and select **Enter Maintenance mode**.

	All virtual machines on the host are migrated to different hosts when the host enters maintenance mode.

4. Exit maintenance mode.

	In the vSphere Client, right click on the ESX host and select **Exit Maintenance mode**.

5. Delete the OVSvApp appliance:

	a. Right-click the **OVSvApp VM**. 

	b. Select **Delete**.

6. On the controller, execute the following command to obtain the `ovsvapp_agent_id`.

		neutron agent-list 

	Note the ID.

7. On the controller, execute the following command to remove the entry from `neutron agent-list`.

		neutron agent-delete <ovsvapp_agent_id>

	**Where:**

	`<ovsvapp_agent_id>` is the OVSvApp ID obtained.

9. Install 1.01 version of OVSvApp VM on that ESX host using the `add_new_hosts` variable under the `new-host-addition` section in `ovs_vapp.ini` file

		add_new_hosts=True

10. Invoke the installer using the following commands:

		sudo su
		cd /hp-ovsvapp/src/installer/
		python invoke_vapp_installer.py

	The installation log file will be located at `/hp-ovsvapp/log/ovs_vapp.log`.

11. Re-enable vMotion on vSwitch properties of that ESX host.

	a. In the vSphere Client, right click on the ESX host.

	b. Click the **Configuration** tab.

	c. In the **Hardware** section, click **Networking**. 

	d. Click **Properties** for the virtual switch where a VMkernel port has been configured.

	e. In the dialog box that opens, select `vmkernel` in the **Ports** tab and click **Edit**. 

	f. Select **Enabled** next to vMotion.

	g. Click **OK**.
-->

----

