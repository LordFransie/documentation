---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Installation Troubleshooting"
permalink: /helion/openstack/1.1/services/troubleshooting/install/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Paul F

---
<!--PUBLISHED-->

<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--

<p style="font-size: small;"> <a href="/helion/openstack/1.1/services/object/overview/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/services/overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/services/reporting/overview/"> NEXT &#9654</a> </p> -->


# HP Helion OpenStack&#174; 1.1: Installation Troubleshooting

This Helion installation troubleshooting guide is written for Helion OpenStack installers who, during or after building the Helion OpenStack environment, find that the cloud environment does not work as expected. The Helion OpenStack environment is not itself complex but it does include a sufficient number of components that isolating a fault can be challenging. 

The goal of this guide is to help you to identify the problem or problems affecting your cloud installation and efficiently fix them.  
For help with troubleshooting Helion OpenStack 1.1 general issues, refer to:
[Troubleshooting Helion](http://docs.hpcloud.com/helion/openstack/services/troubleshooting/)
 
## Installing the cloud environment ##
Helion OpenStack installation requirements are fully documented in the support matrix at:
[Support Matrix](http://docs.hpcloud.com/helion/openstack/support-matrix/)
The process of building the seed host, creating the seed VM, the overcloud, and the undercloud are described at: 
[Helion installation overview](http://docs.hpcloud.com/helion/openstack/install/overview/)
At each step, you should have confirmation that what you have done works. At the conclusion of each major installation process, you will have confirmation that your installation works or does not. If you encounter issues, you should fix them before proceeding. After taking all these steps to resolve an error being reported, you may find that your cloud installation still does not work. This document will help you troubleshoot that experience. 

## Helion installation ##
By the time you are ready to start the Helion OpenStack installation process, you will have confirmed that your hardware and your Linux environments are ready for it. If you do this incorrectly, your installation will likely fail. 

The installation is mostly scripted such that if you run the script and if the outcome is as expected, you can reasonably expect that your installation is correct up to this point. If there are problems, you should be alerted that a problem has occurred. By the end of your installation procedure, you will either have a working Helion environment for your virtual, baremetal, KVM, or ESX environment or you should have the information you need to identify what needs fixing. 

### Standard procedures ###
To prevent known causes of failure, except where specifically noted otherwise, run your installation commands as admin by entering:

		`sudo su -` 

- Enable logging. Pipe your session to a log so you can retrieve error messages and notifications of any configuration results that did not yield the desired result.

- Verify that you have a network connection. Several installation steps require network communications. Make sure that all network infrastructure services are pingable and that forward and reverse lookup of DNS is working properly.

- Make sure the hardware clock on each machine in your cloud configuration is time synced and that all nodes report the same time within a time drift of no more than 10 minutes. The preferred timezone zone is UTC. 

- Do not misapply sizing units when calculating disk and memory size.  A kilobyte (KB) equals 1,000 bytes. A kibibyte (KiB) equals 1024 bytes.  KBs are decimal approximations of the binary value. Do not mix decimal and binary values. To see the available drive capacity for HP ProLiant Gen8 or higher drives, use the HP iLO GUI, system overview, storage tab.

- Monitor any exceptions generated by export commands. If you issue an export command that cannot be processed, you will see an error message. If you miss it from your terminal, it will be reported in your logs.

- Power off your overcloud servers before running the overcloud configuration script. The script expects the target servers to be off. When the script finishes configuring each server, it will bring it back online. When the script completes, you should have CLI output confirming the operational states of all your devices. You should also verify that they are all operational by checking their status LEDs (for physical installations). (The requirement to power down your servers is no longer a requirement, but it is still a best practice.)

- Log into your overcloud and undercloud Horizon instances and confirm that your servers are accounted for. 

- In general, HP recommends that you disable any firewalls inside of your image and use OpenStack security groups to restrict access to instances. The reason is that having a firewall installed on your instance can make it more difficult to troubleshoot networking issues if you cannot connect to your instance.

- DHCP servers configured on the undercloud/overcloud deployment network segment should be disabled, or the MAC addresses used for all nodes should be on the denied list for the DHCP server running on that network segment.

- For all Helion nodes, make sure only one network card is enabled. If you have multiple network cards, disable them in the server BIOS. For information on disabling interfaces in the BIOS, see the **Resolution** section for [PXE boot on target node keeps switching between interfaces](#PXE-boot-on-target).

## Troubleshooting the installation ##
The following content is the result of lessons learned from troubleshooting actual installations. As customers and HP support personnel report issues, this section will be updated.
**Note**: If you are doing a virtual install, HP recommends that you use `virt-manager` for troubleshooting. 

### Could not create the seed VM ###

For KVM installations: Make sure you have all the required software on the KVM host.

- Make sure you have qemu, openswitch, libvirt, and phython-libvert packages loaded on the KVM host. 
- Make sure you have the SSH public key. 
- Make sure you are installing on Ubuntu 13.10 or 14.04. 

For baremetal installations make sure you have the right system resources to support all of your baremetal VMs. 

### Could not access the KVM host on the seed VM ###

Check that you are exporting to the right bridge interface.

Make sure that the seed VM is the gateway. This is a bug in older builds. If you are using an older Helion OpenStack build (prior to release 1.01), see your release notes for more information. Also, verify that your routing is correct. If not, use route commands to fix it.


### Launching a new KVM virtual machine instance fails with `ERRORstate` ###
On the Nova compute server, check the `/var/log/nova/nova-compute.log` file for the message

	libvirtError: internal error no supported architecture for os type 'hvm'

This message indicates that the KVM kernel modules were not loaded. If you cannot start VMs after installation without rebooting, the permissions might not be set correctly. This can happen if you load the KVM module before you install `nova-compute`. To check whether the group is set to `kvm`, run:
 
	# ls -l /dev/kvm 

If it is not set to `kvm`, contact customer support.

### Cannot SSH into the seed VM ###
Check your IP addressing assignments. This could be a networking or Security groups issue.  To determine where the issue lies, you can, for example, use `ip netns` to ping the different interfaces along the route to the VM.

Also, check the state of the VM by entering:

	virsh list

### The under/overcloud fails when parsing the `basemetal.csv` file ###
There are a number of possible causes for this error:

- Check for proper .csv formatting. Look for extra spaces, special characters.
- Is the IPMI network accessible? You need to be able to access this network. Manually verify all login credentials.
- Check your IP addressing. One of the IP addresses may be incorrect.
- The baremetal.csv minimum node disk size is invalid. The minimum node disk size is 512G. If you specify a smaller size in your `baremetal.csv` file, the file will not process.
- The number of rows in your .csv file is smaller than expected. The `compute_scale` parameter reports more rows in your .csv file than are actually found. Resolve the discrepancy.
- Verify RAM size. You need at least 32Gb of RAM. 

### Cannot create the undercloud stack ###
This can happen because the installer times out or terminates abuptly.

#### Installer times out ####
Check that you are exporting to the right bridge interface. For example, if your external interface is `em2`, and not `eth0`, make sure the relevant variables are set correctly in your JSON file. Also:

- Check the iLO console for a `daemon.error tgtd`, meaning the installer is stuck. To fix this:
	- You can reboot.
	- If you have no local disk, use Ubuntu to DD the disk.
	- Examine the seed VM log file /var/log/upstartironic-conductor.log
	- Check the bootstrap OS to make sure you have enough disk space.
- If you see a read-only file system error on the iLO console, make sure the BIOS date/time stamp matches that on the seed VM. Use the `time sync`  or `ntpq p` command across all nodes to verify that time drift is within acceptable limits.
- If the iLO console reports that the boot is stuck but the network is booting, check the NIC order; check that the designated port is on a separate VLAN.
- If the iLO console reports a metadata access error, disable any DHCP servers on the same VLAN. 
- If the iLO console halts at `br-ctlplane` and you cannot reach the undercloud, and you are using Oneview, there is a patch for this. Or, remove Oneview. This problem can also be caused because there is more than one active connection. 

#### The installer terminates abruptly ####
This issue can happen:

- If your disk size is less than what you specify in the `baremetal.csv` file, reduce the size in your .csv file.
- If the server sees the SAN boot LUN as having an incorrect OS, fix it to be Linux OS.
- If the iLO console reports `Fatal PCI Express Device Error PCI Slot`, you need to reset iLO and clear the iLO event logs.

### Installer fails before the overcloud stack can be created ###
The reasons for this problem are:

- The failure happens at the Neutron Public Interface IP address. Fix the export variable (external VLAN support).
- The error is HTTP 500. Look for the error details in the `/var/log/upstart/os-collect-config` log. This will happen if you have a duplicate MAC address in your `baremetal.csv` file.
- The failure happens with IP addressing at the external VLAN. Check your VLAN tags and fix any IP addressing errors.

### Installer fails at the overcloud stack creation ###
As with problems creating the undercloud, this problem can happen because the installer times out or terminates abuptly.

#### Installer times out ####
Check that you are exporting to the right bridge interface. For example, if your external interface is `em2`, and not `eth0`, make sure the relevant variables are set correctly in your JSON file. Also:

- Check the iLO console for a `daemon.error tgtd`, meaning the installer is stuck. To fix this:
	- You can reboot.
	- If you have no local disk, use Ubuntu to DD the disk.
	- Examine the seed VM log file `/var/log/upstartironic-conductor.log`
	- Check the bootstrap OS to make sure you have enough disk space.
- If you see a read-only file system error on the iLO console, make sure the BIOS date/time stamp matches that on the seed VM. Use the time sync command.
- If the iLO console reports that the boot is stuck but the network is booting, check the NIC order; check that the designated port is on a separate VLAN.
- If the iLO console reports a metadata access error, disable any DHCP servers on the same VLAN. 
- If the iLO console halts at `br-ctlplane` and you cannot reach the undercloud, and you are using Oneview, there is a patch for this. Or, remove Oneview. This problem can be caused because there is more than one active connection. 
- If the Undercloud Nova list and Ironic node-list report node states as being `Active` and `Power ON`, the problem is somewhere in the overcloud Heat stack. To check this, run the `heat` `stack-show`, `event-list`, `event-show` commands. Check your logs. Also SSH into the overcloud nodes in your external VLAN and verify that routing is set correctly.

#### The installer terminates abruptly ####
This issue can happen:

- If your disk size is less than what you specify in the `baremetal.csv` file, reduce the size in your .csv file.
- If the server sees the SAN boot LUN as having an incorrect OS, fix it to be Linux OS.
- If the iLO console reports `Fatal PCI Express Device Error PCI Slot`, you need to reset iLO and clear the iLO event logs.
- If one or more time synced nodes do not power on, you should:
	- Use the `time sync` command on each node with the seed VM. 
	- Also reboot iLO and clear the iLO event logs. 
	- And halt the undercloud nova-compute and ironic-conductor processes and then power on the suspect overcloud node. Check the iLO console to verify that it is behaving correctly.
	- Looks at the `nova-compute`, `ironic-conductor`, and `heat-engine` logs of the suspect undercloud node.


### Installer fails after the overcloud stack is successfully created ###
The reasons for this problem are:

- You are stuck at init-keystone. Check that the firewall ports between the management and external VLANs are open. (This does not apply in HA configurations.)
Also you can bypass the firewall by creating the VLAN interface on the KVM host.

- You cannot create the Neutron network. Check if the IP address is a superset of the external network. You can export the `FIXED_RANGE_START`, `FIXED_RANGE_END`, and `FIXED_RANGE_CIDR` variables. You can also recreate the external network and default network to match your requirements.
- The image upload fails. You may have a race condition. Upload the image manually after the installation is complete.
- You cannot ping a launched VM. The VM did not get a DHCP issued IP address. You can:
	- Restart the `neutron-openswitch-agent` on the affected VM.
	- Restart the `neutron-dhcp-agent` on the controller node(s).
	- Run the `tcpdump` command and examine the output.

	Also check the image to make sure it is available.
- Demo VM creation fails. Skip the demo VM instance creation by entering `–skip-demo` on `installer.sh`.

### Installer completed correctly, but my Helion OpenStack cloud does not work ###
The likely reasons this happens are:

- You cannot SSH into or ping the VM. In this case:
	- Make sure your security group rules allow ICMP and SSH.
	- Check that the VM was assigned a DHCP IP address. 
	- Check the console.log file for metadata access failures.

- You cannot attach the Cinder volume. In this case:
	- Check the `nova-compute.log`.
	- Check the keystone-endpoint and Cinder configuration for the IP address of the ISCSI target.
	- There may still be a bug in Helion when configured in External VLAN mode. Check your Helion release notes for more information.

- When you try to launch a VM, you get a `no valid host found` error. In this case:

	- Is the compute node assigned? Check **Admin > Instances**. Also check the `nova-compute.log` on the suspect node. And if the node is not assigned, check the nova-scheduler on the controller node(s).
	- Is this a quota issue?
	- You may have a memory and disk resource allocation issue.

### Installer crashed - did not complete normally ###
Check elasticsearch and solver for corrupted data and remove it.
When you experience a crash, always check for corrupted data. Before you restart the installer, remove any instances of corrupted data or you risk subsequent installs failing because of corrupted data.


## Troubleshooting known issues ##
This section explains known issues and what experience has shown to be effective in resolving them.

This topic provides possible solutions to known issues.

- [Overcloud update failed because of nodes in maintenance state]({#maintmode})
* [Fatal PCI Express Device Error](#fatal-pci)
* [IPMI fails with error- unable to establish IPMI v2 / RMCP+ session](#IPMI-fails)
* [Failure of update on the overcloud](#failure-update-overcloud)
* [PXE boot on target node keeps switching between interfaces](#PXE-boot-on-target)
* [Controller nodes unreachable](#OC-controllers-unreachable)
* [Ephemeral partition fails to unmount](#ephemeral-fail-unmount)
* [BIOS clocks are not set to correct date and time across all nodes](#BIOS-clocks-are-not-set-to-correct-date)
* [iLO console shows hLinux daemon.err tgtd while PXE booting](#ilo-console)
* [iLO console shows null waiting for notice of completion while PXE booting](#ilo-show-null)
* [Failure of hp_ced_installer.sh](#failure-installer)
* [Failure of Seed Installation](#seed-install-failure)
- [Inconsistent Failures in RabbitMQ](#rabbit-failure)
- [Accidental Reboot of Overcloud Controller]({#reboot-accident})
- [Server Power Stuck at Reset](#reset-stuck)
- [Network Bridge Persists After Uninstall](#bridge-persists)
- [Seed Generates Restarting libvertd Script Error]({#seed-error})
- [Node in Error Cannot be Controlled by Ironic or Nova](#ironic-error)
- [Debugging a Windows&#174; Server OS Using a Serial Connection](#windebug)
- [Using the sample DNS JSON file variable names for setup causes errors]({#DNSvariables})
- [Seed cloud host cannot be reached after it is rebooted]({#seednoshow})



<hr />
### Overcloud update failed because of nodes in maintenance state {#maintmode} ###

**System Behavior/Message**

Do NOT proceed with your installation/update if any of the undercloud/overcloud nodes are down or in a maintenance state.
To check if nodes are in maintenance state, source the undercloud credentials:

	root@hLinux:~# ironic node-list

The following is sample output:


	+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
	| UUID | Instance UUID| Power State | Provisioning State | Maintenance |
	+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
	| def2a648-8b8f-493c-8f36-40c898392ec9 | cd3559b6-0fdd-49d6-b7d1-3cc95615e7fc | power on| active | False   |
	| 36854b32-10f8-4e41-8b0b-04901935fabb | 9af2f1d3-7299-4b69-b01e-71d0878c198e | power on| active | False   |
	| 54e0af81-4a17-41da-ab67-267eb07bd1e1 | b3e85509-4f96-4421-9e8b-d04b51297102 | power on| active | False   |
	| 3fb56d36-5ba7-46e7-9464-5252348d5252 | 3d125eee-9712-4888-9862-856627c891fe | power on| active | False   |
	| e21c7122-9a6e-4b56-b66c-846d28fab118 | f2e2bac2-1700-476a-ad53-23a9392d78b1 | power on| active | False   |
	| 964c36a9-53cb-491f-b12c-a24bf3a4ec6e | 012dd248-de6a-43a9-b3f7-0c2fc4b16a48 | power on| active | False   |
	+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+

Verify that all Ironic nodes show `False` for Maintenance mode.

#### Updating Baremetal Seeds ####

The section applies when the seed IP address is the default gateway for the baremetal network (brbm) This is normally the case for virtual installs. For baremetal installs you should not be using the seed as the default gateway for brbm. In this case, the following does not apply.

During seed update, it needs to be shut down and then started up with the updated image. If the seed is the default gateway this mean the undercloud can no longer talk to the overcloud (that is, the network is interrupted). This interruption normally lasts about five minutes. This interruption can cause Ironic running on the undercloud to put some overcloud nodes into maintenance state as Ironic can no longer contact them

**Resolution**

To mitigate nodes being placed in maintenance:

1. Check if any nodes are in maintenance before running the seed update.
1. Fix any nodes that are found to be in maintenance state.
1. Run the seed update.
1. After the seed update completes, check if any nodes are in maintenance state because of the seed update.

	To check if any nodes are in maintenance state, enter:

		root@hLinux:~# ironic node-list

	The following is sample output.

		+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
		| UUID                                 | Instance UUID                        | Power State | Provisioning State | Maintenance |
		+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
		| def2a648-8b8f-493c-8f36-40c898392ec9 | cd3559b6-0fdd-49d6-b7d1-3cc95615e7fc | power on    | active             | False       |
		| 36854b32-10f8-4e41-8b0b-04901935fabb | 9af2f1d3-7299-4b69-b01e-71d0878c198e | power on    | active             | False       |
		| 54e0af81-4a17-41da-ab67-267eb07bd1e1 | b3e85509-4f96-4421-9e8b-d04b51297102 | power on    | active             | False       |
		| 3fb56d36-5ba7-46e7-9464-5252348d5252 | 3d125eee-9712-4888-9862-856627c891fe | power on    | active             | False       |
		| e21c7122-9a6e-4b56-b66c-846d28fab118 | f2e2bac2-1700-476a-ad53-23a9392d78b1 | power on    | active             | False       |
		| 964c36a9-53cb-491f-b12c-a24bf3a4ec6e | 012dd248-de6a-43a9-b3f7-0c2fc4b16a48 | power on    | active             | True        |
		+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+

1. If a node is found to have been put in maintenance state, use the `ironic node-set-maintenance` command to remove the flag. For example:

		ironic node-set-maintenance 964c36a9-53cb-491f-b12c-a24bf3a4ec6e

2. Proceed with your undercloud and overcloud update.

#### If all else fails ####
If you need further assistance, contact [HP Customer Support](http://www.hpcloud.com/about/contact). Before contacting HP Customer Support, run the `run-sosreport` command on the affected nodes. The ` run-sosreport ` command will gather specific reports that will help diagnose your issue. You do not need to be admin (root) to run this command.


<hr />
### Fatal PCI Express Device Error {#fatal-pci} ###

**System Behavior/Message**

When installing on HP ProLiant SL390s and HP ProLiant BL490d systems, the following error has occasionally occurred:

    `Fatal PCI Express Device Error PCI Slot ? B00/D00/F00`


**Resolution**

If you get this error, reset the system that experienced the error:

   1. Connect to the iLO using Internet Explorer:
        `https://<iLO IP address>`
   2. Navigate to Information / Diagnostics.
   3. Reset iLO.
   4. Log back into the iLO after 30 seconds.
   5. Navigate to Remote Console / Remote Console.
   6. Open the integrated remote console (.NET).
   7. Click Power switch / Press and Hold.
   8. Click Power switch / Momentary Press, and wait for the system to restart.

   The system should now boot normally.

<hr />
### IPMI fails with an error- unable to establish IPMI v2 / RMCP+ session {#IPMI-fails} ###

**System Behavior/Message**

When installing on HP ProLiant BL490c systems, the following error has occasionally occurred:


    unable to establish IPMI v2 / RMCP+ session

**Resolution**

If you get this error, perform the following steps:

1. Ensure that the iLO user has administrator privileges, which is required by the IPMITOOL.
2. To check from the iLO remote console, reboot the server and press **F8** to get to iLO Management screen.
3. Click **User** in the menu-bar and select **Edit**. Edit User pop-up box displays .
4. If you are using a BL server in the QA C7000 enclosure, select the **cdl** user to edit.
5. Use &darr;(down arrow key) to select **Administer User Accounts**. 
6. Use the space bar to set the value to **YES**.
7. Select **F10** to save.
8. Click **File** and select **Exit** to close.
<br /><br />
<hr />


### Failure of update on the overcloud {#failure-update-overcloud} ###

**System Behavior/Message**

Update overcloud fails with the following error:

<pre>Inconsistency between heat description ($OVERCLOUD_NODES) and overcloud configuration ($OVERCLOUD_INSTANCES)</pre>

**Resolution**

If you get this error, perform the following steps:

1. Log in to Seed.
 
		ssh root@<Seed IP address>

2. Edit the */root/tripleo/ce_env.json* file and update the variables **build&#95;number** and **installed&#95;build&#95;number** to the correct value. <!-- (CORE-1697) --> They may or may not match but the value cannot be NULL.

3. A sample section of the *ce&#95;env&#95;json* file showing that the **build&#95;number** is changed from NULL to a valid value.

		"host-ip": "192.168.122.1", 
		"hp": { 
		     "build_number": 11, 
		     "installed_build_number": 11 
		...
4. Run the installer script again to update the overcloud. During the installation, the build specified by build&#95;number is installed.
 
		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log

<br />
<hr />


### PXE boot on target node keeps switching between interfaces {#PXE-boot-on-target} ###

**System Behavior/Message**

When node boots up on iLO console it shows node waiting for PXE boot on multiple NICs.


**Probable Cause**

 Multiple NICs are enabled for Network Boot.


**Resolution**

* Reboot the node, using **F9** to get to the BIOS configuration.
* Assuming NIC1(eth0/em1) for the node is connected to a private network shared across node enable it for Network Boot.
* Select System Options > Embedded NICs.
* Set NIC 1 Boot Options = Network Boot.
* Set NIC 2 Boot Options = Disabled.

<hr />
### Controller nodes are unreachable {#OC-controllers-unreachable} ###

**System Behavior/Message**

The returns for `pxe-boot` and `nova list` lists the controller nodes as `active` but the Heat deployment fails, and users cannot log into the nodes.
 For example, the three overcloud Controllers fail as follows:

    23:01:35 | controller1AllNodesDeployment | a60064ab-aa33-4d47-80c9-0e04561bb591 | OS::Heat::StructuredDeployment | CREATE_FAILED | 2015-03-27T22:08:33Z |
    23:01:35 | controller2AllNodesDeployment | 8386d72a-668a-459c-af14-8f67a76f496a | OS::Heat::StructuredDeployment | CREATE_FAILED | 2015-03-27T22:08:36Z |
    23:01:35 | controller0AllNodesDeployment | a0e184fe-dddf-49b3-91c7-8a63a89687ef | OS::Heat::StructuredDeployment | CREATE_FAILED | 2015-03-27T22:08:37Z |
    

**Resolution**

This could be because your controler nodes are failing to get
 their metadata from Nova in the undercloud.

You can confirm this by looking at the overcloud hostname. If the overcloud nodes hostname is `hlinux` instead of the expected hostname, for example, `overcloud-ce-controller-controller0-26x25y6vuxrb` nodes are likely not getting their metadata from Nova.

You an further check for the absence of the meta-data transfer messages in `/var/log/nova-api.log` file on the undercloud node. 

The following is an example of the expected messages. If the `nova-api.log` does not contain messages like this for each overcloud host, the metadata is not being transferred:

	2015-04-01 17:06:33.682 5296 INFO nova.metadata.wsgi.server [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 10.11.39.138 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 386 time: 0.1749260
	2015-04-01 17:06:33.684 5296 INFO nova.api.ec2 [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 0.310s 10.11.39.138 GET /latest/meta-data/ami-id None:None 200 [python-requests/2.5.1 CPython/2.7.8 Linux/3.14.29-4-amd64-hlinux] text/plain text/plain
	2015-04-01 17:06:33.684 5296 INFO nova.metadata.wsgi.server [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 10.11.39.138 "GET /latest/meta-data/ami-id HTTP/1.1" status: 200 len: 153 time: 0.0008781
	2015-04-01 17:06:34.083 5296 INFO nova.api.ec2 [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 0.338s 10.11.39.138 GET /latest/meta-data/ami-launch-index None:None 200 [python-requests/2.5.1 CPython/2.7.8 Linux/3.14.29-4-amd64-hlinux] text/plain text/plain
	2015-04-01 17:06:34.084 5296 INFO nova.metadata.wsgi.server [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 10.11.39.138 "GET /latest/meta-data/ami-launch-index HTTP/1.1" status: 200 len: 141 time: 0.0010879
	2015-04-01 17:06:34.086 5296 INFO nova.api.ec2 [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 0.225s 10.11.39.138 GET /latest/meta-data/ami-manifest-path None:None 200 [python-requests/2.5.1 CPython/2.7.8 Linux/3.14.29-4-amd64-hlinux] text/plain text/plain
	2015-04-01 17:06:34.086 5296 INFO nova.metadata.wsgi.server [req-f899b10b-8bb7-46cd-8a09-d8b36608c957 None] 10.11.39.138 "GET /latest/meta-data/ami-manifest-path HTTP/1.1" status: 200 len: 145 time: 0.0007529


To fix this problem, make sure that you have only one functional
 interface on your undercloud node.  

A workaround to disabling an interface, is to use the `$ifdown <interface>` command. For example to disable `eth1` on the undercloud, enter:

	$ifdown eth1

 The recommended procedure is to permanently disable the interface in the BIOS. Refer to the **Resolution** section in the previous troubleshooting issue for these instructions: [PXE boot on target node keeps switching between interfaces]({#PXE-boot-on-target})
<hr />

### An ephemeral partition fails to unmount ## {#ephemeral-fail-unmount} ###

**System Behavior/Message**

If during an update, you receive the following error message:


        The ephemeral storage of this system failed to be cleaned up properly and processes or files are still in use. The previous ansible play should have information to help troubleshoot this issue.



**Probable Cause** <br>
The most likely reason the partition failed to unmount is due to a process still holding a file handle on the partition. This normally happens when a process has failed to stop (where stop is issued by the Ansible playbook) or has been restarted (by `cron` for example) after the stop was issued. 

**Resolution**

To find out which process is still running:

1. Log onto the failed nodes and enter:

	    ssh heat-admin@10.23.67.139 
    	$ sudo su -
    	root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# lsof /mnt
    	COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME 
		stunnel4 33047 root   24w   REG8,1  4812923 62128141 /mnt/state/var/log/stunnel4/helion_stunnel.log


	In the above example `stunnel4` has failed to stop. 

2. To stop the process and verify nothing is holding a file reference on `/mnt` enter:    
    
	    root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# pkill stunnel4
	    root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# lsof /mnt
	    root@overcloud-ce-controller-swiftstorage0-h7t3467lf7fx:~# 
    
3. Follow the procedure to restart `mysql` and `rabbitmq` from the *Tripleo-ansible Troubleshooting Guide*.
 Once restarted, re-run the overcloud ansible playbook and not the `update_oc.sh` script


<br /><br />
<hr />


### BIOS clocks are not set to correct date and time across all nodes {#bios-clocks-are-not-set-to-correct-date} ###


**System Behavior/Message**

Nodes PXE boot but ISCSI does not start.


**Probable Cause**

Time and date across nodes are incorrect.


**Resolution**

Reboot the node, using **F9** to get to the BIOS configuration. BIOS date and time are set correctly and the same on all the systems.

* Select Date and Time.
* Set the Date.
* Set the Time.
* Use the &lt;ENTER> key to accept the new date and time.
* Save the BIOS, which reboots the node again.
* Once the node has rebooted, you can confirm its data and time from the iLO Overview.
<br /><br />
<hr />

### iLO console shows hLinux daemon.err tgtd while PXE booting {#ilo-console} ###

**System Behavior/Message**

PXE boot gets stuck after `daemon.err tgtd`


**Probable Cause**

Node does not have enough disk space


**Resolution**

* Check if target node has disk space mentioned in `baremetal.csv` and is greater than Node_min_disk mentioned in `tripleo/tripleo-incubator/scripts/hp_ced_functions.sh`.
* If disk space is less than Node&#95;min&#95;disk, change Node&#095;min&#095;disk along with DISK&#95;SIZE in `tripleo/tripleo-incubator/scripts/hp_ced_list_nodes.sh` on Seed.
* Re-run the installation script.
<br />
<hr />

### iLO console shows null waiting for notice of completion while PXE booting {#ilo-show-null} ###

**System Behavior/Message**

Node is powered on and PXE booted but it is powered off after `daemon.err` and stack create fails.

**Probable Cause**

Node does not have enough disk space. SAN boot is enabled for node or local disk is not attached to `/sda`

**Resolution**

Installer expects that SAN boot option is disabled for nodes. Verify that SAN boot is disabled for BL 490c.

Also, you can boot the targeted BL490c with Ubuntu or any Linux ISO to see what device is shown as the local disk. For the installer it should be `/sda`.

<hr />

### Failure of hp_ced_installer.sh {#failure-installer} ###

**System Behavior/Message**

`hp_ced_installer.sh` fails because of bad characters in the `baremetal.csv`.


**Resolution**

Verify `baremetal.csv` does not contain any empty lines or special/corrupt characters.
<br /><br />
<hr />
### Failure of Seed Installation {#seed-install-failure} ###


**System Behavior/Message**

Seed installation fails with no space left on device.


**Resolution**

Verify the tripleo directory user owner and group. It must be **root:root**. 
In case it is not set as **root:root** then change it to root using: 

	chown root:root tripleo

<hr />
### Inconsistent Failures in RabbitMQ {#rabbit-failure} ###
**System Behavior/Message**

Inconsistent RabbitMQ failure seen on controller nodes while listing queues 

	rabbitmqctl list_queues

**Resolution**

Restart the RabbitMQ service.


<hr />
### Accidental Reboot of Overcloud Controller {#reboot-accident} ###

If the overcloud controller is rebooted (due to a power issue, hardware upgrade, or similar event), OpenStack compute tools such as `nova-list` might report that the VMs are in an ERROR state, rendering the overcloud unusable. 

To restore the overcloud to an operational state, follow the steps below:
 
  1. As user `root` on the overcloud controller you must:
  
     A. Run the `os-refresh-config` scripts:

        os-refresh-config

       B. Restart the `mysql` service:

        service mysql restart

       C. Re-run the `os-refresh-config` scripts:

        os-refresh-config

       D. Restart all Networking Operations (Neutron) services:

        service neutron-dhcp-agent restart
        service neutron-l3-agent restart
        service neutron-metadata-agent restart
        service neutron-openvswitch-agent restart
        service neutron-server restart

  2. On each overcloud node, restart the Neutron and Nova services:
  
        sudo service neutron-openvswitch-agent restart
        sudo service nova-compute restart
        sudo service nova-scheduler restart
        sudo service nova-conductor restart


<hr />
### Server Power Stuck at Reset {#reset-stuck} ###
The installer uses IPMI commands to reset nodes and change their power status. Some systems change to a state in which the `Server Power` status as reported by the iLO is stuck in `RESET`. If this occurs, you must physically disconnect the power from the server for 10 seconds. If the problem persists after that, contact HP Support as there might be a defective component in the system.

<hr />
### Network Bridge Persists After Uninstall {#bridge-persists} ###
On the system on which the installer is run, the seed VM's networking is bridged onto the external LAN. If you remove HP Helion OpenStack, the network bridge persists.

To revert the network configuration to its pre-installation state, run the following commands as user `root`: 

	ip addr add 192.168.185.131/16 dev eth0 scope global
	ip addr del 192.168.185.131/16 dev brbm
	ovs-vsctl del-port NIC
where

* eth0 is the external interface     
* 192.168.185.131 is the IP address on the external interface - you should replace this with your own IP address.
* The baremetal bridge is always called 'brbm'

<hr />

<!-- Removed per Ramakrishna Bhupathi. Not required in 1.1  
### Modify the Heat Configuration File Before HP Helion OpenStack Installation {#heat-config} ###

Before you install the HP Helion OpenStack DNSaaS or if you want to use Heat with HP Helion OpenStack, you **must** modify the /etc/heat/heat.conf file on the overcloud controller as follows.

<span style="color:red">**Important**:</span> The installation of the HP Helion OpenStack DNSaaS **fails** if you do not make these modifications.

**Note**: You must have admin ssh access to the overcloud controller.

1. Make sure the IP addresses in the following settings reflect the IP address of the overcloud controller, for example:
    
		heat_metadata_server_url = http://192.0.202.2:8000
		heat_waitcondition_server_url = http://192.0.202.2:8000/v1/waitcondition
		heat_watch_server_url = http://192.0.202.2:8003

2. Save the file.

3. Restart the Heat-related services &#8211; heat-api, heat-api-cfn, heat-api-cloudwatch, and heat-engine.

4. Ensure there are no Heat resources in an error state, and then delete any stale or corrupted Heat-related stacks.
<br />

<hr />

-->


### Seed Generates Restarting libvertd Script Error  {#seed-error} ###

**System Behavior/Message**

When starting your seed VM, if the script fails showing:

    + echo 'Restarting libvirtd...'
    Restarting libvirtd...
    + '[' -e /etc/debian_version ']'
    + service libvirt-bin restart
    stop: Unknown job: libvirt-bin
    start: Unknown job: libvirt-bin


**Resolution**

This problem can be caused by your use of `sudo –E` when executing the `hp_ced_host_manager`. Run  `hp_ced_host_manager ` without specifying `sudo –E`.

<hr />

<br />

### Node in Error Cannot be Controlled by Ironic or Nova {#ironic-error} ###

**System Behavior/Message**

Node goes into ERROR state and/or Ironic commands may result in a 400 or a 403 error code.
A a node being controlled by the seed or undercloud cannot be controlled either through Ironic or Nova.

**Resolution**

Run the following command:

	$mysql --defaults-file=/mnt/state/root/metadata.my.cnf   
	--socket /var/run/mysqld/mysqld.sock ironic -e 
	"select reservation from nodes;"


If the return form the command was not Null, repeat the command to see if Ironic is legitimately holding a lock.

If on the second attempt the lock was was still not NULL, run the following:

	$mysql --defaults-file=/mnt/state/root/metadata.my.cnf \
    --socket /var/run/mysqld/mysqld.sock ironic \
     -e 'update nodes set reservation=NULL where reservation is not null;'

Repeat the tests on the Ironic reservation:

	$mysql --defaults-file=/mnt/state/root/metadata.my.cnf   
	--socket /var/run/mysqld/mysqld.sock ironic -e "select reservation from nodes;"

You should now once again be able to use Ironic commands for the node.

If you were using Nova and the node when into ERROR, run 

	nova reset-state 

to clear the error and rerun the original Nova command.


<hr />

### Debugging a Windows&#174; Server OS Using a Serial Connection {#windebug} ###
Debugging a Windows Server OS running as a Nova Compute VM on HP Helion OpenStack will require setting up a serial connection between the VM and a debug host. Once a serial connection has been established between the VM and debug host, which is usually over the COM2 port, the Windows Debug utility will function properly.

**Before you Begin**: 



- Deploy a [debug host on the same compute host](http://docs.openstack.org/user-guide-admin/content/specify-host-to-boot-instances-on.html) as the target Windows instance.
- The user will need to be able to log into the host with *superuser* privileges. To safely gain superuser privileges, add `sudo` as a prefix to each command executed.

**Set Up Serial Communication**

1. Log on to the Compute hosts where the Windows instances (target and debug hosts) are deployed.
2. Install the *socat* utility on the compute host. 
	
	The Debian package can be copied to the host and installed using:
	 
	 	sudo dpkg -i socat_1.7.2.4-1+b1_amd64.deb
	 
	or, if apt repositories are accessible from the host, use:
	 
		sudo apt-get install socat

3. Get the serial source path of the target and the debug host VMs.
	1. Get the libvirt xml of the target and the debug instance by running the command
	 
			sudo virsh dumpxml $vmname > $filename.xml

		Where *$vmname* is the name of the instance and	*$filename.xml* is the name of the output file to which the libvirt.xml will  be written.

	2. Look in the *filename.xml* file for the value of the **Source Path** for the target host. This value is found under the `Serial type='pty'` section. <br />Example:
	
			 ...
			<serial type='pty'>
				<source path='/dev/pts/4'/>
				<target port='1'/>
				<alias name='serial1'/>
			</serial>
			...

		In this example the value is  */dev/pts/4.*

	1. Repeat these steps to retrieve the serial source path value for the debug host. 
4.	Run the `socat` command to establish connection between the instances. In this example, the source path values were */dev/pts/4* and */dev/pts/5*. 
	
		sudo /usr/bin/socat /dev/pts/4 /dev/pts/5

<hr />

### Using the sample DNS JSON file variable names for setup causes errors {#DNSvariables} ###
**System Behavior/Message**

DNS nameserver is not set up properly.


**Probable Cause**

Using the sample DNS JSON file variable names for setup causes errors.


**Resolution**

Configure your DNS servers using the following values:

`seed_server` - Use this variable to specify the DNS server for the seed node (do not use seed_nameserver).

`undercloud_server` - Use this variable to specify the DNS server for the undercloud node (do not use undercloud_nameserver).

`overcloud_server` - Use this variable to specify the DNS server for the overcloud nodes (do not use overcloud_nameserver).
		
	Do not copy these addresses they are only examples

	"dns": {
		"seed_server": "19.65.175.150:8080",
		"undercloud_server": "19.65.175.150:8080",
		"overcloud_server": "19.65.175.150:8080",
	}

<hr />

### The seed cloud host cannot be reached after it is rebooted {#seednoshow} ###
**System Behavior**

 After the seed cloud host is rebooted, the networking setup does not persist and the seed cloud host cannot be reached.

**Probable Cause**
 
 The networking setup on the FCP seed cloud host must be set up in a particular way. Otherwise, the seed cloud host cannot be reached after it is rebooted. 

**Possible Resolution**
 
 Change the network configuration in the interfaces file to a configuration that supports persisting the network setup.

**On Ubuntu/Debian systems:**

- Define the primary interface in its own configuration file in `etc/network/interfaces.d/.` 
 
	For example, etc/network/interfaces.d/eth2.cfg 

- Alternatively, define the primary interface in /etc/network/interfaces and format the file so it has a line separator between each interface definition. (See man files for an example.)

**On CentOS systems:**

- Ensure that every interface is defined in its on ifcfg-<interface> file within `/etc/sysconfig/network-scripts/`



<hr />
<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593;</a>


