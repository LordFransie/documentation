---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Troubleshooting VSA"
permalink: /helion/openstack/1.1/services/troubleshooting/vsa/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Paul F

---
<!--PUBLISHED-->

<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--

<p style="font-size: small;"> <a href="/helion/openstack/1.1/services/object/overview/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/services/overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/services/reporting/overview/"> NEXT &#9654</a> </p> -->


# HP Helion OpenStack&#174; 1.1: Troubleshooting VSA

HP Helion OpenStack&#174; is an OpenStack technology coupled with a version of Linux&#174; provided by HP. This topic describes all the known issues that you might encounter. To help you resolve these issues, we have provided possible solutions.

Troubleshooting issues related to HP StoreVirtual VSA environments.


* [Failure to retrieve netmask from vsa-bridge](#fails-retrieve-netmask)
* [Installation script detects more than 7 available drive](install-script-detect)
* [Failure of script due to less than two drives](#failure-script)
*  [Cannot enable AO as only one disk is available](#cannot-enable-ao)
* [Unable to update the default input json file ](#unable-update-json)
* [Virtual bridge creation failed for interface <NIC>](#fail-virtual-bridge)
* [Creation of storage pool failed](#storage-pool-fail)
* [Failed during post VSA deployment](#post-vsa-fail)
*  [vsa&#95;network cannot be destroyed](#vsa-network)
* [vsa&#95;storage&#95;pool pool cannot be destroyed](#vsa-pool-cannot-destroy)
* [Overcloud updates fail because VSA VMs not responding to virsh stop](#virshfail)
* [Public interface not set and is ignored during overcloud update](#publicinterfaceset)


###Failure to retrieve netmask from vsa-bridge {#fails-retrieve-netmask}

**System Behavior/Message**
 
Cannot retrieve netmask from interface vsa-bridge

**Probable Cause**

VSA deployment script determines the net-mask and gateway details from the provided interface. When there is no IP address assigned to the VSA bridge, this error may occur.

**Resolution**

To resolve this issue, perform the following steps:

* Check whether the IP address is allocated for the VSA bridge 

* Verify the VSA IP address by using the following command:

  		ifconfig vsa-bridge

<br />
<hr />

###Installation script detects more than 7 available drive {#install-script-detect}

**System Behavior/Message**

Maximum supported devices 7.

**Probable Cause**

This issue occurs when there are more than 7 available drives detected by the installation script to deploy StoreVirtual.

**Resolution**

Perform the following steps:

* HP StoreVirtual VSA supports up to 7 disks

* Execute `fdisk &#45;l` and check for number of available drives in the machine other than `/dev/sda`

<br />
<hr />

###Failure of script due to less than two drives {#failure-script}

**System Behavior/Message**

Minimum number of disks must be 2. No disks are available.

**Probable Cause**
When there are less than two drives in the machine, the script will fail to execute.

**Resolution**

To resolve, perform the following steps:

* Execute `fdisk &#45;l`

* Minimum two drives and maximum of 7 drives should be available for the StoreVirtual deployment other than boot disk(`/dev/sda`)

* At least three drives required for enabling AO
<br />
<hr />

### Cannot enable AO as only one disk is available {#cannot-enable-ao}

**Probable Cause**

For Adaptive Optimization to be enabled, at least three drives must be available. `/dev/sdb` must be SSD drive(Tier 0) and the remaining will be Tier 1.

**Resolution**

To resolve the issue, do the following:

* Use RAID controllers to create RAID groups.

* Ensure that you create the RAID group for SSD drives immediately after creating the RAID group for boot volume. For example: If three RAID groups are to be created. The following is recommended :
	
	* **Step 1** : Create the first RAID group for HDD drives and mark this as boot volume(/dev/sda)

	* **Step 2**: Create the second RAID group for SSD drives which should be used as Tier 0 for AO (/dev/sdb)

	* **Step 3**: Create the third RAID group for HDD drives which will be used as Tier 1(/dev/sdc)

<br /><hr />


### Unable to update the default input json file {#unable-update-json}

**System Behavior/Message**

Parsing the default JSON file failed. Unable to update the default input json file.

**Probable Cause**

The script will parse the configuration file and update the values based on the network and configuration files.

**Resolution**

Perform the following steps:

* Verify whether the JSON content is valid in the following files:

	* `/home/vsa-installer/pyVins/etc/vsa/vsa_config.json`

	* `/etc/vsa/vsa_network_config.json`

<br /><hr />
###Creation of storage pool failed {#storage-pool-fail}

**Probable Cause**

Virtual storage pool will be created for placing the extracted VSA VM image. The storage pool will be created based on local directory  on `/mnt/state/vsa-kvm-storage`

**Resolution**

Perform the following steps:

* Check whether `/mnt/state/vsa-kvm-storage` directory is available.

* Verify for available space to create storage pool in the system.

* Check the libvirt logs for more errors

Refer `/var/log/libvirt/libvirt.log` on VSA system.
 
<br /><hr />

###Failed during post VSA deployment {#post-vsa-fail}

**Probable Cause**

The script will persist required files in `/mnt/state/vsa` which will be used for recreating the VSA VM during re-imaging scenario

**Resolution**

This error will occur if the script fails to find `network_vsa.xml`, `storagepool_vsa.xml` and other configuration files which has to be preserved.

* Check for the configuration files on &rdquo;/&lsquo; path.

* On success, the script updates the `/mnt/state/vsa/vsa_config.json` file with the updated and created time.

<br /><hr />

###VSA installation failed {#vsa-install-fail}

**Probable Cause**

When VSA installation fails for any of the above reasons, the script will rollback the network and storage pool.

**Resolution**

Verify the `/installer.log`

<br /><hr />


###vsa&#95;network cannot be destroyed {#vsa-network}

**Probable Cause**

VSA network will be destroyed when the VSA installation fails.

**Resolution**

Perform the following steps:

* Check whether the network is already undefined

* Check whether the network name in `<PYVINS_DIRS>/etc/vsa/vsa_config.json` is the same as in the output of `virsh net-list -all` command

<br /><hr />

### vsa&#95;storage&#95;pool pool cannot be destroyed {#vsa-pool-cannot-destroy}

**Probable Cause**

The storage pool will be destroyed when VSA installation fails

**Resolution**

Perform the following:

* Verify whether the storage pool is already undefined

* Verify whether the pool name is same as in `<PYVINS_DIRS>/etc/vsa/vsa_config.json`

* Virsh command to list the pools

        Virsh pool-list --all

<br /><hr />


### Failure of newly added compute or VSA node during Scale-out {#failnew}

**System Behavior/Message**

The newly added compute node or VSA node fails during scale-out.

**Resolution**

You must remove a failed compute or VSA node before adding a new compute node.

Perform the following steps to remove a failed compute node:

1. Run `heat stacklist`on the undercloud node and search for the failed stack.

2. Delete the failed stack using the following command:

		# heat stackdelete <stackname or uuid>

3. List the newly added nova node which is created during scale-out.

		# novalist

4. Execute the following command to delete nova node. Node name and the Node ID can be obtained from the above steps.

		# nova delete <node name or node id>

5. View a newly added node using the following command:

		# ironic nodelist

6. If newly added node is in ERROR state or if the maintenance mode is True then remove those node(s) using the following command.

		# ironic nodedelete <uuid>

	where uuid is the ID of the node

<br /><hr />


### Overcloud updates fail because VSA VMs not responding to virsh stop {#virshfail}
<!-- DOCS-1067 -->

**System Behavior/Message**

Overcloud updates require that all VMs be stopped before an update can proceed. During an overcloud update, you may find that you are not able to stop a VSA VM (by issuing a `virsh stop` command). This causes the update to stop.   

**Resolution**

To fix this problem, you can use the VSA command-line interface (CLIQ) to shut down VSA on the VM that is not responding to a shut down command. Access the CLIQ by running SSH to port 16022 on the Virtual IP for the VSA server. If you do not know the IP address/username/password of the VSA system, look in ` /etc/cinder/cinder.conf` on controller0 on the overcloud.

To shut down VSA, enter:
	
    
	    ssh -l <username> <IP> -p 16022
	    ClIQ> shutdownNsm action=Shutdown
        
Proceed with the update.

<br /><hr />


### Public interface not set and is ignored during overcloud update {#publicinterfaceset}
<!--DOCS1240 -->

**System Behavior/Message**

As of Helion OpenStack 1.1,  when you install Helion OpenStack, if you do not specify the public interface for VSA (with the `VSA_PUBLIC_INTERFACE` parameter), then updates will not take this parameter into account.


**Resolution**

To fix this problem, manually update `VSA_PUBLIC_INTERFACE` in the `~/tripleo/overcloud-env.json` file with the interface name, for example, `eth1`.


<HR>


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593;</a>


