---
layout: default
title: "HP Helion OpenStack&#174;: Enabling VLAN Provider Network in HP Helion OpenStack"
permalink: /helion/openstack/1.1/vlan/provider/network/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Paul F

---
<!--UNDER REVISION-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>


<p style="font-size: small;"> <a href="/helion/openstack/1.1/install/post-kvm/">&#9650; Post-Installation for KVM Hypervisor</a></p> 
 
# HP Helion OpenStack&#174; 1.1: Enabling VLAN Provider Network in HP Helion OpenStack 
[See the Helion OpenStack 1.0 version of this page](/helion/openstack/vlan/provider/network/)

This page provides a detailed description to enable VLAN Provider Network in KVM Cloud Type.

<!---HP Helion &#174;OpenStack can be deployed in multiple ways to fulfill certain requirements using an installer. Installers depend on Virtual Extensible Local Area Network (VxLAN) or Generic Routing Encapsulation (GRE) to isolate tenants which is an important requirement.  These two latest networking technologies have become de-facto standards for installers because they ease infrastructure readiness requirements while providing tenant isolation, independent of any hardware (Switch/Router) configuration.-->


HP Helion OpenStack defaults to VxLAN to support tenant network isolation in a KVM Cloud Type. <!---However, we need to deploy Helion Cloud to customers desiring to migrate gradually from legacy VLAN to VxLAN, a non-default install feature. This whitepaper walks through a way to configure Helion OpenStack tenant networks to use VLAN Provider Network.--> The deployment of HP Helion OpenStack&#174; enables  tenant's virtual machines hosted in a legacy infrastructure and/or based on VMWare ESX to communicate to a virtual machine running in HP Helion OpenStack. <!---Typically, a Hybrid Application Deployment across two or more Infrastructure Providers (one being Helion OpenStack).-->

## Deployment Diagram
The following deployment diagrams are based on the assumption that the network infrastructure is carved out in such a way that it allows a range of tagged VLANs through the switches and their subnets are routed to the right destination. 

<a href="javascript:window.open('/content/documentation/media/vlan.provider.network.logical.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Deployment diagram depicting VLAN Provider Network with VSA nodes as Block storage backend. (Opens in a new window)</a>


<a href="javascript:window.open('/content/documentation/media/vlan.network.layout.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Network Layout. (Opens in a new window)</a>

## Install Steps

This section describes the solution to set up the Overcloud Neutron Network to provide tenant network isolation by means of VLAN, instead of the default VxLAN. 

The following prerequisites should be fulfilled to setup the Overcloud Neutron in VLAN:

1. Pass the right configuration parameters to the installer and templates for setting up the neutron configuration files: 

	* `/etc/neutron/plugins/ml2/ml2_conf.ini`- sets up the tenant network type, provides the VLAN ranges and maps to the physical bridge.

	* `/etc/neutron/dhcp-agent.ini`- enables Metadata Server access through DHCP Namespace.

2. Pass the right export variables pertinent to the VLAN Provider Network. 


## Detailed installation steps

The following assumptions are considered during deployment:

* Port 2 of all the Baremetal nodes are wired and used as bm_network - referred in the document as em2 or eth1

* 1 Untagged network for Management - subnet range 192.168.200.0/24 w/ gateway 192.168.200.1

* 90 tagged networks used for tenant VLANs as provider network

<!---* For test purpose VLAN 300 (192.168.1-->

## Process

1. Login to Seed VM Host

2. Create a file `envfile` with the following export variables. Modify the IPs based on your specifications. 
	
		export BRIDGE_INTERFACE=em2
		export BM_NETWORK_SEED_IP=192.168.200.2
		export BM_NETWORK_CIDR=192.168.200.0/24
		export BM_NETWORK_GATEWAY=192.168.200.1
		export BM_NETWORK_SEED_RANGE_START=192.168.200.3
		export BM_NETWORK_SEED_RANGE_END=192.168.200.20
		export BM_NETWORK_UNDERCLOUD_RANGE_START=192.168.200.31
		export BM_NETWORK_UNDERCLOUD_RANGE_END=192.168.200.50
		export OVERCLOUD_NeutronPublicInterface=eth1
		export UNDERCLOUD_NeutronPublicInterface=eth1
		export OVERCLOUD_NTP_SERVER=16.110.135.123
		export UNDERCLOUD_NTP_SERVER=16.110.135.123
		export FLOATING_START=192.168.200.101
		export FLOATING_END=192.168.200.254
		export FLOATING_CIDR=192.168.200.0/24
		export OVERCLOUD_HYPERVISOR_PUBLIC_INTERFACE=eth1
		export OVERCLOUD_HYPERVISOR_PHYSICAL_BRIDGE=br-ex
		export OVERCLOUD_BRIDGE_MAPPINGS=physnet1:br-ex
		export NeutronNetworkType=vlan
		export NeutronNetworkVLANRanges=physnet1:300:398
		export OVERCLOUD_CONTROL_VIRTUAL_ROUTER_ID=37
		export OVERCLOUD_VIRTUAL_INTERFACE=br-ex
		export OVERCLOUD_NEUTRON_DVR=False
		export OVERCLOUD_COMPUTESCALE=1
		#######################################
		export UNDERCLOUD_CODN_HTTP_PROXY=http://16.85.88.10:8080
		export UNDERCLOUD_CODN_HTTPS_PROXY=http://16.85.88.10:8080
		export OVERCLOUD_CODN_HTTP_PROXY=http://16.85.88.10:8080
		export OVERCLOUD_CODN_HTTPS_PROXY=http://16.85.88.10:8080
		export OVERCLOUD_FIXED_RANGE_CIDR=172.0.100.0/24

3. Modify the `dhcp_agent.ini` configuration. 

 	* **Overcloud** - Edit the `tripleo/hp_passthrough/overcloud_neutron_dhcp_agent.json` file to add the parameter settings as defined in the following example and change the DNS servers specific to your environment.  
 
			{"dhcp_agent":
			  {"config":
			    [
			      {"section":"DEFAULT",
			        "values":
			          [
			            {"option":"dhcp_delete_namespaces","value":"True"},
			            {"option":"enable_isolated_metadata","value":"True"},
			            {"option":"dnsmasq_dns_servers", "value":"10.1.0.10,10.1.0.20"}
			          ]
			      }
			    ]
			  }
			}

	* **Undercloud** - Edit the `tripleo/hp_passthrough/undercloud_neutron_dhcp_agent.json` file to add the parameter settings as defined in the following example and change the DNS servers specific to your environment.  

			{"dhcp_agent":
			  {"config":
			    [
			      {"section":"DEFAULT",
			        "values":
			          [
			            {"option":"dhcp_delete_namespaces","value":"True"},
			            {"option":"dnsmasq_dns_servers", "value":"10.1.0.10,10.1.0.20"}
			          ]
			      }
			    ]
			  }
			}

			
4. To update ml2.conf, edit `tripleo/hp_passthrough/overcloud_neutron_ml2_conf.json` and add tenant&#095;network&#095;type and network&#095;vlan_ranges specific to your environment. An example is given below:

	    {
	       "ml2": {
	            "config": [
	                {
	                    "section": "ovs",
	                    "values": [
	                        {
	                            "option": "enable_tunneling",
	                            "value": "True"
	                        }
	                    ]
	    			    "section": "ml2",
	                    "values": [
	                        {
	                            "option": "tenant_network_types",
	                            "value": "vlan"
	                        }
	    				]
	    				"section": "ml2_type_vlan",
	                    "values": [	
	                        {
	                            "option": "network_vlan_ranges",
	                            "value": "physnet1:300:398"
	                        }
	                    ]
	                },
				
6. To create the Seed VM, do the following: 

		# source envfile
		# bash -x tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --create-seed | tee  create-seed.log

7. Copy environment file (created in step 2) to Seed and do the following from within the Seed:

		# ssh into the Seed VM <IP Address>  

	For example <192.168.200.2>
		

		# source envfile
		
8. Create the baremetal.csv with the required number of nodes and specify the  
mac&#095;address,ipmi&#095;user,ipmi&#095;password,ipmi&#095;address,no&#095;of&#095;cpus,memory&#095;MB,diskspace&#095;GB 

	For example

    	78:e7:d1:22:5d:58,administrator,password,192.168.11.1,12,32768,2048
    	78:e7:d1:22:52:9b,administrator,password,192.168.11.6,12,16384,900

9. To install and configure the Undercloud and Overcloud, run the following command from /root. 

		# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --skip-demo | tee installer.log

##Verifying the installation

1. Login to Horizon dashboard and create two projects (Tenant A and Tenant B) and also the users for the projects. 

2. SSH into the controller node. 
 
		# ssh heat-admin@controller.ip

	* Create a VLAN provider network(s) and subnets for tenant A. 

			# neutron net-create --provider:physical_network=physnet1 --provider:network_type=vlan --provider:segmentation_id=300 vlan300

	* Using Horizon UI, create a Subnet for the above network: **192.168.100.0/24**  and gateway **192.168.100.1**
     
	* Create a VLAN provider network(s) and subnets for tenant B.

			# neutron net-create --provider:physical_network=physnet1 --provider:network_type=vlan --provider:segmentation_id=301 vlan301

	* Using Horizon UI, create a Subnet for the network of tenant B network: Subnet: **192.168.101.0/24** gateway **192.168.101.1**
 
3. Launch two instances in vlan300 network and two in vlan301 network. 
 
4. Validate if you can ping the VMs from the KVM Host.
 
5. Validate if the VMs are able to ping one another in the same network and are unable to ping other instances in the other network.


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

----
