---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Backup and Restore Flexible Control Plane Hosts"
permalink: /helion/openstack/1.1/fcp/backup.restore/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Michael B, Fausto Marzi
---
<!--UNDER REVISION-->
	

<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!-- <p style="font-size: small;"> <a href="/helion/openstack/1.1/install/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install/esx/">NEXT &#9654;</a> </p> -->

# HP Helion OpenStack&#174; 1.1: Backup and Restore Flexible Control Plane Hosts

<!-- taken from http://wiki.hpcloud.net/display/core/Helion+1.1+FCP+KVM+Hosts+backup+and+recovery+procedure -->

This document describes how to back up an HP Helion OpenStack Flexible Control Plane (FCP) configuration and how to restore the FCP configuration from the backed-up nodes. 

The HP Helion OpenStack FCP architectural configuration enables you to deploy the control plane in a virtual environment. This configuration reduces the control plane footprint to just three servers from the current seven servers. For more information, see [Flexible Control Plane Overview](/helion/openstack/1.1/flexiblecontrol/overview/).

The backup and restore process involves running commands on each of the three servers that host the HP Helion OpenStack components. The backup process requires an additional server to store backed-up files.

This document contains the following topics:

* [About HP Helion OpenStack FCP](#about)
* [Backing Up Helion FCP](#back)
* [Restore the Helion FCP configuration](#restore)

## About backing up and restoring an FCP configuration {#about}

The FCP backup and restore procedure is based on the architecture diagrams from the Flexible Control Plane Documentation.

<!-- Our overview suggests only one deployment is possible In FCP the above diagram can be changed, as the individual components (Seed, Undercloud, Overcloud, Swift) can be installed with a different layout.
-->
Only one deployment architecture is possible, using three physical hosts.

The following figure illustrates the deployment of overcloud compute nodes and VSA nodes to physical servers. 
In the diagram: 

* one physical system hosts the seed VM, undercloud controller node, and one of the three overcloud controllers (`Overcloud-0`); 
* one physical system hosts one of the three overcloud controller nodes (`Overcloud-1`) and one of two Swift storage nodes (`Swift-Storage-0`); 
* one physical system hosts one of the three overcloud controller nodes (`Overcloud-2`); and one of two Swift storage nodes (`Swift-Storage-1`). 

<img src="media/flexiblecontrolpane1.png"> <br />
Figure 1

Based on this deployment, the following failure scenarios will be covered:

* Backing up and restoring the host where the following components run:

	* Seed cloud VM
	* Undercloud Controller VM
	* Overcloud Controller VM `Overcloud-0`

* Backing up and restoring the host where the following components run:

	* Overcloud Controller VM `Overcloud-1`
	* Swift Storage VM `Swift-Storage-0`

* Backing up and restoring the host where the following components run:

	* Overcloud Controller VM `Overcloud-2`
	* Swift Storage VM `Swift-Storage-0`

The following approach can be also used to recover from a total failure of all three physical systems (disaster recovery).

## Backing up an FCP installation{#back}

This section describes how to perform a full backup of an FCP configuration.

It will take approximately five hours to perform the back up.

* [Back up the Seed, Under Cloud and Overcloud-0](#seed)
* [Back up Overcloud-1 and Swift Storage VMs](#oc1)
* [Back up Overcloud-2 and Swift Storage VMs](#oc2)


### Before you begin {#before}

Before you start the back up and restore, you can review files on your systems for important information you will need for the procedure.

#### Review the installation details {#details}

You can review the current installation layout by checking the contents of several files for information you will use during the back up and restore process. 

#### Review Server MAC addresses

The `baremetal.csv` file contains the MAC addresses of the seed VM, the undercloud, and the overcloud controllers. It does not contains the MAC addresses of the physical nodes.

View the `baremetal.csv` file found on the seed cloud host using the following command:

	cat /root/baremetal.csv

A typical `baremetal.csv` file will look similar to the following:

	00:19:f2:eb:bf:b6,root,undefined,10.22.170.23,6,32768,512,Undercloud,VM
	00:53:f2:cf:a2:b1,root,undefined,10.22.170.20,6,32768,512,OvercloudControl,VM
	00:e1:09:c6:ab:f2,root,undefined,10.22.170.28,6,32768,512,OvercloudControl,VM
	00:47:b1:f1:69:e3,root,undefined,10.22.170.23,6,32768,512,OvercloudControl,VM
	00:f4:af:5e:a1:67,root,undefined,10.22.170.20,6,32768,512,OvercloudSwiftStorage,VM
	00:81:16:a2:1d:bd,root,undefined,10.22.170.28,6,32768,512,OvercloudSwiftStorage,VM

### Review physical mapping of nodes

The `vm-plan` file contains the mapping between physical nodes and VM roles. 

View the `vm-plan` file available on the seed cloud host using the following command:

	cat /tmp/vm-plan 

A typical `vm-plan` file will look similar to the following:

	,root,,10.22.170.23,6,32768,,Undercloud,
	eth2,root,,10.22.170.20,6,32768,,OvercloudControl,
	eth2,root,,10.22.170.28,6,32768,,OvercloudControl,
	,root,,10.22.170.23,6,32768,,OvercloudControl,
	eth2,root,,10.22.170.20,6,32768,,OvercloudSwiftStorage,
	eth2,root,,10.22.170.28,6,32768,,OvercloudSwiftStorage,

### Back up the seed cloud host, seed VM, undercloud node, and overcloud-0 node {#seed}

This procedure will back up the seed cloud host, the seed VM, Undercloud node and `Overcloud-0` node.

The following files will be backed up:

* `/root/baremetal.csv`
* `/root/tripleo/configs`
* `/tmp/vm-plan`
* The seed VM image file and libvirt XML configuration file - seed domain
* The undercloud VM image file and libvirt XML configuration file - baremetal_0 domain
* The Overcloud-0 VM image file and libvirt XML configuration file - baremetal_3 domain
* The content of /root/.ssh

When these components are backed up, make a compressed and encrypted tgz and send it in stream to the backup node.

**Note:** Make sure there is sufficient disk space (at least 150GB) for the back up. 

To back up these components:

1. Log into the seed cloud host as root: 

		sudo -i

2. SSH to the seed VM, Undercloud node and Overcloud-0 node host:

		ssh backup-user@<IP_address>

	where <IP_address> is the address of the host.

2. Use the following command to initialize the `date time var` and `list` domains:

		export NOW=$(date +"%T-%m-%d-%Y")

3. Switch to root:

		sudo su -

4. Make sure that the nodes are running:

		virsh list

		Id    Name                           State
		----------------------------------------------------
 		2     seed                           running
 		4     baremetal_3                    running
 		5     baremetal_0                    running

	By default, the undercloud node and `Overcloud-0` nodes are `baremetal_3` and `baremetal_0` respectively.

5. Dump the XML domain configuration files to disk for each node you are backing up:

	a. Create the back up directory:

		 mkdir dom-xml-config

	b. Run the command for the seed VM:

		virsh dumpxml seed > ~/dom-xml-config/seed-$NOW.xml

	c. Run the command for `baremetal_3`:

		virsh dumpxml baremetal_3 > ~/dom-xml-config/baremetal_3-$NOW.xml

	d. Run the command for `baremetal_0`:

		virsh dumpxml baremetal_0 > ~/dom-xml-config/baremetal_0-$NOW.xml

6. List the contents of the `dom-xml-config` directory, created by the previous commands:

		ls ~/dom-xml-config/

	The follwing files should appear, where <datetime> is the date and time the backup was performed:

		baremetal_0-<datetime>.xml
		seed-<datetime>.xml
		baremetal_<datetime>.xml

7. List the virtual disk name of each domain you are backing up. Note these names for the following step:

	a. Run the command for the seed VM:

		virsh domblklist seed 

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/seed.qcow2

	b. Run the command for `baremetal_0`:

		virsh domblklist baremetal_0

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_0.qcow2

	c. Run the command for `baremetal_3`:

		virsh domblklist baremetal_3

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_3.qcow2

8. Create a point-in-time snapshot of the seed VM, the undervloud node (`baremetal_0`) and overcloud-0 (`baremetal_3`) node, specifying the virtual disk name from the previous step:

	a. Create a snapshot for the seed VM:

		virsh snapshot-create-as seed  snap-1-seed "seed snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-seed.qcow2 --disk-only --atomic

	The following message displays:

		Domain snapshot snap-1-seed created

	b. Create a snapshot for `baremetal 0`:
 
		virsh snapshot-create-as baremetal_0 snap-1-baremetal_0  "baremetal_0 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_0.qcow2 --disk-only --atomic

	The following message displays:

		Domain snapshot snap-1-baremetal_0 created

	c. Create a snapshot for `baremetal 3`:

		virsh snapshot-create-as baremetal_3 snap-1-baremetal_3  "baremetal_3 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_3.qcow2 --disk-only --atomic

		Domain snapshot snap-1-baremetal_3 created

9. List the snapshots for each domain

	a. List the snapshot for the seed VM:

		virsh snapshot-list seed

	The following displays:

		Name                 Creation Time             State
		------------------------------------------------------------
		snap-1-seed          2015-05-06 16:51:14 +0100 disk-snapshot

	b. List the snapshot for `baremetal 0`:

		virsh snapshot-list baremetal_0

	The following displays:

		Name                 Creation Time             State
		------------------------------------------------------------
		snap-1-baremetal_0   2015-05-06 16:51:56 +0100 disk-snapshot

	c. List the snapshot for `baremetal 1`:


		virsh snapshot-list baremetal_3

	The following displays:

		Name                 Creation Time             State
		------------------------------------------------------------
		snap-1-baremetal_3   2015-05-06 16:52:13 +0100 disk-snapshot

10. Use the `virsh domblklist` command to make sure each domain is using the appropriate snapshot for block storage:

	a. Run the command for seed VM:

		virsh domblklist seed       

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap1-seed.qcow


	b. Run the command for `baremetal 0`:

		virsh domblklist baremetal_0

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap1-baremetal_0.qcow

	c. Run the command for `baremetal_3`:

		virsh domblklist baremetal_3

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap1-baremetal_3.qcow

11. Create an encryption key file. 

		vim /root/.ssh/backup_key

12. Make a compressed and encrypted TGZ file from the backed-up files listed above and copy it to the backup node.  Use the appropriate user name and IP address to SSH to the backup node.

		tar SPzcvf - /root/.ssh /root/tripleo /root/baremetal.csv /tmp/vm-plan /var/lib/libvirt/images/baremetal_0.qcow2 /var/lib/libvirt/images/baremetal_3.qcow2 /var/lib/libvirt/images/seed.qcow2 /root/dom-xml-config | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh  ssh <user>@<IP_address> "cat > /backup/kvm-seed-host-$NOW.enc"

13. Rejoin the snapshot image to the original base machine.

	a. Run the command for the seed VM:

		virsh blockcommit seed sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]
		Successfully pivoted

	b. Run the command for `baremetal 0`:
 
		virsh blockcommit baremetal_0 sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]
		Successfully pivoted

	c. Run the command for `baremetal_3`:

		virsh blockcommit baremetal_3 sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]
		Successfully pivoted

The backup of seed host, undercloud, and `Overcloud-0` is complete.

### Back up the Overcloud-1 and Swift Storage VMs {#oc1}

This procedure will back up the Overcloud-1 and Swift Storage nodes.

The following files will be backed up:

* `/root/hp_ced_host_manager.sh` 
* `/root/hp_ced_ensure_host_bridge.sh`
* `/tmp/vm-plan`
* The Overcloud-1 VM image file and libvirt XML configuration file - baremetal_1 domain
* The Swift Storage VM image file and libvirt XML configuration file - baremetal_4 domain
* The content of `/root/.ssh`

When these components are backed up, make a compressed and encrypted TGZ file and copy the file to the backup node.

**Note:** Make sure there is sufficient disk space (at least 150GB) for the backup. 

To back up these components:

1. Log into the seed cloud host as root. 

		sudo -i

2. SSH to the Overcloud-1 and Swift Storage host.

		ssh <user>@<IP_address>

	where <IP_address> is the address of the host.

2. Use the following command to initialize the `date time var` and `list` domains

		export NOW=$(date +"%T-%m-%d-%Y")

3. Switch to root:

		sudo su -

4. Make sure that the nodes are running:

		virsh list

		Id    Name                           State
		----------------------------------------------------
		4     baremetal_4                    running
		5     baremetal_1                    running

	By default, the `Overcloud-1` and `Swift-Storage-0` nodes are `baremetal_1` and `baremetal_4` respectively.


5. Dump the XML domain configuration files to disk for each node to back up:

	a. Create the back up directory:

		mkdir dom-xml-config

	b. Run the command for `baremetal_4`:

		virsh dumpxml baremetal_4 > ~/dom-xml-config/baremetal_4-$NOW.xml

	c. Run the command for `baremetal_1`:

		virsh dumpxml baremetal_1 > ~/dom-xml-config/baremetal_1-$NOW.xml

6. List the contents of the `dom-xml-config` directory, created by the previous commands:

		ls ~/dom-xml-config/

	The follwing files should appear, where <datetime> is the date and time the backup was performed:

		baremetal_1-<datetime>.xml 
		baremetal_4-<datetime>.xml

7. List the virtual disk name of each domain you are backing up. Note these names for the following step:

	a. Run the command for `baremetal_3`:

		virsh domblklist baremetal_3

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_1.qcow2
 
	b. Run the command for `baremetal_4`:

		virsh domblklist baremetal_4

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_4.qcow2

8. Create a point-in-time snapshot of the seed VM, the overcloud-1 (`baremetal_1`) and Swift-Storage-0 (`baremetal_4`) nodes, specifying the virtual disk name from the previous step:

	a. Run the command for `baremetal_1`:

		virsh snapshot-create-as baremetal_1  snap-1-baremtal_1 "baremetal_1 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_1.qcow2 --disk-only --atomic

	The following message displays:

		Domain snapshot snap-1-baremtal_1 created

	b. Run the command for `baremetal_4`:

		virsh snapshot-create-as baremetal_4  snap-1-baremtal_4 "baremetal_4 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_1.qcow2 --disk-only --atomic

	The following message displays:

		Domain snapshot snap-1-baremtal_4 created


9. List the snapshots for each domain.

	a. Run the command for `baremetal_4`:

		snapshot-list baremetal_4
 
	The following displays, with the appropriate date and time:

		Name                 Creation Time             State
		------------------------------------------------------------
		snapshot_baremetal_4 2015-05-05 16:05:42 +0100 disk-snapshot

	b. Run the command for `baremetal_1`:

		virsh snapshot-list baremetal_1

	The following displays, with the appropriate date and time:

		Name                 Creation Time             State
		------------------------------------------------------------
		snapshot_baremetal_1 2015-05-05 15:57:04 +0100 disk-snapshot


10. Use the `virsh domblklist` command to make sure each domain is using the appropriate snapshot for block storage:

	a. Run the command for `baremetal_1`:

		virsh domblklist baremetal_1

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap-1-baremtal_1.qcow
 
	b. Run the command for `baremetal_4`:

		virsh domblklist baremetal_4       

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap-1-baremtal_4.qcow

11. Create an encryption key file. 

		vim /root/.ssh/backup_key

12. Make a compressed and encrypted TGZ file from the backed-up files listed above and copy it to the backup node. Use the appropriate user name and IP address to SSH to the backup node.

		tar SPzcvf - /var/lib/libvirt/images/baremetal_1.qcow2 /var/lib/libvirt/images/baremetal_4.qcow2 /root/dom-xml-config /root/hp_ced_host_manager.sh /root/hp_ced_ensure_host_bridge.sh /root/id_rsa_virt_power.pub | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh <user>@<IP_address> "NOW=$(date +"%T-%m-%d-%Y") cat > /backup/kvm-oc-1-sw-st-1-host-$NOW.enc"


13. Rejoin the snapshot image to the original base machine.

	a. Run the command for `baremetal_1`:

		virsh blockcommit baremetal_1 sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]
		Successfully pivoted

	b. Run the command for `baremetal_4`:

		virsh blockcommit baremetal_4 sda --active --verbose --pivot

	The following displays:
	
		Block Commit: [100 %]


The backup of Overcloud-1 and Swift-Storage-0 KVM host is complete.


### Back up Overcloud-2 and Swift Storage VMs {#oc2}

This procedure will back up the Overcloud-2 and Swift Storage nodes.

The following files will be backed up:

* `/root/hp_ced_host_manager.sh` 
* `/root/hp_ced_ensure_host_bridge.sh`
* `/tmp/vm-plan`
* The Overcloud-1 VM image file and `libvirt` XML configuration file for the `baremetal_2` domain
* The Swift Storage VM image file and `libvirt` XML configuration file for the `baremetal_5` domain
* The contents of the `/root/.ssh` directory 

When these components are backed up, make a compressed and encrypted tgz and send it in stream to the backup node.


To back up these components:

1. Log into the seed cloud host as root. 

		sudo -i

2. SSH to the Overcloud-2 and Swift Storage host:

		ssh backup-user@<IP_address>

	where <IP_address> is the address of the host.

2. Use the following command to initialize the `date time var` and `list` domains

		export NOW=$(date +"%T-%m-%d-%Y")

3. Switch to root:

		sudo su -

4. Make sure that the nodes are running:

		virsh list

		Id    Name                           State
		----------------------------------------------------
		2     baremetal_2                    running
		3     baremetal_5                    running

	By default, the `Overcloud-2` and `Swift-Storage-1` nodes are `baremetal_2` and `baremetal_5` respectively.

5. Dump the XML domain configuration files to disk for each node to back up:

	a. Create the back up directory:

		mkdir dom-xml-config

	b. Run the command for `baremetal_2`:

		virsh dumpxml baremetal_2 > ~/dom-xml-config/baremetal_2-$NOW.xml

	c. Run the command for `baremetal_5`:

		virsh dumpxml baremetal_5 > ~/dom-xml-config/baremetal_5-$NOW.xml

6. List the contents of the `dom-xml-config` directory, created by the previous commands:

		ls ~/dom-xml-config/

	The follwing files should appear, where <datetime> is the date and time the backup was performed:

		baremetal_2-<datetime>.xml 
		baremetal_5-<datetime>.xml

7. List the virtual disk name of each domain you are backing up. Note these names for the following step:

	a. Run the command for `baremetal_2`:

		virsh domblklist baremetal_2

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_2.qcow2

	b. Run the command for `baremetal_5`:

		virsh domblklist baremetal_5

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/baremetal_5.qcow2

8. Create a point-in-time snapshot of the seed VM, the overcloud-2 (`baremetal_2`) and Swift-Storage-1 (`baremetal_5`) nodes, specifying the virtual disk name from the previous step:

	a. Run the command for `baremetal_2`:

		virsh snapshot-create-as baremetal_2  snap-1-baremtal_2 "baremetal_2 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_2.qcow2 --disk-only --atomic

	The following displays:

		Domain snapshot snap-1-baremtal_2 created

	b. Run the command for `baremetal_5`:

		virsh snapshot-create-as baremetal_5  snap-1-baremtal_5 "baremetal_5 snapshot 1" --diskspec sda,file=/var/lib/libvirt/images/snap1-baremetal_5.qcow2 --disk-only --atomic

	The following displays:

		Domain snapshot snap-1-baremtal_5 created

9. List the snapshots for each domain.

	a. Run the command for `baremetal_2`:

		virsh snapshot-list baremetal_2 

	The following displays, with the appropriate date and time:

		Name                 Creation Time             State
		------------------------------------------------------------
		snap-1-baremtal_2    2015-05-08 13:40:33 +0100 disk-snapshot

	b. Run the command for `baremetal_5`:

		virsh snapshot-list baremetal_5 

	The following displays, with the appropriate date and time:

		Name                 Creation Time             State
		------------------------------------------------------------
		snap-1-baremtal_5    2015-05-08 13:40:33 +0100 disk-snapshot

10. Use the `virsh domblklist` command to make sure each domain is using the appropriate snapshot for block storage:

	a. Run the command for `baremetal_2`:

		virsh domblklist baremetal_2

	The following displays:

		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap1-baremetal_2.qcow2

	b. Run the command for `baremetal_5`:

		virsh domblklist baremetal_5


		Target     Source
		------------------------------------------------
		sda        /var/lib/libvirt/images/snap1-baremetal_5.qcow2


11. Create an encryption key file. 

		vim /root/.ssh/backup_key

12. Make a compressed and encrypted TGZ file from the backed-up files listed above and copy the file to the backup node. Use the appropriate user name and IP address to SSH to the backup node.

		tar SPzcvf - /root/.ssh /var/lib/libvirt/images/baremetal_2.qcow2 /var/lib/libvirt/images/baremetal_5.qcow2 /root/dom-xml-config /root/hp_ced_host_manager.sh /root/hp_ced_ensure_host_bridge.sh /root/id_rsa_virt_power.pub | openssl enc -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | ssh <user>@<IP_address> "NOW=$(date +"%T-%m-%d-%Y") cat > /backup/kvm-oc-2-sw-st-1-host-$NOW.enc"

13. Use the following command to rejoin the snapshot image to the original base machine.

	a. Run the command for `baremetal_2`:

		virsh blockcommit baremetal_2 sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]
		Successfully pivoted

	b. Run the command for `baremetal_5`:
 
		virsh blockcommit baremetal_5 sda --active --verbose --pivot

	The following displays:

		Block Commit: [100 %]

The backup of Overcloud-2 and Swift-Storage-1 KVM host is complete.

## Restore the Helion FCP configuration {#restore}

Each of the components, the seed VM, undercloud node, overcloud nodes and Swift nodes, can be restored from backups.

This section describes how to perform a full restore of an FCP configuration.

It will take approximately four hours to perform the backup.

* [Restore the Seed, Under Cloud and Overcloud-0](#seed-rest)
* [Restore Overcloud-1 and Swift Storage VMs](#oc1-rest)
* [Restore Overcloud-2 and Swift Storage VMs](#oc2-rest)

**Note:** Some of the following steps are specific to the physical node where the seed VM runs. Do not use the following steps to install baremetal for nodes where the seed VM will not be running.

### Prerequisites {#pre}

Before starting the restore process, make sure the system you are using meets the following requirements:

* QEMU 2.1 (or above)
* libvirt-1.2.9 (or above).

You must also have SSH access to the backup node and the decryption key.

#### Hardware and software requirements {#hard}

Before you start, if you have not done so already, make sure your environment meets the hardware and software requirements defined in the [Support Matrix](/helion/openstack/1.1/support-matrix/).

Make sure the network setup where the new node will be installed is consistent with the following requirements. 

* Install HP Linux for HP Helion OpenStack, Ubuntu 14.04 LTS or Debian 8

	The seed cloud host must have HP Linux for HP Helion OpenStack, Ubuntu 14.04 LTS or Debian 8 installed before performing the restore procedure.

* Configure SSH

	On the seed cloud host, the OpenSSH server must be running and the firewall configuration should allow access to the SSH ports.

	After Linux installation, additional software needs to be installed as defined in the **Software Requirements** section of the [Support Matrix](/helion/openstack/1.1/support-matrix/#software-requirements).


## Restore the seed VM, undercloud and overcloud controller-0 {#seed-rest}

Use the following section to restore the host where the seed VM, undercloud and overcloud controller-0 reside.

Make sure the system you are restoring to meets the [Prerequisites](#pre) and [Hardware and software requirements](#hard) before you start.

1. Log into the seed cloud host as root. 

		sudo -i

2. Use the following commands to download, decrypt, untar the data from the backup node using the appropriate IP addresses and users:

		vim /root/.ssh/backup_key
		ssh <user>@<IP-address>
		mkdir /var/lib/libvirt/images/restore

	Where <IP-address> is the IP address of the system where the backed-up files are stored.

3. Use the following command:

		cat /backup/kvm-seed-host-backup-date.enc | ssh heat-admin@<IP-address> 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'

	Where <IP-address> is the address of the seed VM.

	All the files will be restored to the same path as the original installation.

3. Use the following command to set required environment variables. 

		BRIDGE_INTERFACE=<interface> 
		HP_VM_MODE=hybrid
		SEED_NTP_SERVER=<NTP_IP-address>
		BM_NETWORK_SEED_IP=<Seed_IP-address>
		BM_NETWORK_CIDR=<Netmask> 

	Where:
		<interface> is the interface to use for the network bridge
		<NTP_IP-address> is the IP address of the NTP server to use
		<Seed_IP-address> is the IP address to assign to the seed VM.
		<Netmask> is the range of IP addressed to assign for the baremetal network,

4. Use the following command to start the restore. Provide the location of the previously backed-up `vm-plan` and `baremetal.csv` files:

		bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup

5. Use the following commands to create the KVM domains from the XML configuration files previously dumped. Enter the appropriate name of the back up file: `system-timedate.xml`. 

	a. Create the seed VM:

		virsh create /root/dom-xml-config/seed-<time-date>.xml

	b. Create the undercloud:

		virsh create /root/dom-xml-config/baremetal_0-<time-date>.xml

	c. Create the `Overcloud-controller-0`:

		virsh create /root/dom-xml-config/baremetal_3-<time-date>.xml

6. Connect to the seed VM using SSH: 
	<!-- Can user run these three commands in order on each system? config, restart, config -->

		ssh <user>@<IP_address>

7. Use the following command to refresh the configuration:

		os-refresh-config

8. Connect to the undercloud using SSH: 

		ssh <user>@<IP_address>

9. Use the following command to refresh the configuration:

		os-refresh-config

10. Connect to the `Overcloud-controller-0` using SSH: 

		ssh <user>@<IP_address>

11. Use the following command to refresh the configuration:

		os-refresh-config

12. Connect to the seed VM using SSH: 

		ssh <user>@<IP_address>

13. Use the following command to restart MySQL:

		service mysql restart

14. Connect to the undercloud using SSH: 

		ssh <user>@<IP_address>

15. Use the following command to restart MySQL:

		service mysql restart

16. Connect to the `Overcloud-controller-0` using SSH: 

		ssh <user>@<IP_address>

17. Use the following command to restart MySQL:

		service mysql restart

18. Again execute the `os-refresh-config` command on each of the three VMs (seed VM, undercloud, `Overcloud-controller-0`)

The restore of the seed VM, undercloud, and `Overcloud-controller-0` is complete.

### Restore Overcloud Controller-1 and Swift Storage-0 	{#oc1-rest}

Use the following steps to restore the physical KVM host where Overcloud Controller-1 and Swift Storage-0 VM will run.

Make sure the system you are restoring to meets the [Prerequisites](#pre) and [Hardware and software requirements](#hard) before you start.

**Note:** The restore will take approximately 4 hours. 


1. Log in as root to the system where you want to restore 

		sudo -i

2. Use the following commands to download, decrypt, untar the data from the backup node using the appropriate IP addresses and users:

		vim /root/.ssh/backup_key
		ssh <user>@<IP-address>
		mkdir /var/lib/libvirt/images/restore

	Where <IP-address> is the IP address of the system where the backed-up files are stored.

3. Use the following command:

		cat /backup/kvm-seed-host-backup-date.enc | ssh heat-admin@<IP-address> 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'

	Where <IP-address> is the address of the system where you are restoring to.

	All the files will be restored to the same path as the original installation.

3. Use the following command to set required environment variables. 

		BRIDGE_INTERFACE=<interface> 
		HP_VM_MODE=hybrid
		SEED_NTP_SERVER=<NTP_IP-address>
		BM_NETWORK_SEED_IP=<Seed_IP-address>
		BM_NETWORK_CIDR=<Netmask> 

	Where:
		<interface> is the interface to use for the network bridge
		<NTP_IP-address> is the IP address of the NTP server to use
		<Seed_IP-address> is the IP address to assign to the seed VM.
		<Netmask> is the range of IP addressed to assign for the baremetal network,

4. Use the following command to start the restore. Provide the location of the previously backed-up `vm-plan` and `baremetal.csv` files:

		bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup

5. Use the following commands to create the KVM domains from the XML configuration files previously dumped. Enter the appropriate name of the back up file: `system-timedate.xml`. 

	a. Create `Overcloud-1`:

		virsh create /root/dom-xml-config/baremetal_1-<time-date>.xml

	b. Create `Swift-Storage-0`:

		virsh create /root/dom-xml-config/baremetal_4-<time-date>.xml

6. Connect to the `Overcloud-1 VM` using SSH: 

		ssh <user>@<IP_address>

7. Use the following command to refresh the configuration:

		os-refresh-config

8. Connect to `Swift-Storage-0` using SSH: 

		ssh <user>@<IP_address>

9. Use the following command to refresh the configuration:

		os-refresh-config

12. Connect to `Overcloud-1` using SSH: 

		ssh <user>@<IP_address>

13. Use the following command to restart MySQL:

		service mysql restart

The `Overcloud-1` and `Swift-Storage-0` are restored.

## Restore the overcloud controller-2 and Swift storage-1 {#oc2-rest}

Use the following steps to restore the physical KVM host where overcloud Controller-2 and Swift Storage-1 VM will run.

Make sure the system you are restoring to meets the [Prerequisites](#pre) and [Hardware and software requirements](#hard) before you start.

**Note:** The restore will take approximately 3 hours. 

1. Log in as root to the system where you want to restore.

		sudo -i

2. Use the following commands to download, decrypt, untar the data from the backup node using the appropriate IP addresses and users:

		vim /root/.ssh/backup_key
		ssh <user>@<IP-address>
		mkdir /var/lib/libvirt/images/restore

	Where <IP-address> is the IP address of the system where the backed-up files are stored.

3. Use the following command:

		backup-node# cat /backup/kvm-oc-2-sw-st-1-host-date.enc | ssh heat-admin@<IP-address> 'NOW=$(date +"%T-%m-%d-%Y") cat | openssl enc -d -aes-256-cbc -salt -pass file:/root/.ssh/backup_key | tar -S -P -z -x -v -'

	Where <IP-address> is the address of the system where you are restoring to.

	All the files will be restored to the same path as the original installation.

3. Use the following command to set required environment variables. 

		BRIDGE_INTERFACE=<interface> 
		HP_VM_MODE=hybrid
		SEED_NTP_SERVER=<NTP_IP-address>
		BM_NETWORK_SEED_IP=<Seed_IP-address>
		BM_NETWORK_CIDR=<Netmask> 

	Where:
		<interface> is the interface to use for the network bridge
		<NTP_IP-address> is the IP address of the NTP server to use
		<Seed_IP-address> is the IP address to assign to the seed VM.
		<Netmask> is the range of IP addressed to assign for the baremetal network,

4. Use the following command to start the restore. Provide the location of the previously backed-up `vm-plan` and `baremetal.csv` files:

		bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --vm-plan /tmp/vm-plan --local-setup

5. Use the following commands to create the KVM domains from the XML configuration files previously dumped. Enter the appropriate name of the back up file: `system-timedate.xml`. 

	a. Create `Overcloud-2`:

		virsh create /root/dom-xml-config/baremetal_2-<time-date>.xml

	b. Create `Swift-Storage-1`:

		virsh create /root/dom-xml-config/baremetal_5-<time-date>.xml
		
6. Connect to the `Overcloud-2 VM` using SSH: 
		ssh <user>@<IP_address>

7. Use the following command to refresh the configuration:

		os-refresh-config

8. Connect to `Swift-Storage-1` using SSH: 

		ssh <user>@<IP_address>

9. Use the following command to refresh the configuration:

		os-refresh-config

12. Connect to `Overcloud-2` using SSH: 

		ssh <user>@<IP_address>

13. Use the following command to restart MySQL:

		service mysql restart

The `Overcloud-2` and `Swift-Storage-1` are restored.

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>
 
----

<style type="text/css">
pre {
color: black;
}
</style>
