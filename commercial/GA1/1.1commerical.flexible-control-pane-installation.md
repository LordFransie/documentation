---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Flexible Control Plane Installation"
permalink: /helion/openstack/1.1/flexiblecontrol/install/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Geraldine K,

---
<!--PUBLISHED-->

<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

# HP Helion OpenStack&#174; 1.1: Flexible Control Plane Installation
[See the Helion OpenStack 1.0 version of this page](/helion/openstack/flexiblecontrol/install/).

As you can see in the [overview](/helion/openstack/1.1/flexiblecontrol/overview/) of the documentation for 1.0, HP Helion OpenStack Flexible Control Plane (FCP) allows you to deploy the control plane in a virtual environment, reducing the control plane footprint to just three servers for proof of concept, evaluation, and exploration of HP Helion OpenStack features.

**Note:** As of HP Helion OpenStack 1.1.1, FCP is in an experimental phase and we do not recommend you use FCP in your production environment. We recommend that you test FCP in your test environment, for small-scale clouds of up to eight compute nodes.

For additional information about FCP, contact the HP Helion Support Center.

The overview covered the [prerequisites](/helion/openstack/1.1/flexiblecontrol/overview/). Now we will walk through the installation.

This page contains:

- [Step-by-step Installation Instructions](#instruct)
- [Post-Installation Steps](#postinstall)
- [Adding the Overcloud Nodes](#addnodes)
- [Known Issues and Resolutions](#knownissues)

##Before You Begin


Before you can install Helion OpenStack using the FCP approach, you will need to:

- [Review prerequisites and configure the infrastructure](/helion/openstack/1.1/flexiblecontrol/overview/#prereq) accordingly
- [Create configuration files](/helion/openstack/1.1/flexiblecontrol/overview/#configfiles)
- [Set up KVM hosts](/helion/openstack/1.1/flexiblecontrol/overview/#kvmsetup)
- Deploy an HP Helion cloud

##Step-by-step Installation Instructions {#instruct}

1. The installer will create a bridge on each host and a port for the external device will be added to the bridge. The installer will also move the IP address of the external device to the bridge device.
   This creates a baremetal network through which the virtual machines will communicate with each other and with the real baremetal systems.

  	 If the system running the installer and seed VM does *not* use the external device name eth0, then determine the device name *before* running the next step and

   	 	$ export BRIDGE_INTERFACE=<devicename>
   		For example: 
		$ export BRIDGE_INTERFACE=em1
   		or
		$ export BRIDGE_INTERFACE=eth5

  	 In the same manner, establish the value of BRIDGE_INTERFACE for each remote host (see below).


2.    Set up the remote hosts. You must set up each remote host identified in your vm-plan file.

	 a. Use an existing user or create a new user on *each* remote host to run the virtual machines. The user must have sufficient privileges to launch virtual machines using libvirt, for example, as a member of the 'libvirtd' group. 
	
 	This user is denoted &lt;kvmuser&gt; below and the host is denoted &lt;hvmhost&gt;. 

	b. Create a virtual power key on the seed's host. As root, run this command on the seed's host:

   		
   		$ export HP_VM_MODE=hybrid
    	$ bash /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --local-setup

 	 This command will perform all prerequisite system checks and create a Secure Shell (SSH) key that can be used for virtual power operations between hosts, if a key does not previously exist.

	c. Append the key to the &lt;kvmuser&gt;'s authorized_keys file on *each* remote host.
    
	As root:

		 $ scp /root/.ssh/id_rsa_virt_power.pub <kvmuser>@<kvmhost>:
		 $ ssh <kvmuser>@<kvmhost>
		 $ cat id_rsa_virt_power.pub >> ~<kvmuser>/.ssh/authorized_keys
		 $ chmod 600 ~<kvmuser>/.ssh/authorized_keys



	d. Copy hp&#95;ced&#95;host&#95;manager.sh to *each* remote host. 

    	For example: $ scp /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh \
             <kvmuser>@<kvmhost>:hp_ced_host_manager.sh

	e. Copy hp&#95;ced&#95;ensure&#95;host&#95;bridge.sh to *each* remote host.

		For example: $ scp /root/work/tripleo/tripleo-incubator/scripts/hp_ced_ensure_host_bridge.sh \
             <kvmuser>@<kvmhost>:hp_ced_ensure_host_bridge.sh



	f. As root on *each* remote host, run:

    	export BRIDGE_INTERFACE=<devicename>
    	bash -x ~<kvmuser>/hp_ced_host_manager.sh --remote-setup

	g. Create the *overcloud-config.json* file with the following contents. For definitions of the variables used and suggested values refer to the list of available [environment variables](http://docs.hpcloud.com/helion/openstack/1.1/install/envars). 

		 {
		    "cloud_type": "KVM",
		    "vsa_scale": 0,
		    "vsa_ao_scale": 0,
		    "so_swift_storage_scale": 0,
		    "so_swift_proxy_scale": 0,
		    "compute_scale": 4,
		    "bridge_interface": "em1",
		    "virtual_interface": "eth0",
		    "fixed_range_cidr": "172.0.100.0/24",
		    "control_virtual_router_id": "209",
		    "baremetal": {
		        "network_seed_ip": "192.168.130.3",
		        "network_cidr": "192.168.130.0/24",
		        "network_gateway": "192.168.130.1",
		        "network_seed_range_start": "192.168.130.4",
		        "network_seed_range_end": "192.168.130.22",
		        "network_undercloud_range_start": "192.168.130.23",
		        "network_undercloud_range_end": "192.168.130.126"
		    },
		  "neutron": {
		        "overcloud_public_interface": "vlan331",
		        "public_interface_raw_device": "eth0",
		        "undercloud_public_interface": "eth0"
		    },
		    "dns": {
		        "seed_server": "16.110.135.123",
		        "overcloud_server": "16.110.135.123",
		        "undercloud_server": "16.110.135.123"
		    },
		    "ntp": {
		        "overcloud_server": "16.110.135.123",
		        "undercloud_server": "16.110.135.123",
		        "seed_server": "16.110.135.123"
		    },
		"floating_ip": {
		        "start": "192.168.131.2",
		        "end": "192.168.131.245",
		        "cidr": "192.168.131.0/24"
		    },
		    "svc": {
		        "interface": "vlan332",
		        "interface_default_route": "192.168.132.1",
		        "allocate_start": "192.168.132.2",
		        "allocate_end": "192.168.132.250",
		        "allocate_cidr": "192.168.132.0/24",
		        "overcloud_bridge_mappings": "svcnet1:br-svc",
		        "overcloud_flat_networks": "svcnet1",
		        "customer_router_ip": "10.23.69.129"
		    },
		  "codn": {
		        "undercloud_http_proxy": "http://16.85.175.150:8080",
		        "undercloud_https_proxy": "http://16.85.175.150:8080",
		        "overcloud_http_proxy": "http://16.85.175.150:8080",
		        "overcloud_https_proxy": "http://16.85.175.150:8080"
		    }
		}

	

	h. Load the configuration file

		source /root/tripleo/tripleo-incubator/scripts/hp_ced_load_config.sh overcloud-config.json

8. Start the seed.
  
	 This will also launch the remote virtual machines.
   You must execute this command as root on the seed's host.

	    export BRIDGE_INTERFACE=<devicename>
	    export HP_VM_MODE=hybrid
	    bash -x /root/work/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh \
              --create-seed --vm-plan <vm-plan-filename>

  	 If the seed startup is successful, a message similar to this appears:

    "Tue Jan  6 11:58:04 GMT 2015 --- completed setup seed"


1. Build the cloud (all of the following commands should be run from /root):

	a. Login to the seed:

		$ ssh root@192.0.2.1

	b. Update the file called '/root/baremetal.csv' in the following format:

		<mac_address>,<user>,<password>,<ip_address>,<no_of_cpus>,<memory_MB>,<diskspace_GiB>,<role>,<power_managemenment>


   	The VMs you have pre-provisioned should already be present in this file so you need to add the details of the physical hardware you want to add to the system.

	For more information, see [Create the baremetal.csv File for Installation](helion/openstack/1.1/install/csv/).

	c. Load the configuration file

		source /root/tripleo/triple-incubator/scripts/hp_ced_load_config.json overcloud-config.json


	d. Run the second stage, that is, configure under and overclouds. Export all relevant environment variables, 

		$ bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh

 	**Important:** As these are hybrid systems comprising real hardware and virtual machines, your compute systems may not use the same networking configuration as your virtual machines:

		hypervisor.public_interface   
Set the value of this variable so it has the same name as the interface that carries the neutron external traffic on your overcloud compute nodes. If you do not specify the value of this variable, the default value is `eth0`.

	For more information about creating an optional second network, see [Editing the Installation Configuration JSON File](/helion/openstack/1.1/install/envars/) for more information.

		The following example shows how to set up the optional second network using "vlan331" and use eth0 on all control nodes, while using a different physical NIC (for example, eth1) on compute nodes.

	    "neutron": {
	        "overcloud_public_interface": "vlan331",
	        "public_interface_raw_device": "eth0",
	     },
	     "hypervisor": {
	         "public_interface": "vlan331",
	    }


    
5. Wait for the completion of the installer script to display a message similar to the following:
 
		HP - completed - Tue Oct 23 16:20:20 UTC 2014

##Post-Installation Steps {#postinstall}

- [Verify the installation](/helion/openstack/1.1/install/kvm/#verifying-your-installation)
- Configure Block Storage
	- [For VSA](/helion/openstack/1.1/install/vsa/overview/)
	- [For 3PAR](/helion/openstack/1.1/install/3par/)

##Adding Additional Overcloud Compute Nodes {#addnodes}

For more information, see: [Add, remove and replace nodes](/helion/openstack/1.1/technical-overview/#add-remove-replace-nodes).

##Known Issues and Resolutions {#knownissues}

###After the reboot of the compute node, new FIPs associated to the instance are not accessible

**System Behavior**

After updating the cloud, any new FIP being allocated to and associated with an existing or new VM is not accessible.

**Possible Resolution**	

Currently, the issue can be resolved by deleting the link being added where it is giving an error. 
<hr />
###After the  reboot of compute nodes, the instance goes into error state
**System Behavior**

After the reboot of compute nodes which have instances running, two issues are noticed:

Issue 1

	error: Failed to start domain instance-0000020d
	error: unsupported configuration: Domain requires KVM, but it is not available. 

Check that virtualization is enabled in the host BIOS and that host configuration is set up to load the KVM modules.
 
Issue 2

	error: failed to connect to the hypervisor
	error: no valid connection
	error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory.

**Possible Resolution**

Issue 1: Manual steps to recover

	lsmod | grep kvm
   If there is no output, issue the following commands:

	modprobe -v kvm 
	modprobe -v kvm_intel
	service libvirt-bin restart

Issue 2: Manual steps to recover

	pkill -ulibvirt-qemu
	reboot
<hr />
###A few VMs with two interfaces (pvt and svc) lost network plumbing on compute node (post update)
**System Behavior**

After updating, the VM guest is not accessible on its SVC interface.

**Possible Resolution**

No Resolution 
<hr />
###Environment may be unstable upon rebooting of overcloud controllers
**System Behavior**

Environment may be unstable for some time when overcloud controllers are rebooted.

**Probable Cause**

On the overcloud controller reboot scenario the cloud takes some time to stabilize.

**Possible Resolution**

If overcloud controllers are rebooted, you must wait for some time for the cloud to become stable. Verify basic cloud functionality before proceeding further.
<hr />
###If the KVM environment is rebooted, the  instances in the  spawned state cannot be reached via ping
**System Behavior**

In the case of an entire KVM environment reboot, previously spawned instances may not be reachable via the ping command.

**Possible Resolution**

Hard reboot the spawned instance from the Horizon dashboard to make it reachable.
<hr />
###A seed cloud host has been rebooted but the VMs hosting the control plane did not start automatically
**System Behavior**

A seed cloud host has been rebooted but the VMs hosting the control plane did not start automatically.


**Probable Cause**

The seed cloud host responsible for hosting the seed node has been rebooted. This requires manually restarting the VMs. 

**Possible Resolution**

**Restart the VMs on the seed's KVM host**

1.	Recover the seed by running the appropriate `hp_ced_host_manager.sh ` command with the --boot-seed argument.

	For example:
	`SEED_NTP_SERVER=12.12.12.12 bash -x ~root/tripleo/tripleo-incubator/scripts/hp_ced_host_manager.sh --boot-seed`
	For example, use the same `hp_ced_host_manager.sh` used during installation but substitute '--create-seed'.

2.	After the seed has booted, ensure that networking works correctly by logging into the seed's IP address, for example, 192.0.2.1.

3.	Once the seed is booted and working as expected, use virsh commands to start the other VMs.

	For example:

    	virsh start baremetal_0

    	virsh start baremetal_3


**Restart the VMs on a KVM host not running the seed**

1.	Use the virsh command to start the VMs. 
	
	For example:

		virsh start baremetal_1

		virsh start baremetal_4


2.	Verify that the networking works by logging into all nodes after they have been booted. 

	For more information on networking issues, see [Troubleshooting](/helion/openstack/1.1/services/troubleshooting/).

<hr />
###The seed cloud host cannot be reached after it is rebooted
**System Behavior**

After the seed cloud host is rebooted, the networking setup does not persist and the seed cloud host cannot be reached.

**Probable Cause**

The networking setup on the FCP seed cloud host must be set up in a particular way. Otherwise, the seed cloud host cannot be reached after it is rebooted. 

**Possible Resolution**

Change the network configuration in the interfaces file to a configuration that supports persisting the network setup.

**On Ubuntu/Debian systems:**

- Define the primary interface in its own configuration file in `etc/network/interfaces.d/.` 

    	For example: etc/network/interfaces.d/eth2.cfg 

- Alternatively, define the primary interface in `/etc/network/interfaces` and format the file so it has a line separator between each interface definition.  (See man files for an example.)
	
**On CentOS systems:**

- Ensure that every interface is defined in its on `ifcfg-<interface>` file within `/etc/sysconfig/network-scripts/`
<hr />

Return to the [architectural and configuration files overview](/helion/openstack/1.1/flexiblecontrol/overview/) information.
