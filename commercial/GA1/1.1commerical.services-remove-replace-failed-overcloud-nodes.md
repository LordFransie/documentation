---
layout: default
title: "HP Helion OpenStack&#174; Removing and replacing a failed Overcloud Controller"
permalink: /helion/openstack/1.1/removing/failedovercloud/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Systems Administrator 
role2: Cloud Architect 
role3: Storage Administrator 
role4: Network Administrator 
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
role8: Network Engineer 
authors: Stephen H, Nancy M

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/">&#9664; PREV | <a href="/helion/openstack/1.1/">&#9650; UP</a> | <a href="/helion/openstack/1.1/faq/">NEXT &#9654; </a></p>
-->

# HP Helion OpenStack&reg; 1.1: Removing and Replacing a Failed Overcloud Controller
[See version 1.0 of this page](/helion/openstack/removing/failedovercloud/)

The three node Overcloud Controller cluster provides a highly available Cloud Control Plane. In the event of a single point of failure in any one of the Controller nodes, the following procedures will allow you to recover that functionality.

In the rare event that your deployed operating cloud incurs an irrecoverable hardware failure in one of the Controller Servers, you will have to:


* Decommission the failed server (not covered here)
* Add a new server into your cloud (not covered here)
* Remove the failed controller from the heat configuration
* Add the new controller to the configuration and deploy it
* Clean up the environment

The following sections give instructions on how to replace such a failed controller node.

##Removing and replacing a failed controller

The following steps are to be run on the seed, as root unless stated otherwise and assumes that the bash shell is being used. They are based on the default install and you should not need to alter any of the commands or variables given unless stated otherwise.

There are some differences in the procedure for removing/replacing nodes depending on which controller is replaced:

- Controller0: By default, controller0 is the bootstrap controller. Controller1 is temporarily flagged as the bootstrap controller when controller0 is removed.
- Controller1: By default, nodes are contiguous, so some editing of the templates is required for non-contiguous nodes.
- Controller2: This is the most straightforward case; the CONTROLSCALE can be adjusted to remove the failed controller and deploy a new one.

##Removing and replacing controller0

####Remove the failed controller
Ensure controller0 is halted.

Configure the overcloud NTP server to one appropriate for the site.
 
	export OVERCLOUD_NTP_SERVER=16.110.135.123 # Use an NTP server appropriate for your environment
		
Set some environment variables (these should not need amending).

	export TRIPLEO_ROOT=~root/tripleo
	export TE_DATAFILE=$TRIPLEO_ROOT/ce_env.json
	export PATH=$PATH:$TRIPLEO_ROOT/bin:$TRIPLEO_ROOT/tripleo-incubator/scripts
	source $TRIPLEO_ROOT/tripleo-incubator/undercloudrc
	OVERCLOUD_EXTRA_CONFIG=$(hp_ced_load_passthrough.sh -v -p overcloud -x overcloud-complete
	get_ce_env() {
	    local var=$1
	    cat ~/tripleo/ce_env.json \
	        | jq ".overcloud.${var}" \
	        | sed -e "s/\"//g"
	}
	COMPUTESCALE=$(get_ce_env computescale)
	SOSWIFTPROXYSCALE=$(get_ce_env soswiftproxyscale)
	SOSWIFTSTORAGESCALE=$(get_ce_env soswiftstoragescale)
	SWIFTSTORAGESCALE=$(get_ce_env swiftstoragescale)
	VSASTORAGESCALE=$(get_ce_env vsastoragescale)


Save the existing controller template prior to generating a new one.

	cp -p $TRIPLEO_ROOT/tripleo-heat-templates/trickle/overcloud-ce-controller ~root/

Controller2 will temporarily become the bootstrap controller. We need to flag that the initialization has already been completed. **On controller2**, create the flag file that indicates this.

	mkdir -p /mnt/state/var/lib/boot-stack/
	touch /mnt/state/var/lib/boot-stack/init-openstack.ok


<!---If CORE-1797 is not fixed, we need to patch rabbit start up.    -->

Patch rabbit start up

	remaining_controllers=$(nova list \
	    | grep overcloud-ce-controller-controller \
	    | grep -v controller0 \
	    | cut -f 7 -d"|" \
	    | cut -f 2 -d"=")
	rabbit_post_configure="/opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	ssh_sudo="ssh heat-admin@controller_ip sudo"
	save_original="cp -p $rabbit_post_configure ~root/"
	patch_rabbitmq="sed -i '2aexit 0' $rabbit_post_configure"
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$save_original" <<< $remaining_controllers
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$patch_rabbitmq" <<< $remaining_controllers
	
	

Remove the failed node from the rabbit cluster (this assumes that the rabbitmq-server is no longer running on controller2, but this should be the case since we halted it as the first step). On any remaining controller, do the following:

	rabbitmqctl cluster_status # show the name of the failed controller
	rabbitmqctl forget_cluster_node <node> # the full rabbit name of the failed node as it appeared in the above output (including 'rabbit@..')


We will use the id of the failed node in some of the later clean-up steps. Back on the seed node, do the following.



	# set the nova node id for use later
	failed_node_id=$(nova list --minimal | grep controller-controller0 | cut -d '|' -f 2)
	ironic_failed_node_id=$(ironic node-show --instance $failed_node_id | grep " uuid" | cut -f3 -d"|" | sed 's/\s//')


Reduce the number of overcloud nodes by 1 and remove the failed node from the overcloud template.


	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	export CONTROLSCALE="2"
	make overcloud-ce-trickle


The template will have been created with controller0 and controller1. We need to change controller0 references  to controller2.

	sed -i 's/controller0/controller2/g' trickle/overcloud-ce-controller


Via trickle, deploy the heat template to remove the node.


	prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
	    -e /root/tripleo/overcloud-env.json \
	    -t 360 \
	    -f /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller \
	    -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller

Monitor status (it may take a few minutes for the status to move to UPDATE_COMPLETE).

	watch -d heat stack-list


###Add a replacement controller


Ensure the new node is available in Ironic.
Restore the original template configuration from the copy we saved earlier.

	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	cp ~root/overcloud-ce-controller trickle/

We need to edit the template so that the bootstrap node remains controller2. 

	sed -i '/bootstrap_nodeid/,/nodeid/s/controller./controller2/' $overcloud_template

<!-- If CORE-1797 is not fixed, remove rabbit patch from above, here -->
Remove rabbit patch from above

	restore_original="cp -p ~root/51-rabbitmq /opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	xargs -d" " -n 1 -I controller_ip $sudo_ssh "$restore_original" <<< $remaining_controllers


Increase the scale parameter back to its original value.

	export CONTROLSCALE=3

Deploy heat templates via trickle to provision the new node.

	prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
	    -e /root/tripleo/overcloud-env.json \
	    -t 360 \
	    -f /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller \
	    -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller

Monitor the status (it may take a few minutes for the status to move to UPDATE_COMPLETE).

	watch -d heat stack-list

###Reinstating controller0 as the bootstrap node

On controller0, add the flag that indicates the cluster has been initialized.

	mkdir -p /mnt/state/var/lib/boot-stack/
	touch /mnt/state/var/lib/boot-stack/init-openstack.ok

Run the installer script, passing in the update-overcloud parameter 

	tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo

When the script finishes, the cloud is ready for use, with the replacement management controller. Proceed to the section: Clean up the environment after controller removal and replacement.


##Removing and replacing controller1

###Remove the failed controller

Ensure controller1 is halted.


Configure the overcloud NTP server to one appropriate for the site.


	export OVERCLOUD_NTP_SERVER=16.110.135.123 # Use an NTP server appropriate for your environment

Set some environment variables (these should not need amending).

	export TRIPLEO_ROOT=~root/tripleo
	export TE_DATAFILE=$TRIPLEO_ROOT/ce_env.json
	export PATH=$PATH:$TRIPLEO_ROOT/bin:$TRIPLEO_ROOT/tripleo-incubator/scripts
	source $TRIPLEO_ROOT/tripleo-incubator/undercloudrc
	OVERCLOUD_EXTRA_CONFIG=$(hp_ced_load_passthrough.sh -v -p overcloud -x overcloud-compute)
	get_ce_env() {
	    local var=$1
	    cat ~/tripleo/ce_env.json \
	        | jq ".overcloud.${var}" \
	        | sed -e "s/\"//g"
	}
	COMPUTESCALE=$(get_ce_env computescale)
	SOSWIFTPROXYSCALE=$(get_ce_env soswiftproxyscale)
	SOSWIFTSTORAGESCALE=$(get_ce_env soswiftstoragescale)
	SWIFTSTORAGESCALE=$(get_ce_env swiftstoragescale)
	VSASTORAGESCALE=$(get_ce_env vsastoragescale)

Save the existing controller template prior to generating a new one.

	cp -p $TRIPLEO_ROOT/tripleo-heat-templates/trickle/overcloud-ce-controller ~root


<!--If CORE-1797 is not fixed, we need to patch rabbit startup. here -->

Patch rabbit startup

	remaining_controllers=$(nova list \
	    | grep overcloud-ce-controller-controller \
	    | grep -v controller1 \
	    | cut -f 7 -d"|" \
	    | cut -f 2 -d"=")
	ssh_sudo="ssh heat-admin@controller_ip sudo"
	rabbit_post_configure="/opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	save_original="cp -p $rabbit_post_configure ~root/"
	patch_rabbitmq="sed -i '1aexit 0' $rabbit_post_configure"
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$save_original" <<< $remaining_controllers
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$patch_rabbitmq" <<< $remaining_controllers

Remove the failed node from the rabbit cluster (this assumes that rabbitmq-server is no longer running on controller2, but this should be the case since we halted it as the first step). On any remaining controller, do the following:

	rabbitmqctl cluster_status # show the name of the failed controller
	rabbitmqctl forget_cluster_node <node> # the full rabbit name of the failed node as it appeared in the above output (including 'rabbit@..')


We will use the id of the failed node in some of the later clean-up steps. Back on the seed node, do the following.


	# set the nova node id for use later
	failed_node_id=$(nova list --minimal | grep controller-controller1 | cut -d '|' -f 2)
	ironic_failed_node_id=$(ironic node-show --instance $failed_node_id | grep " uuid" | cut -f3 -d"|" | sed 's/\s//')


Reduce the number of overcloud nodes by 1 and remove the failed node from the overcloud template.


	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	export CONTROLSCALE="2"
	make overcloud-ce-trickle


The template will have been created with controller0 and controller1. We need to change controller1 references to controller2.


	sed -i 's/controller1$/controller2/g' trickle/overcloud-ce-controller

Via trickle, deploy the heat template to remove the node.

	prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
	    -e /root/tripleo/overcloud-env.json \
	    -t 360 \
	    -f /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller\
	    -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller


Monitor status (it may take a few minutes for the status to become UPDATE_COMPLETE).

	watch -d heat stack-list


###Add a replacement controller


Ensure the new node is available in ironic.
Restore the original template configuration from the copy we saved earlier.

	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	cp ~root/overcloud-ce-controller trickle/


<!-- If CORE-1797 is not fixed, remove rabbit patch from above. -->

Remove rabbit patch from above

	restore_original="cp -p ~root/51-rabbitmq /opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$restore_original" <<< $remaining_controllers


Increase the scale parameter back to its original value.

	export CONTROLSCALE=3

Deploy heat templates to provision the new node


	hp_ced_install.sh --update-overcloud --skip-demo


When the script completes successfully, the stack will be available for use with the replacement management controller. Continue to Clean up the environment after controller removal and replacement.

##Removing and replacing controller2

###Remove the failed controller

Configure the overcloud NTP server to one appropriate for the site.

	export OVERCLOUD_NTP_SERVER=16.110.135.123 # Use an NTP server appropriate for your environment

Set some environment variables (these should not need amending).


	export TRIPLEO_ROOT=~root/tripleo
	export TE_DATAFILE=$TRIPLEO_ROOT/ce_env.json
	export PATH=$PATH:$TRIPLEO_ROOT/bin:$TRIPLEO_ROOT/tripleo-incubator/scripts
	source $TRIPLEO_ROOT/tripleo-incubator/undercloudrc
	OVERCLOUD_EXTRA_CONFIG=$(hp_ced_load_passthrough.sh -v -p overcloud -x overcloud-compute)
	get_ce_env() {
	    local var=$1
	    cat ~/tripleo/ce_env.json \
	        | jq ".overcloud.${var}" \
	        | sed -e "s/\"//g"
	}
	COMPUTESCALE=$(get_ce_env computescale)
	SOSWIFTPROXYSCALE=$(get_ce_env soswiftproxyscale)
	SOSWIFTSTORAGESCALE=$(get_ce_env soswiftstoragescale)
	SWIFTSTORAGESCALE=$(get_ce_env swiftstoragescale)
	VSASTORAGESCALE=$(get_ce_env vsastoragescale)


<!-- If CORE-1797 is not fixed, we need to patch rabbit startup. -->

Patch rabbitmq startup:


	remaining_controllers=$(nova list \
	    | grep overcloud-ce-controller-controller \
	    | grep -v controller2 \
	    | cut -f 7 -d"|" \
	    | cut -f 2 -d"=")
	ssh_sudo="ssh heat-admin@controller_ip sudo"
	rabbit_post_configure="/opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	save_original="cp -p $rabbit_post_configure ~root/"
	patch_rabbitmq="sed -i '1aexit 0' $rabbit_post_configure"
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$save_original" <<< $remaining_controllers
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$patch_rabbitmq" <<< $remaining_controllers

Remove the failed node from the rabbit cluster (this assumes that rabbitmq-server is no longer running on controller2, but this should be the case since we halted it as the first step). On any remaining controller, do the following:

	rabbitmqctl cluster_status # show the name of the failed controller
	rabbitmqctl forget_cluster_node <node> # the full rabbit name of the failed node as it appeared in the above output (including 'rabbit@..')

We will use the id of the failed node in some of the later clean-up steps. Back on the seed node, do the following.

	# set the nova node id for use later
	failed_node_id=$(nova list --minimal | grep controller-controller2 | cut -d '|' -f 2)
	ironic_failed_node_id=$(ironic node-show --instance $failed_node_id | grep " uuid" | cut -f3 -d"|" | sed 's/\s//')

Reduce the number of overcloud nodes by 1 and remove the failed node from the overcloud template.

	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	export CONTROLSCALE="2"
	make overcloud-ce-trickle

Save the existing controller template prior to regenerating

	cp -p $TRIPLEO_ROOT/tripleo-heat-templates/trickle/overcloud-ce-controller ~root


Remove the failed node from the rabbit cluster (this assumes that rabbitmq-server is no longer running on controller2, but this should be the case since we halted it as the first step). On any remaining controller, do the following:

	rabbitmqctl cluster_status # show the name of the failed controller
	rabbitmqctl forget_cluster_node <node> # the full rabbit name of the failed node as it appeared in the above output (including 'rabbit@..')

We will use the id of the failed node in some of the later clean-up steps. Back on the seed node, do the following.

	
	# set the nova node id for use later
	failed_node_id=$(nova list --minimal | grep controller-controller2 | cut -d '|' -f 2)
	ironic_failed_node_id=$(ironic node-show --instance $failed_node_id | grep " uuid" | cut -f3 -d"|" | sed 's/\s//')

Reduce the number of overcloud nodes by 1 and remove the failed node from the overcloud template.

	cd $TRIPLEO_ROOT/tripleo-heat-templates/
	export CONTROLSCALE="2"
	make overcloud-ce-trickle


<!--If CORE-2875 is not fixed, work around that by replacing invalid references to controller2 to controller1.-->


Replace invalid controller2 references to controller1  <!-- ?? -->

	sed -i 's/controller2$/controller1/' trickle/overcloud-ce-controller

Via trickle, deploy the heat template to remove the failed controller.

	
	prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
	    -e /root/tripleo/overcloud-env.json \
	    -t 360 \
	    -f /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller \
	    -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller


Monitor status (it may take a few minutes for the update to complete).

	watch -d heat stack-list


###Add a replacement controller

Ensure the new node is available in Ironic.

<!--If CORE-1797 is not fixed, remove rabbit patch from above. -->

Remove the rabbit patch from above.

	restore_original="scp -p ~root/51-rabbitmq /opt/stack/os-config-refresh/post-configure.d/51-rabbitmq"
	xargs -d" " -n 1 -I controller_ip $ssh_sudo "$restore_original" <<< $remaining_controllers

Increase the scale parameter back to its original value and redoploy the overcloud.

	cd $TRIPLEO_ROOT
	export CONTROLSCALE=3
	tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo



When the script completes successfully, the stack will be ready for use with the replacement controller. Proceed to the section Clean up the environment after controller removal and replacement.



##Clean up the environment after controller removal and replacement

Remove the failed node from Ironic.

	# show the failed node in ironic (using the failed_node_id variable from above)
	ironic node-show --instance $failed_node_id
	# remove the failed node from ironic, using the uuid
	ironic node-delete $ironic_failed_node_id

The final step is to remove the nova service entries for the failed controller: This should be done on the new management controller.


	# display service list for management controllers, including only the failed ones
	nova-manage service list | grep mgmt | grep XXX
	# set a variable to the failed host name from above
	failed_host=<name>
	# remove the failed node
	nova-manage service disable --service=nova-conductor --host=$failed_host
	nova-manage service disable --service=nova-cert --host=$failed_host
	nova-manage service disable --service=nova-scheduler --host=$failed_host
	nova-manage service disable --service=nova-consoleauth --host=$failed_host

Remove the failed node from Icinga monitoring. Run the following on the undercloud.

	cd /etc/check_mk/conf.d
	# when running the command below, replace <ip of failed controller> with the ip addressrm <ip of failed controller>.mk
	# show the monitored hosts
	check_mk --list-hosts

The procedure to remove and replace the controller is now complete.

<!-- left off here 841  -->
#Controller troubleshooting

If a heat stack-update appears to be taking a long time (30 minutes or longer):

Identify which resource is updating: From the seed, run following command to get an indication of which nodes to look at.

	heat resource-list overcloud-ce-controller | grep -v -i complete

###Checking Logs

1. On the controller node, look at the /var/log/upstart/os-collect-config.log for errors, especially those running os-refresh-config.

1. On the seed node, check the logs in /var/log/upstart, particularly those from heat, nova, and ironic.



	sudo tail -f /var/log/upstart/os-collect-config.log

**Problem 1:** If you see waiting on "os-svc-restart -n rabbitmq-server" message hanging for long time.

**Solution:** Run following commands.


	sudo pkill -u rabbitmq
	sudo rm -rf /mnt/state/var/lib/rabbitmq
	sudo rm -rf /mnt/state/var/log/rabbitmq
	sudo os-refresh-config

(If you see this error when you run os-refresh-config: "ERROR: os-refresh-config:Could not lock /var/run/os-refresh-config.lock. [Errno 11] Resource temporarily unavailable.", skip running os-refresh-config or rm /var/run/os-refresh-config.lock and re-run os-refresh-config )

**Problem 2:** After adding back controller0, If you see controller1 or Management controller is only node in its cluster and waiting other node to join for long time. 

**Solution:** Run following command on controllers to show cluster status:


	sudo rabbitmqctl cluster_status

and if cluster name is not controller0, run following command to join controller0 cluster:

	sudo rabbitmqctl stop_app
	sudo rabbitmqctl forget_cluster_node <cluster node name>
	sudo rabbitmqctl start_app
	sudo rabbitmqctl join_cluster_node <controller0 clustername>
	sudo os-refresh-config



<!--  end    --->



<!-- end -->




<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593;</a>
