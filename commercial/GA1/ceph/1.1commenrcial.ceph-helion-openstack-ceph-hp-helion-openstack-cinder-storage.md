---
layout: default
title: "HP Helion OpenStack&#174; HP Helion Ceph Cinder Storage"
permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-cinder-storage/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F, Binamra S

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->


# HP Helion OpenStack&#174; 1.1 Cinder Ceph Storage

The following steps are automatically performed when you download and execute `Ceph_client` script from the controller and compute nodes as per the readme file in the tar file.

1.  Verify the Ceph health by entering:

		ceph health
	
	Output:	
	
		HEALTH_OK

2. Create a new pool for the Cinder volume by entering:

		ceph osd pool create &#60;cinder pool name> &#60;pg-num>

	For example:
		
		ceph osd pool create helion-ceph-cinder 100

3.  To verify a new pool creation, enter:

		rados lspools
		
	Output:
		
		rbd ls helion-ceph-cinder

4. Create symbolic or softlinks to the relevant files on overcloud controller nodes by entering:

		ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
		ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

###Configuring Cinder

HP Helion OpenStack requires the `rbd` driver to interact with Ceph block devices. The pool name must be specified for the block device. To specify the pool name on the overcloud controller nodes edit `cinder.conf`.

Perform the following steps to configure Cinder.

1. Edit `/etc/cinder/cinder.conf` and add the following:

		[DEFAULT]
	
		# For ceph integration
		volume_driver=cinder.volume.drivers.rbd.RBDDriver
		rbd_pool=helion-ceph-cinder
		rbd_ceph_conf=/etc/ceph/ceph.conf
		rbd_flatten_volume_from_snapshot=false
		rbd_max_clone_depth=5
	
	You can also insert a comment on the `lvm` backend in the `cinder.conf` file for your reference.

		# LVM thin provision. This way we do not dd the disk
	
		#enabled_backends = lvm-1


2. Restart the Cinder service by entering:

		service cinder-volume restart
		service cinder-scheduler restart
		service cinder-api restart


Once the cinder service is restarted, you can create a volume. 


## Helion OpenStack Cinder Backup Ceph Storage

The Ceph backup driver performs the data backup operation on the volume to a Ceph RADOS Block Device (RBD). Backups can be performed between different Ceph pools and Ceph clusters. This section explains the backup and restore procedure for Cinder volumes.

On the Helion controller and compute node, if the Ceph client softwares are successfully installed and the Ceph configuration and keyring files are copied in the Helion controller and compute nodes, then the Helion OpenStack Ceph Configuration service performs the following steps:


1. Verify Ceph health status by entering:

		ceph health
		
	Output:

		HEALTH_OK

2. Create a new pool for Cinder backup by entering:

		ceph osd pool create &#60;cinder backup pool name> &#60;pg-num>

	For example:

		ceph osd pool create helion-ceph-backups 100



3. To verify that a new backup pool is created, enter:

		rados lspools
	
	Output:

		rbd ls helion-ceph-backups

4. Create symbolic or softlinks to the relevant files on overcloud controller nodes (if the following links are already created for other services, then ignore them) by entering:

		ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
		
		ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

###Configuring Cinder backup

Perform the following steps to enable the Ceph backup driver:

1. Edit `/etc/cinder/cinder.conf` and add the following options in all three controller nodes [OCC mgmt, OCC0, OCC1].


		[DEFAULT]
		
		# For cinder backup
			backup_driver=cinder.backup.drivers.ceph
		backup_ceph_conf=/etc/ceph/ceph.conf
		backup_ceph_user=cinder-backup
		backup_ceph_chunk_size=134217728
		backup_ceph_pool=helion-ceph-backups
		backup_ceph_stripe_unit=0
		backup_ceph_stripe_count=0
		restore_discard_excess_bytes=true

	The following table provides the description of the parameters.

	<table>
	<table style="text-align: left; vertical-align: top; width:650px;">
	<tr style="background-color: #C8C8C8;">
		<th> Parameters</th>
	<th>Descriptions </th>
		</tr>
		<tr>
	<td>backup_ceph_chunk_size = 134217728</td>
	<td>The chunk size, in bytes, that a backup is broken into before transfer to the Ceph object store.</td>
	</tr>
	<tr>
	<td>backup_ceph_conf = 
	/etc/ceph/ceph.conf</td>
	<td>Ceph configuration file to use.</td>
	</tr>
	<tr>
	<td>backup_ceph_pool = helion-ceph-backups</td>
	<td>The Ceph pool where volume backups are stored.</td>
	</tr>
	<tr>
	<td>backup_ceph_stripe_count = 0</td>
	<td>RBD stripe count to use when creating a backup image.</td>
	</tr>
	<tr>
	<td>backup_ceph_stripe_unit = 0</td>
	<td>RBD stripe unit to use when creating a backup image.</td>
	</tr>
	<tr>
	<td>backup_ceph_user = cinder</td>
	<td>The Ceph user to connect with. Default here is to use the same user as for Cinder volumes. If not using cephx this should be set to None.</td>
	</tr>
	<tr>
	<td>restore_discard_excess_bytes = True</td>
	<td>If True, always discard excess bytes when restoring volumes (that is, pad with zeroes).</td>
	</tr>
	  </table>


2. Restart the Cinder service by entering:

		service cinder-backup restart


## Next Steps

[Ceph Nova Storage]( /helion/openstack/1.1/ceph-helion-openstack-nova-ceph-storage/)

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

