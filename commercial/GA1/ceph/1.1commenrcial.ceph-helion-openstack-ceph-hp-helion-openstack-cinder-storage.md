---
layout: default
title: "HP Helion OpenStack&#174; HP Helion Ceph Cinder Storage"
permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-cinder-storage/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F, Binamra S

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->


# HP Helion OpenStack&#174; 1.1 Cinder Ceph Storage

The following steps are automatically performed when you download and execute `Ceph_client` script from the controller and compute nodes as per the readme file in the tar file.

1.  Verify the Ceph health by entering:

		ceph health
	
	Output:	
	
		HEALTH_OK

2. Create a new pool for the Cinder volume by entering:

		ceph osd pool create &#60;cinder pool name> &#60;pg-num>

	For example:
		
		ceph osd pool create helion-ceph-cinder 100

3.  To verify a new pool creation, enter:

		rados lspools
		
	Output:
		
		rbd ls helion-ceph-cinder

4. Create symbolic or softlinks to the relevant files on overcloud controller nodes by entering:

		ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
		ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

###Configuring Cinder

HP Helion OpenStack requires the `rbd` driver to interact with Ceph block devices. The pool name must be specified for the block device. To specify the pool name on the overcloud controller nodes edit `cinder.conf`.

Perform the following steps to configure Cinder.

1. Edit `/etc/cinder/cinder.conf` and add the following:

		[DEFAULT]
	
		# For ceph integration
		volume_driver=cinder.volume.drivers.rbd.RBDDriver
		rbd_pool=helion-ceph-cinder
		rbd_ceph_conf=/etc/ceph/ceph.conf
		rbd_flatten_volume_from_snapshot=false
		rbd_max_clone_depth=5
	
	You can also insert a comment on the `lvm` backend in the `cinder.conf` file for your reference.

		# LVM thin provision. This way we do not dd the disk
	
		#enabled_backends = lvm-1


2. Restart the Cinder service by entering:

		service cinder-volume restart
		service cinder-scheduler restart
		service cinder-api restart


Once the cinder service is restarted, you can create a volume. 

<!---
####Creating a Volume

There are two ways to create a volume:

1. * Horizon overcloud dashboard 
1. * Using the Command Line Interface (CLI) from the overcloud controller node running the Ceph client

**Using the Horizon overcloud dashboard**

To create a volume see [OpenStack User Guide](http://docs.openstack.org/user-guide/content/dashboard_manage_volumes.html)

<!---
1. Login in to Horizon overlcoud dashboard.
2. In the Horizon overlcoud dashboard, click the **Project** Tab. The tab displays with options in the left panel.

2. Click **Compute** and then select **Volume** to open Volume page.
3. Click Create Volume displayed at the top right corner of the page. The Create Volume dialog box is displayed.
4. Enter the required information in the respective fields.
5. Click **Create Volume**. The volume is created and it gets displayed in the Volumes page  as shown below. 


<img src="media/commercial-ceph-volume-create.png"/)>

---->
<!----
**Using the CLI**

To create a volume using the command-line interface (CLI) run the following command from the overcloud controller node running the Ceph client:

	Cinder create -display-name &#60;name of the volume> &#60;volume size>

For example:

	# cinder create --display-name vol2-RBD 1 

Output:

			+---------------------+-----------------------------------------------------------------+
			| Property			  | Value                                                           |     
			+---------------------+-----------------------------------------------------------------+
			| attachments  	      | []                     											|
			| availability_zone   | nova														    |
			| bootable            | false                                         					|
			| created_at          |  2014-08-01T14:56:21.423821                                     |
			| display_description |  None                                                      		|
			| display_name        | vol2-RBD 														|
			| encrypted     	  | False 															|
			| id    		      |d6064822-d1c1-4e72-b496-ee807174ef96  							|
			| metadata            | {}  															|
			| size				  |	1																|									
			| snapshot_id    	  | None 															|
			| source_volid 	      | None 															|
			| status              | creating                                     					|  
			|volume_type      	  | None                                                     		|
			+--------------+------------------------------------------------------------------------+

---->
## Helion OpenStack Cinder Backup Ceph Storage

The Ceph backup driver performs the data backup operation on the volume to a Ceph RADOS Block Device (RBD). Backups can be performed between different Ceph pools and Ceph clusters. This section explains the backup and restore procedure for Cinder volumes.

If the Ceph client softwares are installed and the Ceph configuration and keyring files are copied in the Helion controller and compute nodes, then the Helion OpenStack Ceph Configuration service performs the following steps:


1. Verify Ceph health status by entering:

		ceph health
		
	Output:

		HEALTH_OK

2. Create a new pool for Cinder backup by entering:

		ceph osd pool create &#60;cinder backup pool name> &#60;pg-num>

	For example:

		ceph osd pool create helion-ceph-backups 100



3. To verify that a new backup pool is created, enter:

		rados lspools
	
	Output:

		rbd ls helion-ceph-backups

4. Create symbolic or softlinks to the relevant files on overcloud controller nodes (if the following links are already created for other services, then ignore them) by entering:

		ln -s /usr/lib/python2.7/dist-packages/rados* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.
		
		ln -s /usr/lib/python2.7/dist-packages/rbd* /opt/stack/venvs/openstack/lib/python2.7/site-packages/.

###Configuring Cinder backup

Perform the following steps to enable the Ceph backup driver:

1. Edit `/etc/cinder/cinder.conf` and add the following options in all three controller nodes [OCC mgmt, OCC0, OCC1].


		[DEFAULT]
		
		# For cinder backup
			backup_driver=cinder.backup.drivers.ceph
		backup_ceph_conf=/etc/ceph/ceph.conf
		backup_ceph_user=cinder-backup
		backup_ceph_chunk_size=134217728
		backup_ceph_pool=helion-ceph-backups
		backup_ceph_stripe_unit=0
		backup_ceph_stripe_count=0
		restore_discard_excess_bytes=true

	The following table provides the description of the parameters.

	<table>
	<table style="text-align: left; vertical-align: top; width:650px;">
	<tr style="background-color: #C8C8C8;">
		<th> Parameters</th>
	<th>Descriptions </th>
		</tr>
		<tr>
	<td>backup_ceph_chunk_size = 134217728</td>
	<td>The chunk size, in bytes, that a backup is broken into before transfer to the Ceph object store.</td>
	</tr>
	<tr>
	<td>backup_ceph_conf = 
	/etc/ceph/ceph.conf</td>
	<td>Ceph configuration file to use.</td>
	</tr>
	<tr>
	<td>backup_ceph_pool = helion-ceph-backups</td>
	<td>The Ceph pool where volume backups are stored.</td>
	</tr>
	<tr>
	<td>backup_ceph_stripe_count = 0</td>
	<td>RBD stripe count to use when creating a backup image.</td>
	</tr>
	<tr>
	<td>backup_ceph_stripe_unit = 0</td>
	<td>RBD stripe unit to use when creating a backup image.</td>
	</tr>
	<tr>
	<td>backup_ceph_user = cinder</td>
	<td>The Ceph user to connect with. Default here is to use the same user as for Cinder volumes. If not using cephx this should be set to None.</td>
	</tr>
	<tr>
	<td>restore_discard_excess_bytes = True</td>
	<td>If True, always discard excess bytes when restoring volumes (that is, pad with zeroes).</td>
	</tr>
	  </table>


2. Restart the Cinder service by entering:

		service cinder-backup restart

<!---
	**Note**: For Cinder backup, the Cinder volume whose backup is required must be in a detached state. The volume should not be attached to any of the instances or Virtual Machines.

#####Creating a Cinder Backup

Once the Cinder service is restarted, you can create a backup of a Cinder volume. There are two ways to create a backup:

<<<<<<< HEAD
1. Horizon Overcloud Dashboard 
=======
1. The Horizon Overcloud Dashboard 
>>>>>>> 4b632df03c1544e4fd7572c21d99115cf875e2ce
2. The CLI from the overcloud controller node running the Ceph client

If you want to do a cinder backup, the cinder volume that needs to be backed up should be in a detached state. That volume should not be attached to any of the instances or VMs.


**Using the Horizon overcloud dashboard**

Create a volume backup  see [OpenStack User Guide](http://docs.openstack.org/user-guide/content/dashboard_manage_volumes.html)

1. Log into the  Horizon overcloud dashboard.
2. From the left panel, click **Projects** and then select **Compute**.
3. Click **Volumes**. 
4. Click the **Edit Volume** drop down list and select **Create Backup**. The **Create Volume Backup** page is displayed.
5. Enter the required information in the respective fields.
6. Click **Volume Backup**. A volume backup is created as shown below.

<img src="media/commercial-ceph-volume-create-backup.png"/)>



**Using the CLI**

To create a cinder backup, enter:
	 
	cinder backup-create [--container <container>] [--display-name <display-name>] [--display-description <display-description>] <volume>


The following example shows how to create a backup with the name of **deb7rawbackup** for an existing Cinder volume with the ID **0a2c6c62-627f-42d3-9b66-e4ba56db0ba7**, run:

		# cinder backup-create --display-name deb7rawbackup 0a2c6c62-627f-42d3-9b66-e4ba56db0ba7.

**Detailed procedure to create a backup using the CLI**

The following example provides a detailed procedure to create a backup uisng CLI.

1. Login to overcloud `controllermanagement`.
2. Enter the following command:

		cinder list

	Output:

			+--------------------------------------+-----------+-----------------------+------+-------------+----------+--------------------------------------+
			|                  ID                  |   Status  |      Display Name     | Size | Volume Type | Bootable |             Attached to              |
			+--------------------------------------+-----------+-----------------------+------+-------------+----------+--------------------------------------+
			| 0219d66e-d69d-4e28-bf4f-cb5f096696e3 | available |       Rsmallvol4      |  1   |     None    |  false   |                                      |
			| 0285ee63-4ebd-4cd1-915c-933c48503d00 |   in-use  |         Rvol1         |  10  |     None    |  false   | 54938de0-49dd-4b01-931e-dafcddc41518 |
			| 054bfa98-1d69-4cb8-b195-9b9481f5b8c7 |   in-use  |   Rwin2012Cowrawvol1  |  26  |     None    |   true   | 0bf98387-b0c9-4814-a2ef-f81c1ef1322e |
			| f628002a-6cc4-4e70-a98f-d575e36fca75 |   in-use  |    Rdeb7Cowrawvol3    |  10  |     None    |   true   | 54938de0-49dd-4b01-931e-dafcddc41518 |
			| ff8d13a5-3083-424b-a626-0b75cbe8cf66 | available |  cindervol_forbackup  |  15  |     None    |  false   |                                      |
			+--------------------------------------+-----------+-----------------------+------+-------------+----------+--------------------------------------+

		
2. Create the Cinder backup by entering:

			cinder backup-create --display-name cindervol_backup ff8d13a5-3083-424b-a626-0b75cbe8cf66

		Output:

				+-----------+--------------------------------------+
				|  Property |                Value                 |
				+-----------+--------------------------------------+
				|     id    | 60764712-c456-465a-828b-5f45d3a14ff5 |
				|    name   |           cindervol_backup           |
				| volume_id | ff8d13a5-3083-424b-a626-0b75cbe8cf66 |
				+-----------+--------------------------------------+
				

3. View a list of Cinder backups, enter: 

			cinder backup-list

		Output:

			+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+
			|                  ID                  |              Volume ID               |   Status  |        Name       | Size | Object Count |      Container      |
			+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+
			| 244aa3a1-b291-4cfe-9999-438f7611da2b | eb170c5e-d227-40ef-b515-b84b82c38eb0 | available |    Rvol6backup    |  15  |     None     | helion-ceph-backups |
			| 32ea7668-9179-433c-8fe3-44b98cd9d85b | 8aefafcc-4171-4c11-b900-362fbda40015 | available | ubuntu1404-backup |  10  |     None     | helion-ceph-backups |
			| 60764712-c456-465a-828b-5f45d3a14ff5 | ff8d13a5-3083-424b-a626-0b75cbe8cf66 |  creating |  cindervol_backup |  15  |     None     |         None        |
			| beeccb71-81e7-4860-8d38-add05a2e610d | eb170c5e-d227-40ef-b515-b84b82c38eb0 | available |    Rvol6backup    |  15  |     None     | helion-ceph-backups |
			+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+

4. To view details of a selected volume, enter:

	 		cinder backup-show 60764712-c456-465a-828b-5f45d3a14ff5

		Output:

			+-------------------+--------------------------------------+
			|      Property     |                Value                 |
			+-------------------+--------------------------------------+
			| availability_zone |                 None                 |
			|     container     |                 None                 |
			|     created_at    |      2014-10-01T18:14:50.000000      |
			|    description    |                 None                 |
			|    fail_reason    |                 None                 |
			|         id        | 60764712-c456-465a-828b-5f45d3a14ff5 |
			|        name       |           cindervol_backup           |
			|    object_count   |                 None                 |
			|        size       |                  15                  |
			|       status      |               creating               |
			|     volume_id     | ff8d13a5-3083-424b-a626-0b75cbe8cf66 |
			+-------------------+--------------------------------------+

5. Enter:

			rbd ls -l helion-ceph-backups

		Output:

				NAME                                                                                                                     SIZE PARENT FMT PROT LOCK
				volume-0a2c6c62-627f-42d3-9b66-e4ba56db0ba7.backup.base                                                                10240M          2
				volume-0a2c6c62-627f-42d3-9b66-e4ba56db0ba7.backup.base@backup.02c6df2c-d03a-44ad-847a-ce03b580ee23.snap.1412106395.75 10240M          2
				volume-0a2c6c62-627f-42d3-9b66-e4ba56db0ba7.backup.base@backup.c9c20a09-403e-4011-a3f8-2fea11a560ee.snap.1412042130.16 10240M          2
				volume-3adf1c83-2efa-4a1e-bef6-cdaffd13b489.backup.base                                                                 3072M          2
				volume-3adf1c83-2efa-4a1e-bef6-cdaffd13b489.backup.base@backup.cdd27130-1791-45f6-8b6e-cc284922b02e.snap.1412041965.31  3072M          2 


6. To view cluster utilization, enter:

			rados df

		Output:

			pool name       category                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB
			.rgw            -                       9411        51459            0            0           0       110460        85700       130906        51952
			.rgw.buckets    -                      20004       188711            0            0           0      6466145      4329147      8000069      1563078
			helion-ceph-backups -                   54343272        13300         2190            0           0        16295     40106569        38157     70071922

<<<<<<< HEAD
**Note**: you can delete a cinder volume by executing the following command:
cinder backup-delete <backup>
=======
**Note**: you can delete a Cinder volume by executing:
`cinder backup-delete`
>>>>>>> 4b632df03c1544e4fd7572c21d99115cf875e2ce

####Mount the Volume and Copy a new Image

Now, mount the volume and copy the new image file from the VM. To do so, perform the following steps.
	
1. Log in as root.
2. List the block devices by entering:
	
		lsblk
	
	Output:

		NAME                     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
		vda                      254:0    0    10G  0 disk
		aavda1                   254:1    0   243M  0 part /boot
		aavda2                   254:2    0     1K  0 part
		aavda5                   254:5    0   7.8G  0 part
		  aadebian-root (dm-0)   253:0    0   7.4G  0 lvm  /
		  aadebian-swap_1 (dm-1) 253:1    0   376M  0 lvm  [SWAP]
		vdb                      254:16   0    26G  0 disk
		vdc                      254:32   0    10G  0 disk
		vdd                      254:48   0    10G  0 disk
		vde                      254:64   0    10G  0 disk
		vdf                      254:80   0    10G  0 disk
		vdg                      254:96   0    10G  0 disk
		vdi                      254:128  0    15G  0 disk
		vdj                      254:144  0    15G  0 disk

3. To mount `dev` to `vol` enter:

		mount /dev/vdj /mnt/vol

	Output:

		mount: mount point /mnt/vol does not exist

4. To mount `dev` to `vol1` enter:

		mount /dev/vdj /mnt/vol1 

5. Change the directory by entering: 
 
		cd /mnt/vol1

6. List all the sub-directories by entering:

		ls -ltr

7. To view disk usage enter:

		df -h

	Output:

		Filesystem               Size  Used Avail Use% Mounted on
		rootfs                   7.3G  6.7G  248M  97% /
		udev                      10M     0   10M   0% /dev
		tmpfs                    397M  208K  397M   1% /run
		/dev/mapper/debian-root  7.3G  6.7G  248M  97% /
		tmpfs                    5.0M     0  5.0M   0% /run/lock
		tmpfs                    794M     0  794M   0% /run/shm
		/dev/vda1                228M   18M  199M   9% /boot
		/dev/vdj                  15G  847M   14G   6% /mnt/vol1

8. To mount enter:

		mount

	Output:

		sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
		proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
		udev on /dev type devtmpfs (rw,relatime,size=10240k,nr_inodes=506385,mode=755)
		devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)
		tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=406364k,mode=755)
		/dev/mapper/debian-root on / type ext4 (rw,relatime,errors=remount-ro,user_xattr,barrier=1,data=ordered)
		tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
		tmpfs on /run/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=812720k)
		/dev/vda1 on /boot type ext2 (rw,relatime,errors=continue)
		/dev/vdj on /mnt/vol1 type ext4 (rw,relatime,user_xattr,barrier=1,data=ordered)


	Now cinder volume has additional file system changes within the volume.	

9. To list the additional file system changes enter:

		ls-ltr

	Output:

		total 6328752
		drwx------ 2 root root      16384 Oct  1 05:22 lost+found
		-rwxr-x--- 1 root root  702939136 Oct  1 05:23 CentOS_65.qcow2
		-rw-r--r-- 1 root root   10870593 Oct  1 05:24 initrd.img-3.2.0-4-amd64
		-rw-r--r-- 1 root root 5766807552 Oct  2 00:34 Debian_7.raw


10. To view disk usage enter:

		df -h

	Output:

		Filesystem               Size  Used Avail Use% Mounted on
		rootfs                   7.3G  6.7G  248M  97% /
		udev                      10M     0   10M   0% /dev
		tmpfs                    397M  208K  397M   1% /run
		/dev/mapper/debian-root  7.3G  6.7G  248M  97% /
		tmpfs                    5.0M     0  5.0M   0% /run/lock
		tmpfs                    794M     0  794M   0% /run/shm
		/dev/vda1                228M   18M  199M   9% /boot
		/dev/vdj                  15G  6.2G  8.6G  42% /mnt/vol1


11. To unmount the volume enter:

		umount /dev/vdj

12. Enter:

		mount

	Output:

		sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
		proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
		udev on /dev type devtmpfs (rw,relatime,size=10240k,nr_inodes=506385,mode=755)
		devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)
		tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=406364k,mode=755)
		/dev/mapper/debian-root on / type ext4 (rw,relatime,errors=remount-ro,user_xattr,barrier=1,data=ordered)
		tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
		tmpfs on /run/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=812720k)
		/dev/vda1 on /boot type ext2 (rw,relatime,errors=continue)
		

###Restoring Data from a Cinder Backup

You can restore the backed up volume to a new volume or an existing volume.

In the following example, a new volumne is created and the data is restored to it.

To create a new volume and to restore data backup perform the following steps.

1. To create a volume enter:

		 cinder create --display-name restore_volume1 15 

	Output:

			+---------------------+--------------------------------------+
			|       Property      |                Value                 |
			+---------------------+--------------------------------------+
			|     attachments     |                  []                  |
			|  availability_zone  |                 nova                 |
			|       bootable      |                false                 |
			|      created_at     |      2014-10-01T21:48:42.727440      |
			| display_description |                 None                 |
			|     display_name    |           restore_volume1            |
			|      encrypted      |                False                 |
			|          id         | 1b4614f8-8069-4211-8a3e-797be5641964 |
			|       metadata      |                  {}                  |
			|         size        |                  15                  |
			|     snapshot_id     |                 None                 |
			|     source_volid    |                 None                 |
			|        status       |               creating               |
			|     volume_type     |                 None                 |
			+---------------------+--------------------------------------+


2. Execute the following command to the cinder backup restore:


		cinder backup-restore --volume-id restore_volume1 60764712-c456-465a-828b-5f45d3a14ff5


3. View the Cinder backup by entering:

		cinder backup-list

	Output:

		+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+
		|                  ID                  |              Volume ID               |   Status  |        Name       | Size | Object Count |      Container      |
		+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+
		| 02c6df2c-d03a-44ad-847a-ce03b580ee23 | 0a2c6c62-627f-42d3-9b66-e4ba56db0ba7 | available |   deb7rawbackup   |  10  |     None     | helion-ceph-backups |
		| 244aa3a1-b291-4cfe-9999-438f7611da2b | eb170c5e-d227-40ef-b515-b84b82c38eb0 | available |    Rvol6backup    |  15  |     None     | helion-ceph-backups |
		| 32ea7668-9179-433c-8fe3-44b98cd9d85b | 8aefafcc-4171-4c11-b900-362fbda40015 | available | ubuntu1404-backup |  10  |     None     | helion-ceph-backups |
		| 60764712-c456-465a-828b-5f45d3a14ff5 | ff8d13a5-3083-424b-a626-0b75cbe8cf66 | restoring |  cindervol_backup |  15  |     None     | helion-ceph-backups |
		| beeccb71-81e7-4860-8d38-add05a2e610d | eb170c5e-d227-40ef-b515-b84b82c38eb0 | available |    Rvol6backup    |  15  |     None     | helion-ceph-backups |
		| c9c20a09-403e-4011-a3f8-2fea11a560ee | 0a2c6c62-627f-42d3-9b66-e4ba56db0ba7 | available |  RDebian7_backup  |  10  |     None     | helion-ceph-backups |
		| cdd27130-1791-45f6-8b6e-cc284922b02e | 3adf1c83-2efa-4a1e-bef6-cdaffd13b489 | available |      Rbackup1     |  3   |     None     | helion-ceph-backups |
		+--------------------------------------+--------------------------------------+-----------+-------------------+------+--------------+---------------------+



4. View the Cinder list by entering:

		cinder list

	Output:

		+--------------------------------------+------------------+-----------------------+------+-------------+----------+--------------------------------------+
		|                  ID                  |      Status      |      Display Name     | Size | Volume Type | Bootable |             Attached to              |
		+--------------------------------------+------------------+-----------------------+------+-------------+----------+--------------------------------------+
		| 0219d66e-d69d-4e28-bf4f-cb5f096696e3 |    available     |       Rsmallvol4      |  1   |     None    |  false   |                                      |
		| 0285ee63-4ebd-4cd1-915c-933c48503d00 |      in-use      |         Rvol1         |  10  |     None    |  false   | 54938de0-49dd-4b01-931e-dafcddc41518 |
		| 054bfa98-1d69-4cb8-b195-9b9481f5b8c7 |      in-use      |   Rwin2012Cowrawvol1  |  26  |     None    |   true   | 0bf98387-b0c9-4814-a2ef-f81c1ef1322e |
		| 1b4614f8-8069-4211-8a3e-797be5641964 | restoring-backup |    restore_volume1    |  15  |     None    |  false   |                                      |
		+--------------------------------------+------------------+-----------------------+------+-------------+----------+--------------------------------------+
		
	Once the backup is created the volume name remains the same as the cinder-backup name.


5. Verify the volume name by entering:

		cinder list

	Output:

		+--------------------------------------+-----------+-------------------------------------+------+-------------+----------+--------------------------------------+
		|                  ID                  |   Status  |             Display Name            | Size | Volume Type | Bootable |             Attached to              |
		+--------------------------------------+-----------+-------------------------------------+------+-------------+----------+--------------------------------------+
		| 0219d66e-d69d-4e28-bf4f-cb5f096696e3 | available |              Rsmallvol4             |  1   |     None    |  false   |                                      |
		| 0285ee63-4ebd-4cd1-915c-933c48503d00 |   in-use  |                Rvol1                |  10  |     None    |  false   | 54938de0-49dd-4b01-931e-dafcddc41518 |
		| 1b4614f8-8069-4211-8a3e-797be5641964 | available |         cindervol_forbackup         |  15  |     None    |  false   |                                      |
		+--------------------------------------+-----------+-------------------------------------+------+-------------+----------+--------------------------------------+

##Verify the attachment of volume to VM

Perform the following to verify the attachment of volume to VM and content verification.

1. Change the directory by entering: 
 
		cd /mnt/vol1

2. Execute the following command:

		ls -ls

**Output**

	total 697100
	686468 -rwxr-x--- 1 root root 702939136 Oct 1 05:23 CentOS_65.qcow2
	10616 -rw-r--r-- 1 root root 10870593 Oct 1 05:24 initrd.img-3.2.0-4-amd64
	16 drwx------ 2 root root 16384 Oct 1 05:22 lost+found


####Understanding Volume Snapshots

Volume snapshots are saved in a Cinder pool.

######Creating a Volume Snapshot for Backup

You can create new and identical volumes by taking snapshot of the volume.  

There are two ways to create a backup:

1. Horizon Overcloud dashboard 
2. Command Line Interface (CLI)

**Notes**

Mind the following things while taking snapshots:

- The volume must be detached and must be in an **available** status to take a snapshot of it. An error occurs if you try to snapshot a used volume.
- To function properly, keep the original volume, whose snapshot was taken. If the original volume is deleted then the snapshot becomes unusable.

**Using the Horizon overcloud dashboard**

To create a snapshot from volume, see [OpenStack User Guide](http://docs.openstack.org/user-guide/content/dashboard_manage_volumes.html)

**Using the CLI**

The following steps show how to create a snapshot.

1. To create a snapshot, enter:

		nova volume-snapshot-create --force [TRUE or FALSE] --display_name [DISPLAY_NAME] --display_description [DISPLAY_DESCRIPTION] [VOLUME_ID]

2. To view the snapshot, enter:

		nova volume-snapshot-list

**Output**

	+--------------------------------------+--------------------------------------+-----------+-------------------------------------+------+
	| ID | Volume ID | Status | Display Name | Size |
	+--------------------------------------+--------------------------------------+-----------+-------------------------------------+------+
	| 1f9cae44-e3c9-4326-a1f9-68aeea34d672 | 0285ee63-4ebd-4cd1-915c-933c48503d00 | available | snapshot for Rinstgeneral_snapshot | 10 |
	| 2758b62c-8f2a-482c-bf2c-9183c8304227 | f628002a-6cc4-4e70-a98f-d575e36fca75 | available | snapshot for Rinstgeneral_snapshot | 10 |
	| 32267733-bf04-4180-a1ea-bc133726bb7b | 525bd6a2-05cc-4bba-9a6a-f8e8a3f6ce68 | available | snapshot for Rinstgeneral_snapshot | 10 |
	| 4b1e37f6-04f6-41c4-a80c-34ce8d8c743a | 386ed069-71c4-428b-9ecc-f21d572d74b2 | available | snapshot for Rdeb8cowinst7_snapshot | 10 |
	+--------------------------------------+--------------------------------------+-----------+-------------------------------------+------+


---->

---

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>
