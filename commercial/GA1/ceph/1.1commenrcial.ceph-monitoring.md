---
layout: default
title: "HP Helion OpenStack&#174; Edition: HP Helion Ceph"
permalink: /helion/openstack/1.1/ceph-rados-gateway/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Binamra S

---
<!--UNDER REVISION-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p> --->

#Helion OpenStack Ceph Monitoring


Administrative monitoring is available through the Helion built-in monitoring packages running on the UnderCloud:  Icinga.  Icinga is configured as follows:

1.	Icinga: This is the main alerting server that runs each check periodically on every host
2.	Icinga-web: This is the UI tool used to view the monitoring results
3.	Check_mk is the utility that runs on the undercloud controller and each of the hosts that must be monitored. This utility runs some local checks and send results back to the main server.
4.	`mk_livestatus` gathers the results of the checks

Icinga-web can be accessed through  http://&lt;undercloudcontrollerip>/icinga. The default login credentials are as follows:

* Username:icingaadmin
* Password:icingaadmin



The servers for Helion OpenStack Overcloud and the Undercloud are monitored. In the Service Details section on the UI, there is a list of the services and applications monitored by Icinga.

For Ceph Monitoring, the following scripts run on Helion OpenStack Overcloud controller nodes and report back the check results to the Icinga service:

1.	`check_ceph_osd_status.sh` which monitors the status of the available OSDs (if all up and in the cluster)
2. `check_ceph_pg_status.sh` which monitors the status of the placement groups (if all are active and clean)
3.	`check_ceph_health.sh` which monitors the overall health of the ceph storage cluster

The Ceph checks run every 10 secs and reported back in the Icinga UI as:

1. `Ceph_OSD_Status`
2. `Ceph_PG_Status`
3. `Ceph_Cluster_Health`

The ceph client installer script installs the  ceph health monitoring scripts into `/usr/lib/check_mk_agent/local/` directory. The `check_mk` utility detects the presence of those scripts automatically via a cron job that runs periodically and begins using them to add the reporting details into the UI.
 


