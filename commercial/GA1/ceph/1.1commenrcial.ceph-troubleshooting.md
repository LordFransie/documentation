---
layout: default
title: "HP Helion OpenStack&#174; 1.1: troubleshooting tips"
permalink: /helion/openstack/1.1/ceph-troubleshooting/
product: commercial

product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<p style="font-size: small;"> <a href="/helion/openstack/1.1/ceph-rados-gateway-keystone-authentication/">&#9664; PREV</a> | <a href="/helion/openstack/siteindex/">&#9650; UP</a> | <a href=" /helion/openstack/1.1/ceph-glossary/">NEXT &#9654;</a> </p>


# HP Helion OpenStack&#174; 1.1 troubleshooting tips #

The following tips will help you troubleshoot errors:

* Verify that the http proxy settings are correct in both `/etc/environment` and `/etc/apt/apt.conf` for connecting to external resources.

* The Firefly version of Ceph packages has a default replication set to 3x (three times). Therefore you need a minimum of three OSDs.

* If a `Decrypt error` occurs from any of the Helion nodes with glance, cinder, or nova, make sure that all the Ceph cluster nodes are in same time zone. HP recommends that you configure a NTP server in the `/etc/ntp.conf` file on all Ceph cluster nodes.  After making changes, restart NTP [`/etc/init.d/ntp restart`]. Install the `ntp` package, if it is not there.

* Make sure the Ceph cluster uses the same IP range as the Helion OpenStack overcloud nodes. Also, make sure the IP range for the Ceph cluster does not conflict with those used by Helion setup.

* Be sure to check the log files in `/var/log/ceph/` for each node. Any errors or exceptions are useful for troubleshooting.

* Any changes you make to the Ceph configuration file requires a restart for the changes to take affect.

* You cannot change the default Cephx Authentication to `None` once the cluster is up and running. You will have to purge and reinstall the Ceph packages and build the cluster. 

* Enable logging if you encounter a problem at any of these steps:
	
	To do this, edit `glance-api.conf`, `cinder.conf` and `nova.conf` files with the following in the default section.

		debug = True
		verbose = True

	Restart Glance, Cinder, and Nova services respectively.

	On HP Helion Commercial, the logs for Glance and Cinder are stored by default in the following directories:

		Glance - /var/log/glance
		Cinder - /var/log/upstart

	Gather these logs and contact HP support team. 

* If any Placement Group (PG)-related issues occur in the Ceph cluster, refer to the following link:

	[http://docs.ceph.com/docs/master/rados/operations/placement-groups/](http://docs.ceph.com/docs/master/rados/operations/placement-groups/)

###Choosing the number of the placement group

If there are more than 50 OSDs, we recommend approximately 50-100 PGs per OSD to balance resource usage, data durability and distribution. To get a baseline for a single pool of objects, use the following formula:
		
<img src="media/commercial-ceph-formula-placement-group.png"/)>


Where pool size is either the number of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by `ceph osd erasure-code-profile get`).
Round up the results to the nearest power of two. Rounding up is optional, but recommended for CRUSH to evenly balance the number of objects among PGs.

For example, for a cluster with 200 OSDs and a pool size of 3 replicas, estimate the number of PGs as follows:

	(200 * 100)
	----------- = 6667. Nearest power of 2: 8192
     3

### Ceph OSD cleanup ###

To do a Ceph OSD cleanup:

1. Enter:
	`ceph osd crush remove osd.x`
1. Enter: `ceph auth del osd.x`
1. Run this command multiple times until it says osd.x is already down. Then run the next step immediately:` ceph osd down osd.x` 
1. If this command throws a warning saying osd is busy, repeat the previous step:  ` ceph osd rm osd.x` 
2. Enter: `umount <osd data drive (as specified in playbook)>`
1. Enter: `rm –r /var/lib/ceph/osd/ceph-x`
1. Remove the `osd` entry from `ceph.conf` and `scp ceph.conf` to other cluster nodes.
1. Verify that `osd` was removed by executing: `- sudo ceph –w` 

## Next Steps

[Ceph Manual Installation]( /helion/openstack/1.1/ceph-manual-install/)


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>
