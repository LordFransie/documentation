---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Ceph RADOS Gateway- Keystone Authentication"
permalink: /helion/openstack/1.1/ceph-rados-gateway-keystone-authentication/
product: commercial

product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F, Binamra S

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<p style="font-size: small;"> <a href="/helion/openstack/1.1/ceph-rados-gateway-pools/">&#9664; PREV</a> | <a href="/helion/openstack/siteindex/">&#9650; UP</a> | <a href=" /helion/openstack/1.1/ceph-rados-gateway-pools/">NEXT &#9654;</a> </p>


# HP Helion OpenStack&#174; 1.1: Ceph RADOS Gateway- Keystone Authentication

Integrating the RADOS Gateway with the Helion OpenStack identity service (Keystone) sets up the Gateway to authorize and accept Keystone users automatically. Users are created in RADOS pools provided they have valid Keystone token. For more details see, [Ceph OpenStack documentation](http://ceph.com/docs/master/radosgw/keystone/ "The RADOSGW and Keystone authentication")  

Helion OpenStack 1.1 HAProxy can be used to enable high availability and load balancing for RADOS Gateway nodes. With HAProxy, Swift requests are load balanced between two gateway nodes. If one of them goes down, then all requests are processed by the remaining live node. Once the failed node recovers, load balancing is re-enabled. Load balancing is done using the round robin algorithm so that each node is used in turn, and load balancing is fair. 

###Assumptions
The following assumptions apply to this configuration:

* Apache2/FastCGI without `100 continue support`

* Two RADOS gateway nodes with two distinct users sharing same RADOSGW keyring

###Steps
The steps to achieve integration of Helion OpenStack HAProxy, Keystone, and RADOSGW are:

#### Ceph Admin Node ####

* On the Ceph admin node, edit the `ceph.conf` file to include the following (the Keystone URL is Helion VIP and the Keystone admin token is taken from `/etc/keystone/keystone.conf` on the controller node):

		[global]
		rgw keystone url = https://192.0.2.21:5000 
		rgw keystone admin token = 337b8816f019a04396a2e00e65e6c30ea96ba59b
		rgw keystone accepted roles = Member, admin, swiftoperator
		rgw keystone token cache size = 500
		rgw keystone revocation interval = 500
		rgw s3 auth use keystone = true
		rgw nss db path = /var/ceph/nss

* Re-deploy the `ceph.conf` file on all nodes in the Ceph cluster and on the Helion Controller nodes.

####Helion Controller Nodes

* On any controller node, register the RADOS gateway endpoint with Keystone. The IP address used is the Helion VIP address and port 10080 is the RADOSGW port:

		keystone service-create --name swift --type object-store [Note down service ID]

		keystone endpoint-create --service-id e001b96dc4b54a6c9f672418c21eb132 --publicurl https://192.0.2.x:10080/swift/v1 --internalurl https://192.0.2.x:10080/swift/v1 --adminurl https://192.0.2.x:10080/swift/v1


* Execute `keystone endpoint-get --service object-store`. This should return a pointer to the Ceph store and not default to the Swift store. Before proceeding:
	* Download and backup any images from Glance.
	* Delete existing Swift endpoints if the above command returns a Swift store.
	* Configure the Ceph store as explained in this section.
	* Re-upload Glance images and use the Ceph store for Glance images going forward

* On all controller nodes, edit the `haproxy.cfg` file at `/etc/haproxy/haproxy.cfg `as shown below (IP address used is Helion VIP):

		listen radosgw
 		bind 192.0.2.x:10080
		balance roundrobin
 		server gateway 192.x.x.x:443 check inter 2000 rise 2 fall 5 maxconn 1500
 		server gateway1 192.x.x.x:443 check inter 2000 rise 2 fall 5 maxconn 1500


* Restart the HAProxy service on all controller nodes by entering:

		service haproxy restart

* On all controller nodes, `edit /etc/stunnel4/from-heat.conf` where the IP address used is the physical IP address of the respective controller node. For example:

		[radosgw]
		accept = 192.0.2.x:10080
		connect = 127.0.0.1:10080
		ciphers=ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS


* Restart the `stunnel4service` on all controller nodes as shown below:

		/etc/init.d/stunnel4 restart

* Open port 10080 on all controller nodes as shown below:

		add-rule INPUT -p tcp --dport 10080 -j ACCEPT
		iptables-save|grep 10080


* On all controller nodes, edit the  `/etc/apache2/sites-enabled/keystone_modwsgi.conf` to include `WSGIChunkedRequest` as shown below (for more details, see [http://tracker.ceph.com/issues/7796](http://tracker.ceph.com/issues/7796)):

		<VirtualHost *:35357>
		......
		
		<Directory /etc/keystone>
		......
		WSGIChunkedRequest On
		......
		</Directory>
		......
		</VirtualHost>
		
		<VirtualHost *:5000>
		......
		<Directory /etc/keystone>
		......
		WSGIChunkedRequest On
		......
		</Directory>
		......
		</VirtualHost>

* Restart the Apache2 service on all controller nodes by entering: 
	
		service apache2 restart

<!--
*  On any controller node, convert OpenSSL certificates that Keystone uses to the NSS database format. To perform this, make sure the `certutil` package is available on controller or gateway nodes.

	* `apt-get install libnss3-tools`
	* `mkdir /var/ceph/nss`
	* `openssl x509 -in /mnt/state/etc/keystone/ssl/certs/ca.pem -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"`
	* `openssl x509 -in /mnt/state/etc/keystone/ssl/certs/signing_cert.pem -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"`
-->
####Ceph RADOS Gateway Nodes

1. Create the following directory on gateway node(s) by entering:

		mkdir /var/ceph/nss

2. As root, convert OpenSSL certificates that Keystone uses to the NSS DB format. To do this, make sure that the `certutil` package is available on  the gateway nodes. 

	-	Copy `ca.pem` and `signing_cert.pem` in `/mnt/state/etc/keystone/ssl/certs/` on any controller node to the `/tmp` directory on the gateway node. 
    
    		openssl x509 -in /tmp/ca.pem -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
    
    		openssl x509 -in /tmp/signing_cert.pem -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"
    
3. Copy following certificates from Helion setup to `/etc/ssl/certs`:
	* `eca` keys from the seed node
	* `ca-certificates.crt` from controller node 
	* `from-heat.cr`t and `from-heat.key` from the Swift or controller nodes    


4. On the RADOS gateway node(s), edit the `/etc/apache2/sites-available/rgw.conf` file as shown below:

		SSLEngine on
		SSLCertificateFile /etc/ssl/from-heat.crt
		SSLCertificateKeyFile /etc/ssl/from-heat.key
		SSLCACertificateFile /etc/ssl/certs/eca.crt
		SetEnv SERVER_PORT_SECURE 443

5. Restart the Apache2 service by entering:

		/etc/init.d/apache2 restart

6. Restart the RADOSGW service by entering:

		/etc/init.d/radosgw restart



####Validating the Configuration

1. Make sure proxy is not set on the gateway nodes by entering:

		unset http_proxy https_proxy

2. Run the RADOSGW service in debug mode to make sure that there are no obvious errors in the logs on both gateway nodes by entering:

		/usr/bin/radosgw -d -c /etc/ceph/ceph.conf --debug-rgw 20 --rgw-socket-path=/var/run/ceph/ceph.radosgw.gateway.fastcgi.sock

3. From the controller node, make a Swift v1 request like that shown below. In this example the IP address used is the Helion VIP address followed by the RADOSGW service port. It is assumed that `s3User` is already created using the `radosgw-admin` command and that correct credentials for `s3User` were used in making the request. Output should list containers, if any.

		swift --insecure -V 1.0 -A https://192.0.2.x:10080/auth/v1.0 -U s3User:swiftUser -K abc list

4. From the controller node, make a Swift v2 request using Keystone. Ceph Object Gateway's user:subuser tuple maps to the tenant:user tuple expected by Swift.  The IP address used will is the Helion VIP address followed by the Keystone service port. Admin credentials are considered. Output should list containers, if any.

		swift -V 2.0 -A http://192.0.2.21:5000/v2.0 -U admin:admin -K abc list

5. If  these Swift calls fail:
	* Make sure port 10080 is open on all controller nodes.
	* Verify that certificates in use have not expired. If any have expired, copy the respective certificates from Helion nodes as explained above,
6. From the controller node, get the admin tenant ID by entering:

		Keystone tenant-list

	Output:

		+----------------------------------+---------+---------+
		| id 							   | name    | enabled |
		+----------------------------------+---------+---------+
		| 627770d0c17c4423b8c27a2607e60798 | admin 	 | True    |
		| aa70711bd69e4958ac239e2564c18054 | demo    | True    |
		| 250bf66045814455a5b3c3e6c7fb7c19 | service | True    |
		+----------------------------------+---------+---------+

7. Verify that the admin user is created in the RADOS pool as shown below, and make sure the admin user is indeed present

		rados --pool .users.uid ls
		s3User
		s3User.buckets
		627770d0c17c4423b8c27a2607e60798
		627770d0c17c4423b8c27a2607e60798.buckets

<!--not required
## Next Steps

[Ceph Related Topics]( /helion/openstack/1.1/ceph/ceph-related-topics/)
-->

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

