---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Ceph Configuration"
permalink: /helion/openstack/1.1/ceph-hp-helion-openstack-ceph-configuration/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->


# HP Helion OpenStack&#174; 1.1: Ceph Configuration

This section describes the integration of HP Helion OpenStack 1.1 and Ceph Firefly 0.80.7 on a hLinux3.14.29.4 kernel.

Before configuring Helion OpenStack Ceph make sure that HP Helion OpenStack 1.1 is installed successfully. 

**Note**: The Ceph client script is only run on the Helion controller and compute nodes during a manual deployment of the Ceph cluster. If you are using Ansible to configure your Helion nodes and Ceph cluster, you can ignore the manual section.


###Ceph Client Install

The Helion Overcloud Controller and compute nodes are consumers of the storage services provided by the Ceph cluster. To enable them to act as Ceph clients, perform the following steps (either the automated or manual process).

**Note**: See note above. If you are using Ansible to configure your Helion nodes and Ceph cluster, you can ignore the manual section.

####Automated Install

The `ceph_client_setup.tar` contains Debian packages, Helion OpenStack Ceph configuration scripts, and the patch file to make the required alterations to the Nova, Glance, and Cinder configuration files. These Debian packages must be installed while installing the Ceph firefly 0.80.7 client packages on the HP Helion overcloud controller and compute nodes. The script present in the tar file helps to automate the installation process.

Once the Ceph cluster is fully operational, run the tar file on the Helion controller nodes (occ0, occ1, and oc mgmt nodes) and the compute node. At the final stage of the installation process, the Ceph client confirms connectivity to the Ceph cluster.


#####Running the automated scripts

Perform the following steps to run the automated installation scripts on the Helion overcloud controller nodes and the compute nodes. 

For more details about the Ceph storage cluster with Glance integration, refer to [http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance ](http://ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack%20glance )


1. Untar `ceph_client_setup.tar` in root home on each of the Helion overcloud controller and compute nodes where the Ceph client needs to be installed.

		/home

	A directory named `ceph_client_setup` is created. There are several sub directories created under the `ceph_client_setup` directory.


2. Copy `ceph.conf` file and the keyrings from the admin node to the install directories. 

3. Copy the Ceph cluster config files  from the ceph admin node into the `ceph_cluster_config_files` directory. 


4. The script `fixuuid.sh` in the `setup_scripts` directory modifies the Helion configuration file patches with the new UUID from the cephx authentication process. To execute  
`fixuuid.sh`, enter:

		fixuuid.sh *<the new-UUID*>


	The new Cephx UUID is passed as an argument to the `fixuuid.sh` script. This will load the new UUID into the patch files that will be used to modify the Helion config files.

5. Run `ceph_client_install.sh` on one of the controller nodes or compute nodes.

	**Note**: The installer script is located at `/home/ceph_client_setup/setup_scripts/ceph_client_install.sh`


The installation script does the following:

1. Checks if the file is untarred in the correct location (`/home/ceph_client_setup`) and exits if the script is not available. 
2. Verifies that the ceph cluster `ceph.conf` and the keyring files are copied in the correct location (`/home/ceph_client_setup/ceph_cluster_config_files`/) and exits if they are not present. If they are present in the correct location, the `/etc/ceph` directory is created and the `ceph.conf` and the keyring files are copied into the `/etc/ceph` directory.
3. The script will search for the following files in `/home/ceph_client_setup/ceph_cluster_config_files/`

	a. ceph.conf - main Ceph configuration file

	b. ceph.client.nova.keyring - Nova user authorization key file

	c. ceph.client.cinder.keyring - Cinder user authorization key file

	d. ceph.client.cinder-backup.keyring - Cinder backup user authorization key file

	e. ceph.client.glance.keyring - Glance user authorization key file

	f. ceph.client.admin.keyring - admin user authorization key file

	g. ceph.client.radosgw.keyring - RADOSGW authorization key file


 
	**NOTE**: if a RADOS gateway is not going to be used for this installation, create an empty file with this name in the `/home/ceph_client_setup/ceph_cluster_config_files/` directory so the script will still find a file with the expected name. 

4. Checks the availability of the Ceph client Debian packages in `/ceph_client_setup/ceph_client_debs/` and exits the script if it is unavailable.

5. Checks whether the system is a hLinux system and verifies the existence of the `/etc/apt/sources.list` file. If it meets these requirements, then a backup copy of the original `sources.list` file is created in `/home/ceph_client_setup/orig_backup/` directory and then the local Ceph packages location is added to the file as a valid local repository.

6. Does an `apt-get update` and `apt-get install` of the Ceph client packages once the repository is added to the `apt sources.list` file. If `apt-get update` and `apt-get install` of the Ceph client packages executes without errors,  `dpkg` is executed to verify the installation of the correct packages.

7. Checks the `ceph` command to verify if it is available and executable. If the `ceph` command is available and executable then it is executed to verify whether the health of the Ceph cluster can be determined. If the Ceph cluster health shows as `HEALTH_OK`, the script displays the status and continues. 

The following sub-scripts run upon successful completion of the above steps:

**`copy_icinga_monscripts.sh`**- copies the Ceph cluster monitoring scripts in the iCinga script directory on the Helion overcloud controller nodes only.

**`patch_config_files.sh`** - patches the Helion configuration files with the changes required for  Ceph communication.

**`ceph_pythonlinks_create.sh`** - creates softlinks to the necessary Ceph Python library files in the Helion Python directories.

**`create_helion_pools.sh`** - creates the required Helion pools in the Ceph storage (only runs on  the overcloud controller node and management node)

**`helion_service_restarts.sh`** - restarts the Helion services.


<!--


####Availability of Ceph Client Script

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
	<th > Description</th>
	<th>File Location </th>
</tr>
	<tr>
<td>Ceph client package file to be installed on the running Controller and Compute nodes in the
HP Helion OpenStack commercial build</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<br /></td>
</tr>
  <table>

As a sanity-check, cross check with the attached "dpkg -l" output after Ceph package installation:

<table>
<table style="text-align: left; vertical-align: top; width:650px;">
<tr style="background-color: #C8C8C8;">
	<th > Description</th>
	<th > Description</th>	
	<th>File Location </th>
</tr>
	<tr>
<td>Controller node</td>
<td>DPKG -l output after Ceph package install</td>
<td>`https://helion.hpwsportal.com` Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files.<tdt>
</tr>
<tr>
<td>Compute node</td>
<td>DPKG -l output after Ceph package install</td>
<td>https://helion.hpwsportal.com Click on the workloads category on the left side and then you click on the storage subcategory to find all the Ceph related files. <tdt>
</tr>
  </table>
-->
<!--

####Verify the Ceph Version

* Execute the following command to verify the Ceph version on the overcloud nova compute node:

		# dpkg -l|grep ceph

The following example displays the ceph version:

	root@overcloud-novacompute0-c6y5lbj2hvlu:/home# dpkg -l|grep ceph
	ii ceph 0.80.7-1+hLinux amd64
	[hLinux]distributed storage and file system
	ii ceph-common 0.80.7-1+hLinux amd64
	[hLinux]common utilities to mount and interact with a ceph storage cluster
	ii libcephfs1 0.80.7-1 amd64
	Ceph distributed file system client library
	ii python-ceph 0.80.7-1+hLinux amd64
	[hLinux]Python libraries for the Ceph distributed filesystem



-->

## Next Steps

[Ceph RADOSGW Authentication]( /helion/openstack/1.1/ceph-hp-helion-openstack-ceph-configuration/)

----
<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>












