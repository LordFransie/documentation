---
layout: default
title: "HP Helion OpenStack&#174; HP Helion Ceph Prerequisites"
permalink: /helion/openstack/1.1/ceph/prerequisite/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Binamra S

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->


#Ceph Automated Install

The following diagram depicts the installation workflow of HP Helion OpenStack&#174;1.1.

<img src="media/ceph-installation-flow.png"/)>

The following diagram depicts the logical architecture of Ceph.

<img src="media/ceph-logical-architecture.png"/)>

Ceph-automation tools are bundled into two tarballs:

* cephsetup.tar
* cephconfiguration.tar

These tarballs can be obtained from [https://helion.hpwsportal.com](https://helion.hpwsportal.com)

###cephsetup.tar 

cephsetup.tar must be downloaded and copied to Helion seed VM. cephsetup.tar is bundled with the following packages:

* images
* helion-patch
* node-provisioner

**Images**

Ceph-automation provides two types of image:

* ceph-admin-image – includes all the packages required to run configuration scripts. This image are used on Ceph admin node.
* ceph-cluster-image – includes all the packages for setting up `ceph-monitor`, `ceph-osd` and `ceph-rados` Gateway. ceph-cluster-image is shared across all the nodes participating in cluster, except ceph admin node.

**helion-patch**

* `hp_ced_check_deployed_images`
* `hp_ced_pre_update_checks.sh`
* `register-cephnodes.sh` 

**node-provisioner**

The deployment of these images are through provisioning tool that runs from seed VM.

1. Login to seed VM on your Helion setup.

		ssh root @ <seed IP address>


2. Create a directory on any location 

	mkdir /helion-ceph dir 

3. Download the tar file from [https://helion.hpwsportal.com](https://helion.hpwsportal.com) 
4. Untar and copy the downloaded file to `/helion-ceph` folder on the seed VM.

Following directories are present under `/helion-ceph` on seed VM

* images
* node-provisioner

1. From seed VM, execute the following command to view the contents in the `/helion-ceph/images`

	For example:

		root@hLinux:~/helion-ceph/images# ls
		bm-deploy-kernel
		bm-deploy-ramdisk
		overcloud-ceph-admin-initrd
		overcloud-ceph-admin-vmlinuz
		overcloud-ceph-admin.qcow2
		overcloud-ceph-cluster-initrd
		overcloud-ceph-cluster-vmlinuz
		overcloud-ceph-cluster.qcow2

2. From seed, execute the following command to  view the contents  in the `/helion-ceph/node-provisioner`.

	For example:

		/helion-ceph/node-provisioner# ls
		baremetal.csv
		cleanup.py
		common.py
		orchestration.json
		orchestration.py	

		/helion-ceph/node-provisioner/server# ls
		app.wsgi
		bottle.py
		httpd.conf
		server.json
		 server.py

		/helion-ceph/node-provisioner/client# ls
		baremetal.csv
		common.py
		orchestration.json
		orchestration.py

3.  Modify `/server/server.json` to include OpenStack credentials from undercloud (stackrc), network, and keypair that you want to use.


Sample of `server.json`

	root@hLinux:~/helion-ceph/node-provisioner/server# cat server.json
	{
	    "authentication": {
	       "HOST": "127.0.0.1",
	       "PORT": "8085",
	       "OS_VERSION": "2",
	       "OS_USER": "admin",
	       "OS_PASSWORD": "de44ff17c309c7b2a74465104cd59728b121dd60",
	       "OS_TENANT_NAME": "admin",
	       "OS_AUTH_URL": "http://<undercloud IP>:5000/v2.0",
	       "keypair": "cephadmin",
	       "netid": "823ad730-6c69-42e6-b455-8a8173ceae81"
	    }
	}

Description of server.json

**HOST** - local machine IP which will be your seed IP

**PORT** - same 8085 port can be used as service port

**OS_VERSION** - OpenStack API version. Should be set to "2"

**OS_USER** - this is from the undercloud stackrc file. For example: **admin"**

**OS_PASSWORD** - this is from the undercloud stackrc file. For example: "de44ff17c309c7b2a74465104cd59728b121dd60"

**OS	&#95;TENANT_NAME": "admin"** - this is from the undercloud stackrc file

**OS	&#95;AUTH_URL"** -  undercloud IP. For example: "http://192.168.51.23:5000/v2.0"

**keypair**- "cephadmin" - you will have to generate from undercloud node**["nova keypair-add cephadmin > cephadmin.pem"**

"netid": Execute **["nova network-list"]** from undercloud to get the netid. For example: "823ad730-6c69-42e6-b455-8a8173ceae81-


Directory list that shows `pem` file is present in undercloud node. To know the file,

1. Login to undercloud.

		# ssh heat-admin@<undercloud IP address>

2. Execute the following command.

 		ls -ls

 	For example:

		root@undercloud-undercloud-zyjo2vylo3tb:~# ls -ls
		total 12
		4 -rw------- 1 root       root       1680 Feb 10 22:17 cephadmin.pem
		4 -rw------- 1 heat-admin heat-admin  347 Feb 11 21:12 overcloud.stackrc
		4	-rw-r--r-- 1 root       root        311 Feb 10 20:34 stackrc


**<Paul we need to have some heading for the following content. Can you please insert it>**


1.	Open a new terminal.
2.	Login to seed and launch a server from `/helion-ceph/node-provisioner/server/' directory  using `./server.py' script.

	When you start the server, the following information is displayed. 

		root@hLinux:~/helion-ceph/node-provisioner/server# ./server.py
		Bottle v0.13-dev server starting up (using WSGIRefServer())...
		Listening on http://192.168.51.22:8085/
		Hit Ctrl-C to quit.

3. Open another seed terminal window and modify `orchestration.json`  file under `/helion-ceph/node-provisioner/client/orchestration.json`

Your `baremetal.csv` inputs should match with the flavor list. For example you need to specify the right Ram, vCPUs, disk size in "GB" and version number.

* Update `"ws_url"` with url of the seed node where server is running

		"ws_url": "http://192.168.51.22:8085/"

* `"imagepath"` to include path to images (deploy images + qcow + ramdisk + kernel):

		"imagepath": "/root/helion-ceph/images/"

            You can make changes to the below high lighted ones based on your setup.

	**Sample**

		{
		"authentication": {
		       "ws_url": "http://192.168.51.22:8085/"
		    },
		    "api":{
		       "imagepath": "/root/helion-ceph/images/",
		       "deploy-image-prefix": "bm-deploy"
		    },
		    "orchestration":{
		       "hypervisorsleepduration": "300",
		       "hypervisorsmoniteringfrequency": "10",
		       "bootsleepduration": "1200",
		       "bootinitialwaitduration": "30",
		       "hypervisortype": "baremetal",
		      "hypervisordriver": "ironic",
		       "bootmoniteringfrequency": "5",
		       "destinationpath": "/root/helion-ceph/"
		    },
		    "flavor": {
		           "001": {
		               "ram": "163840",
		               "vcpus": "2",
		               "disk": "275",
		               "architecture": "x86_64",
		               "version" : "001"
		          },
		          "002": {
		               "ram": "163840",
		               "vcpus": "2",
		               "disk": "200",
		               "architecture": "x86_64",
		               "version" : "001"
		           }
		    },
		….
6.Modify `baremetal.csv` in `/helion-ceph/node-provisioner/client/` to include the details of the node.


	**Example**

		f0:92:1c:05:db:48,helioncsel,m0ng00s3,10.1.67.134,2,163840,200,overcloud-ceph-rados,mon2,002
		<mac address>, <ipmiusername>, <impipassword>, <ipaddress>, <cpu>, <memory>, <disksize>, <imagename>, <node name>, <flavor ID>
		f0:92:1c:05:57:30,helioncsel,m0ng00s3,10.1.67.123,2,163840,200,overcloud-ceph-cluster,c1mon1,002
		f0:92:1c:05:bb:b0,helioncsel,m0ng00s3,10.1.67.124,2,163840,200,overcloud-ceph-cluster,c1mon2,002
		f0:92:1c:05:4c:10,helioncsel,m0ng00s3,10.1.67.125,2,163840,200,overcloud-ceph-cluster,c1mon3,002
		f0:92:1c:05:6c:78,helioncsel,m0ng00s3,10.1.67.126,2,163840,200,overcloud-ceph-cluster,c1gw1,002
		f0:92:1c:05:ac:30,helioncsel,m0ng00s3,10.1.67.127,2,163840,200,overcloud-ceph-cluster,c1gw2,002
		f0:92:1c:05:aa:d8,helioncsel,m0ng00s3,10.1.67.128,2,163840,200,overcloud-ceph-cluster,c1osd1,002
		9c:b6:54:97:44:10,helioncsel,m0ng00s3,10.1.67.129,2,163840,200,overcloud-ceph-cluster,c1osd2,002
		f0:92:1c:05:7c:58,helioncsel,m0ng00s3,10.1.67.130,2,163840,200,overcloud-ceph-cluster,c1osd3,002
		9c:b6:54:97:a3:e0,helioncsel,m0ng00s3,10.1.67.131,2,163840,200,overcloud-ceph-admin,c1admin,002

Note that the flavor definition <flavor ID> in `baremetal.csv` should match the flavor defined in `orchestration.json`. 
 
7.Open another seed terminal window  and launch client `python orchestration.py` that is in seed node on `/helion-ceph/node-provisioner/client/`

8.To monitor this process open a new terminal seed window and run – `tail -f -50 /client/orchestration.log`


###Verification of Provisioning 

Provisioning tool takes about 8-13 minutes to provision single baremetal node.

Check `nova list` to see if the instance is in spawning and then goes to active state. You should be able to ssh to the instance after 12 min once the script is run.

If the instance does not spawn, check for following:

* Flavor definition matches the actual machine hardware configuration
* Check for any errors in `orchestration.log`
* Verfy entries in `server.json` and `orchestration.json`
* Check for any errors in server terminal window
* Check for any errors in orchestration.log
* Check if all the images are loaded to glance
* Make sure that instance you are tying to provision does not exist in `ironic node-list` or `nova list`
* Check for any other nova related errors `/var/log/nova/nova-compute.log or /var.log/ironic/ dir`
* Once all your instances are active and up move on to the next step.

SSH to your newly launched Instance using `cephadmin` key that was generated from undercloud in previous steps.

For example:

	root@undercloud-undercloud-zyjo2vylo3tb:~# ssh -i cephadmin.pem hlinux@192.168.51.133

The programs included with the hLinux system are free software; the exact
license terms for each program are described in the individual files in
`/usr/share/doc/*/copyright`.
 
	Last login: Thu Feb 12 04:33:42 2015 from 192.168.51.23
	hlinux@admin-overcloud-ceph-admin:~$ 

###Provisioning New additional Ceph nodes

To add new nodes, create new `baremetal.csv` entries and follow the above provisioning steps.

**Note**:  Do not append to the existing entries in `baremetal.csv` file for the new nodes that will be provisioned else the existing nodes will be reprovisioned.


 ---
<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>