---
layout: default
title: "HP Helion OpenStack&#174; 1.1: Ceph Automated Installation"
permalink: /helion/openstack/1.1/commercial.ceph-automated-install/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Binamra S, Paul F

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->


# HP Helion OpenStack&#174; 1.1: Ceph Automated Installation

The installation workflow of HP Helion OpenStack&#174;1.1 is shown in the following diagram.

<img src="media/ceph-installation-flow.png"/)>

The next diagram shows the logical architecture of Ceph.

<img src="media/ceph-logical-architecture.png"/)>

This section covers the following topics:

1. [Tarball Files](#tarball-files)
2. [Running the Provisioning Tool](#running-provision-tool)
3. [Verifying your Instance](#verify-instance)
4. [Provisioning Additional Ceph Nodes](#provisioning-additional-ceph-nodes)


##Tarball Files {#tarball-files}

The Ceph-automation tools are bundled into two tarball files:

* `cephsetup.tar`
* `cephconfiguration.tar`

You get these files from [https://helion.hpwsportal.com](https://helion.hpwsportal.com)

###The cephsetup.tar file

You must downloaded and copy the `cephsetup.tar` file to the Helion seed VM. The cephsetup.tar file is bundled with the following packages:

* `images`
* `helion-patch`
* `node-provisioner`

#### Images package ###

Ceph-automation provides two types of images:

* `ceph-admin-image` includes all the packages required to run configuration scripts. This image are used on Ceph admin node.
* `ceph-cluster-image` includes all the packages for setting up `ceph-monitor`, `ceph-osd` and `overcloud-ceph-rados` Gateway. 

	Except for the Ceph admin node, the `ceph-cluster-image` is shared across all the nodes participating in cluster.

#### helion-patch package ###

The `helion-patch` includes:

* `hp_ced_check_deployed_images`
* `hp_ced_pre_update_checks.sh`
* `register-cephnodes.sh` 

#### node-provisioner package ###

To deploy these images using the provisioning tool that runs from the seed VM:

1. Log into the Helion seed VM.

		ssh root @ <seed IP address>


2. Create a directory on any location 

		mkdir /helion-ceph

3. Download the tar file from [https://helion.hpwsportal.com](https://helion.hpwsportal.com) 
4. Untar and copy the downloaded file to the `/helion-ceph` folder on the seed VM. Untarring this file results in the follow directory structure:

	* `images`
	* `node-provisioner`
	* `cephsetup`

1. To view the contents of the `/helion-ceph/images` directory from the seed VM, enter (for example): 


		root@hLinux:~/helion-ceph/images# ls
		bm-deploy-kernel
		bm-deploy-ramdisk
		overcloud-ceph-admin-initrd
		overcloud-ceph-admin-vmlinuz
		overcloud-ceph-admin.qcow2
		overcloud-ceph-cluster-initrd
		overcloud-ceph-cluster-vmlinuz
		overcloud-ceph-cluster.qcow2

2. To view the contents in the `/helion-ceph/node-provisioner`directory from the seed VM, enter (for example):


		/helion-ceph/node-provisioner# ls
		server
		client	

		/helion-ceph/node-provisioner/server# ls
		app.wsgi
		bottle.py
		httpd.conf
		server.json
		 server.py

		/helion-ceph/node-provisioner/client# ls
		baremetal.csv
		common.py
		orchestration.json
		orchestration.py

3.  Modify `/server/server.json` to include OpenStack credentials from the Undercloud (`stackrc`), network, and keypair that you want to use. The following is a sample of `server.json`:


 

		root@hLinux:~/helion-ceph/node-provisioner/server# cat server.json
		{
	    "authentication": {
	       "HOST": "127.0.0.1",
	       "PORT": "8085",
	       "OS_VERSION": "2",
	       "OS_USER": "admin",
	       "OS_PASSWORD": "de44ff17c309c7b2a74465104cd59728b121dd60",
	       "OS_TENANT_NAME": "admin",
	       "OS_AUTH_URL": "http://<undercloud IP>:5000/v2.0",
	       "keypair": "cephadmin",
	       "netid": "823ad730-6c69-42e6-b455-8a8173ceae81"
	    	}
		}

where:

**HOST** is the local machine IP which will be your seed IP.

**PORT** is 8085 which can be used as a service port.

**OS_VERSION** is the OpenStack API version. Should be set to "2".

**OS_USER** is supplied from the Undercloud `stackrc` file. For example: **admin"**.

**OS_PASSWORD** -is supplied from the Undercloud `stackrc` file. For example: "de44ff17c309c7b2a74465104cd59728b121dd60".

**OS	&#95;TENANT_NAME"** is supplied from the Undercloud `stackrc` file.

**OS	&#95;AUTH_URL"** is the Undercloud IP. For example: "http://192.168.51.23:5000/v2.0".

**keypair** for "cephadmin". You generate this from the Undercloud node using **"nova keypair-add cephadmin > cephadmin.pem"**

**Note**: To enable login to other nodes, provide 600 permission to the `cephadmin.pem` key. Execute `chmod 600 cephadmin.pem` command to update the permission. 

**netid**: Execute `nova network-list` from the Undercloud to get the `netid`. For example: "823ad730-6c69-42e6-b455-8a8173ceae81"


You can verify that the `pem` file is present in Undercloud node by:

1. Logging to the Undercloud.

		# ssh heat-admin@<undercloud IP address>

2. Then enter.

 		ls -ls

 	For example:

		root@undercloud-undercloud-zyjo2vylo3tb:~# ls -ls
		total 12
		4 -rw------- 1 root       root       1680 Feb 10 22:17 cephadmin.pem
		4 -rw------- 1 heat-admin heat-admin  347 Feb 11 21:12 overcloud.stackrc
		4 -rw-r--r-- 1 root       root        311 Feb 10 20:34 stackrc


##Running the Provisioning Tool {#running-provision-tool}
To start the provisioning tool:

1.	Open a new terminal.
2.	Log into the seed VM and launch a server from the `/helion-ceph/node-provisioner/server/' directory  using the `./server.py' script.

	When you start the server, the following information is displayed. 

		root@hLinux:~/helion-ceph/node-provisioner/server# ./server.py
		Bottle v0.13-dev server starting up (using WSGIRefServer())...
		Listening on http://192.168.51.22:8085/
		Hit Ctrl-C to quit.

3. Open another seed terminal window and modify  the `orchestration.json`  file in `/helion-ceph/node-provisioner/client/orchestration.json`

	Your `baremetal.csv` inputs should match the flavor list. For example you need to specify the right RAM, vCPUs, disk size in "GB" and version number.

* Update the  `"ws_url"` with the URL of the seed node VM where  the server is running by entering:

		"ws_url": "http://192.168.51.22:8085/"

* Modify `imagepath` to include the path to images (deploy images + qcow + ramdisk + kernel) by entering:

		"imagepath": "/root/helion-ceph/images/"

      For example, make changes to the following based on your setup.


		{
		"authentication": {
		       "ws_url": "http://192.168.51.22:8085/"
		    },
		    "api":{
		       "imagepath": "/root/helion-ceph/images/",
		       "deploy-image-prefix": "bm-deploy"
		    },
		    "orchestration":{
		       "hypervisorsleepduration": "300",
		       "hypervisorsmoniteringfrequency": "10",
		       "bootsleepduration": "1200",
		       "bootinitialwaitduration": "30",
		       "hypervisortype": "baremetal",
		      "hypervisordriver": "ironic",
		       "bootmoniteringfrequency": "5",
		       "destinationpath": "/root/helion-ceph/"
		    },
		    "flavor": {
		           "001": {
		               "ram": "163840",
		               "vcpus": "2",
		               "disk": "275",
		               "architecture": "x86_64",
		               "version" : "001"
		          },
		          "002": {
		               "ram": "163840",
		               "vcpus": "2",
		               "disk": "200",
		               "architecture": "x86_64",
		               "version" : "001"
		           }
		    }
6.Modify the `baremetal.csv` file in `/helion-ceph/node-provisioner/client/` to include the details of the node. The following is an example:

		f0:92:1c:05:57:30,helioncsel,m0ng00s3,10.1.67.123,2,163840,200,overcloud-ceph-cluster,c1mon1,002
		f0:92:1c:05:bb:b0,helioncsel,m0ng00s3,10.1.67.124,2,163840,200,overcloud-ceph-cluster,c1mon2,002
		f0:92:1c:05:4c:10,helioncsel,m0ng00s3,10.1.67.125,2,163840,200,overcloud-ceph-cluster,c1mon3,002
		f0:92:1c:05:6c:78,helioncsel,m0ng00s3,10.1.67.126,2,163840,200,overcloud-ceph-cluster,c1gw1,002
		f0:92:1c:05:ac:30,helioncsel,m0ng00s3,10.1.67.127,2,163840,200,overcloud-ceph-cluster,c1gw2,002
		f0:92:1c:05:aa:d8,helioncsel,m0ng00s3,10.1.67.128,2,163840,200,overcloud-ceph-cluster,c1osd1,002
		9c:b6:54:97:44:10,helioncsel,m0ng00s3,10.1.67.129,2,163840,200,overcloud-ceph-cluster,c1osd2,002
		f0:92:1c:05:7c:58,helioncsel,m0ng00s3,10.1.67.130,2,163840,200,overcloud-ceph-cluster,c1osd3,002
		9c:b6:54:97:a3:e0,helioncsel,m0ng00s3,10.1.67.131,2,163840,200,overcloud-ceph-admin,c1admin,002

	Note that the flavor definition (`flavor ID`) in  the `baremetal.csv` file should match the flavor defined in `orchestration.json`. 
 
7.Open another seed terminal window  and launch the client `python orchestration.py` that is on the seed node VM in `/helion-ceph/node-provisioner/client/`

8.To monitor this process, open a new terminal seed window and run

	`tail -f -50 /client/orchestration.log`


## Verifying your Instance {#verify-instance}

The provisioning tool takes about 10-15 minutes to provision a single baremetal node.

Check the `nova list` to see if the instance goes from spawning  to active state. You should be able to SSH to the instance after 12 minutes once you run the script.

If the instance does not spawn, check:

* The flavor definition matches the actual machine hardware configuration
* Check for any errors in `orchestration.log`
* Verify entries in `server.json` and `orchestration.json`
* Check for any errors in the server terminal window
* Check that all the images are loaded to Glance
* Make sure that the instance you are tying to provision does not exist in `ironic node-list` or `nova list`
* Check for any other Nova-related errors in `/var/log/nova/nova-compute.log or /var.log/ironic/ dir`

Once all your instances are active and up, move on to the next step.

SSH to your newly launched instance using the `cephadmin` key that was generated from the Undercloud in the previous steps.

For example:

	root@undercloud-undercloud-zyjo2vylo3tb:~# ssh -i cephadmin.pem hlinux@192.168.51.133

The programs included with the hLinux system are free software. For information on specific
licensing terms for each program, see the individual files in
`/usr/share/doc/*/copyright`.

When you SSH into your instance, you should see (for example):
 
	Last login: Thu Feb 12 04:33:42 2015 from 192.168.51.23
	hlinux@admin-overcloud-ceph-admin:~$ 

## Provisioning Additional Ceph Nodes {#provisioning-additional-ceph-nodes}

To add new nodes, create new `baremetal.csv` entries and follow the above provisioning steps.

**Note**:  Do not append new entries to existing entries in  the `baremetal.csv` file or else the existing nodes will be reprovisioned.

## Next Steps

[Configuring Ceph Clusters and Client Nodes using Ansible Playbooks]( /helion/openstack/1.1/ceph-hp-helion-openstack-cluster-client-node-configuration-ansible/)
 ---
<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>
