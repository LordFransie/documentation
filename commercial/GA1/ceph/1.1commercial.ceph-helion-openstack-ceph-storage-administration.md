---
layout: default
title: "HP Helion OpenStack&#174; HP Helion Ceph Administration Services"
permalink: /helion/openstack/1.1/ceph-helion-openstack-ceph-administration-services/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F, Binamra S

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<p style="font-size: small;"> <a href=" /helion/openstack/1.1/ceph-monitoring/">&#9664; PREV</a> | <a href="/helion/openstack/siteindex/">&#9650; UP</a> | <a href=" /helion/openstack/1.1/ceph-rados-gateway-dmz-ha-proxy/">NEXT &#9654;</a> </p>

# HP Helion OpenStack&#174; 1.1 Ceph Administration Services

The following sections include useful commands and approaches to mange and administrator a growing Ceph cluster.

###Block Device Commands

The `rbd` command enables you to create, list, inspect and remove block device images. You can also use it to clone images, create snapshots, rollback an image to a snapshot, view a snapshot, etc.

To use Ceph Block Device commands, you must have access to a running Ceph cluster.

####Creating a block device name

Before adding a block device to a node, you must first create an image for it in the Ceph Storage Cluster. To create a block device image, enter:

	rbd create {image-name} --size {megabytes} --pool {pool-name}

For example, to create a 1GB image named image1 that stores information in a pool named imagelist, enter:

	rbd create image1 --size 1024
	rbd create bar --size 1024 --pool imagelist

**Note**: You must first create a pool before you can specify it as a source.

####Listing block device images

* To list block devices in the `rbd` pool, (where rbd is the default pool name), enter:

	rbd ls

* To list block devices in a particular pool, execute the following command:

	rbd ls {poolname}

For example:

	rbd ls imagelist

####Retrieving image information

* To retrieve information from a particular image, enter:

	rbd --image {image-name} info

For example:

	rbd --image image1 info

* To retrieve information from an image within a pool, enter:

	rbd --image {image-name} -p {pool-name} info

For example:

	rbd --image lin -p imagelist info

####Resizing a block device image

Ceph block device images are thin provisioned. They do not actually use any physical storage until you begin saving data to them. However, they do have a maximum capacity that you set with the `--size` option. If you want to increase (or decrease) the maximum size of a Ceph block device image, enter:

	rbd resize --image image1 --size 2048


####Removing a block device image 

To remove a block device, enter:

	rbd rm {image-name}

For example:

	rbd rm image1

To remove a block device from a pool, enter:

	rbd rm {image-name} -p {pool-name}

For example:

	rbd rm lin -p imagelist

####Kernel module operations

**Important** - To use kernel module operations, you must have a running Ceph cluster.

######Get a list of images

To mount a block device image, return a list of the images.

	rbd list

#####Map a block device

Use `rbd` to map an image name to a kernel module. You must specify the image name, the pool name, and the user name. If it is not already loaded, `rbd` loads the RBD kernel module.

	sudo rbd map {image-name} --pool {pool-name} --id {user-name}

For example:

	sudo rbd map --pool rbd myimage --id admin

If you use cephx authentication, you must also specify a secret. It may come from a keyring or a file containing the secret.

	sudo rbd map --pool rbd myimage --id admin --keyring /path/to/keyring
	sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file


#####Show Mapped block devices

To show block device images mapped to kernel modules with the `rbd` command, use the showmapped option.

	rbd showmapped
	UNMAPPING A BLOCK DEVICE

To unmap a block device image with the `rbd` command, use the unmap option and the device name. (By convention, the device name is the same as the block device image name).

	sudo rbd unmap /dev/rbd/{poolname}/{imagename}

For example:

	sudo rbd unmap /dev/rbd/rbd/foo

###Control Commands

#####Monitor commands 

Use the ceph utility to issue the monitor commands:

	ceph [-m monhost] {command}

The command is usually (though not always) of the form:

	ceph {subsystem} {command}

####System Commands

* To display the current status of the cluster, enter:

		ceph -s
		ceph status

* To display a running summary of the status of the cluster, and major events, enter:

		ceph -w

* To show the monitor quorum, including which monitors are participating and which one is the leader, enter:

		ceph quorum_status

* To query the status of a single monitor, including whether or not it is in the quorum, enter:

		ceph [-m monhost] mon_status

####Authentication subsystem

* To add a keyring for an OSD, enter:

		ceph auth add {osd} {--in-file|-i} {path-to-osd-keyring}

* To list the cluster's keys and their capabilities, enter:

		ceph auth list

####Placement group subsystem

* To display the statistics for all placement groups, enter:

		ceph pg dump [--format {format}]

The valid formats are plain (default) and JSON. 

* To display the statistics for all placement groups stuck in a specified state, enter:

		ceph pg dump_stuck inactive|unclean|stale [--format {format}] [-t|--threshold {seconds}]

	`--format` may be plain (default) or JSON

	`--threshold` defines how many seconds “stuck”. The default value is 300.

* Inactive placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back.

* Unclean placement groups contain objects that are not replicated the desired number of times. They should be  in the state of recover.

* Stale Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by `mon_osd_report_timeout`).

* To revert "lost" objects to their prior state, either a previous version or delete them if they were just created, enter:

		ceph pg {pgid} mark_unfound_lost revert


####OSD subsystem

* To query the OSD subsystem status, enter:

		ceph osd stat

* To write a copy of the most recent OSD map to a file, enter:

		ceph osd getmap -o file

* To write a copy of the crush map from the most recent OSD map to file, enter:

		ceph osd getcrushmap -o file

Which is functionally equivalent to:

		ceph osd getmap -o /tmp/osdmap
		osdmaptool /tmp/osdmap --export-crush file 

* To dump the OSD map, enter:

		ceph osd dump [--format {format}]

Valid formats for `--format` are plain and JSON. If you do not specify a format, the OSD map is dumped as plain text.

* To dump the OSD map as a tree with one line per OSD containing weight and state, enter:

		ceph osd tree [--format {format}]

* To find out where a specific object is or would be stored in the system, enter:

		ceph osd map <pool-name> <object-name>

* To add or move a new item (OSD) with the given `id/name/weight` at the specified location, enter:

		ceph osd crush set {id} {weight} [{loc1} [{loc2} ...]]

* To remove an existing item from the CRUSH map, enter:

		ceph osd crush remove {id} 

* To move an existing bucket from one position in the hierarchy to another, enter:

		ceph osd crush move {id} {loc1} [{loc2} ...] 

* To set the weight of the item given by {name} to {weight}, enter:

		ceph osd crush reweight {name} {weight}

* To create a cluster snapshot, enter:

		ceph osd cluster_snap {name}

* To mark an OSD as lost, enter:

		ceph osd lost {id} [--yes-i-really-mean-it]

**Caution**: This may result in a permanent data loss. 

* To create a new OSD, enter: 

		ceph osd create [{uuid}]

* If you do not provide a UUID, it will be set automatically when the OSD starts up.

* To remove the given OSD(s), enter:

		ceph osd rm [{id}...]

* To query the current `max_osd` parameter in the OSD map, enter:

		ceph osd getmaxosd

* To import the given OSD map, enter:

		ceph osd setmap -i file

 **Caution**: Since the OSD map includes the dynamic state about which OSDs are current on or offline, only use this command if you have just modified a (very) recent copy of the map.

* To import the given crush map, enter:

		ceph osd setcrushmap -i file

* To set the `max_osd` parameter in the OSD map, enter: 

		ceph osd setmaxosd

* To mark OSD `{osd-num}` down

		 osd down {osd-num} 

**Note**: This is necessary when expanding the storage cluster.

* To mark the OSD `{osd-num}` out of the distribution (that is, allocated no data), enter:

		ceph osd out {osd-num}

* To mark `{osd-num}` in the distribution (that is, allocated data), enter:

		ceph osd in {osd-num}

* To list classes that are loaded in the Ceph cluster, enter:

		ceph class list

* To set or clear the pause flags in the OSD map, enter: 

		ceph osd pause
		ceph osd unpause

**Note**: If set, no IO requests are sent to OSDs. Clearing the flags through `unpause` results in resending pending requests.

* To set the weight of `{osd-num}` to `{weight}`, enter: 

		ceph osd reweight {osd-num} {weight}

**Note**: Two OSDs with the same weight receive a similar number of I/O requests and store a similar amount of data.

* To reduce the weight of OSDs which are heavily overused, enter:

		ceph osd reweight-by-utilization [threshold]

Note: By default, this command will adjust the weights downward on OSDs which have 120% of the average utilization, but if you include threshold it will use that percentage instead.

* To add/remove an address to/from the blacklist, enter:

	ceph osd blacklist add ADDRESS[:source_port] [TIME]
	ceph osd blacklist rm ADDRESS[:source_port]
	
**Note**: When adding an address, you can specify the time (in seconds) to be blacklisted; otherwise, it will configure a default time of 1 hour. A blacklisted address is prevented from connecting to any OSD. Blacklisting is most often used to prevent a lagging metadata server from making unwanted changes to data on the OSDs.

These commands are useful for failure testing, as blacklists are normally maintained automatically, and do not need manual intervention.

* To create/delete a snapshot of a pool, execute the following command:
	
		ceph osd pool mksnap {pool-name} {snap-name}
	
* To delete a snapshot of a pool, enter:

		ceph osd pool rmsnap {pool-name} {snap-name}
	
* To create a storage pool, enter:
	
		ceph osd pool create {pool-name} pg_num [pgp_num]
	
* To delete a storage pool, enter:
	
		ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]

* To rename a storage pool, enter:
	
		ceph osd pool rename {old-name} {new-name}

* To change a pool setting, enter:
	
		ceph osd pool set {pool-name} {field} {value}
	
Where:

* **size**: Sets the number of copies of data in the pool.

* **crash&#95;replay&#95;interval**: The number of seconds to allow clients to replay acknowledged but 
uncommited requests.

* **pg_num**: The placement group number.

* **pgp_num**: Effective number when calculating pg placement.

* **crush_ruleset**: rule number for mapping placement.


To get the value of a pool setting, enter:

	ceph osd pool get {pool-name} {field}

Where:

* **pg_num**: The placement group number.

* **pgp_num**: Effective number of placement groups when calculating placement.

* **lpg_num**: The number of local placement groups.

* **lpgp_num**: The number used for placing the local placement groups.


To send a scrub command to OSD `{osd-num}`, execute the following command:

		ceph osd scrub {osd-num}


**Note**: To send the command to all OSDs, use <b>*</b>.

To send a repair command to `OSD.N`, enter:

		ceph osd repair N

**Note**: To send the command to all OSDs, use *.

To run a simple throughput benchmark against `OSD.N`, enter: 

		ceph osd tell N bench [BYTES_PER_WRITE] [TOTAL_BYTES]

**Note**: This writes `TOTAL_BYTES` in write requests of `BYTES_PER_WRITE` each. By default, the test writes 1 GB in total in 4-MB increments. 

####MON Subsystem

The following commands show the status of the monitor.

* To show monitor statistics, execute the following command:

		ceph mon stat

* To list the monitor nodes that are part of the current quorum, execute the following command:

		$ ./ceph quorum_status

* To get a status of just the monitor you connect to (use -m HOST:PORT to select), execute the following command:

		ceph mon_status

* To get a dump of the monitor state, execute the following command:

		ceph mon dump

## Next Steps

[Ceph RADOSGW DMZ HAProxy]( /helion/openstack/1.1/ceph-rados-gateway-dmz-ha-proxy/)

<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>


