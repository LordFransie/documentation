layout: default
title: "HP Helion OpenStack&#174; HP Helion Ceph Gateway Pools, Users and Sub-users, Access and Secret Keys"
permalink: /helion/openstack/1.1/ceph-rados-gateway-pools/
product: commercial
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F

---
<!--PUBLISHED-->


<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p> -->


# HP Helion OpenStack&#174; 1.1 Gateway Pools, Users and Sub-users, Access and Secret Keys #

This topic explains how Ceph Object Gateways create pools to store data and how users access these pools.

## Pools ##

Ceph Object Gateways require Ceph Storage Cluster pools to store specific gateway data.  The gateway automatically creates pools. Created users will need permissions to access these pools. 

To list available pools, enter:

	rados lspools

Verify that the .`rgw.buckets` and `.rgw.buckets.index` pools are created by default. If not, create these pools using `ceph osd pool create` command. For more details, refer to [http://https://ceph.com/docs/master/radosgw/config-ref/#pools](http://https://ceph.com/docs/master/radosgw/config-ref/#pools)

<!--the image has been removed from the recent cookbook
<img src="media/helion-ceph-rados-lspools.png"/)>
-->

##User and Sub-User

The User is a user of the S3 interface, and Sub-user is a user of Swift interface. The sub-user is always associated to a user. For details, refer to https://ceph.com/docs/master/radosgw/admin/. 

* To create a User and a Sub-user, enter:

	radosgw-admin user create --subuser=s3User:swiftUser --display-name="First User " --key-type=swift --access=full


<!--the image has been removed from the recent cookbook
	<img src="media/helion-ceph-create-user-subuser.png"/)>
-->

* Ensure that the user (**s3User**) and subuser (**s3User:swiftUser**) are stored in respective `.users.uid` and `.users.swift` pools.


<!--the image has been removed from the recent cookbook
	<img src="media/helion-ceph-user-uid-user-swift.png"/)>
-->

##Access and Secret Keys

S3 users and Swift users must have access and secret keys to enable end users and to interact with a gateway instance. S3 user access and its secret key are created by entering:

	radosgw-admin key create --uid=s3User --key-type=s3 --gen-access-key --gen-secret


<!--the image has been removed from the recent cookbook

	<img src="media/helion-ceph-generate-secrete-key.png"/)>
-->

The key generated must be free of JSON escape (\) characters.

If the user or application writes more than 1k containers, then modify the `max_buckets` variable. Also, right-sizing of Placement Groups per Pool is required. Ensure `max_buckets` is set to unlimited size by setting the value to 0. <!--It is important in order to write unlimited containers in `.rgw.buckets` default pool during workload testing-->. To do this, enter:

	radosgw-admin user modify --uid=s3User --max-buckets=0


<!--
###Configuring the S3 Client

The S3 client is not supported by HP for user data, other than as a validation step during installation and configuration. For the gateway instance, S3 users created can be verified using  the s3cmd tool on the gateway node or Ceph client. For more details on the s3cmd tool, refer to http://s3tools.org/s3cmd.

* To install the s3cmd tool, enter:

		sudo apt-get install s3cmd

* Configure the s3cmd tool as shown below. Enter relevant information like access key, secret key, etc when prompted. This information is stored in the `.s3cfg` file:

		s3cmd --configure

* Access and secret key generated for the S3 user can be collected as shown below:

		radosgw-admin user info -uid=s3User

* List buckets using `radosgw-admin` command or `s3cmd` or by listing the .rgw.buckets pool as shown below. Initially, the list is empty.

* Create the bucket by entering:

		s3cmd mb s3://my-Bucket

* Upload the image to the bucket

		s3cmd put <image> <bucket>

* List bucket contents (and verify that it was created) by entering:
		s3cmd ls <bucket>

* Download the uploaded image by entering:

		s3cmd get <bucket> <image>

* Run checksum to validate that the downloaded image is not corrupt by entering:

		md5sum <image uploaded> <image downloaded>

<!--
###Configuring the Swift Client

For the gateway instance, swift users can be verified using the Swift client on the gateway node or the Ceph client. For more details on the Swift client, refer to https://www.swiftstack.com/docs/integration/python-swiftclient.html


* Create `creds.py` with following file contents by entering:

		#Auth URL pointing to gateway node:

		export ST_AUTH=http://gateway.ex.com/auth/v1.0

		#Swift user:

		export ST_USER=s3User:swiftUser

		#Swift user - secret key:

		export ST_KEY= Pp3YqVoyqOpFF28kby03e55j3akd0wEE3NYGjXsK

* Source the Swift credentials as shown below:

		source creds.py

* List the Swift container (aka S3 bucket) as shown below:

		swift list

* Display the Swift container information by entering:

		swift stat &#60;container>

* Upload the image into the Swift container by entering:

		swift upload &#60;container> &#60;image to upload>

* Verify the upload by entering:

		swift stat &#60;container>

* Verify that the uploaded image is residing in the rgw pool by entering:

		rados -p .rgw.buckets ls
-->

<!--
		
**Verifying the Ceph RADOSGW Client**

Assuming that the Ceph client packages are already installed, perform following steps on the client node to verify the gateway instance, user and sub-user created in the previous section.

* Edit the `/etc/hosts` file to include the gateway node by entering:

		192.x.x.x gateway.ex.com gateway

* Edit `/etc/environment` to include the gateway node entries and source the same. For example, enter:

		export no_proxy=localhost,127.0.0.1,192.x.x.x,gateway.ex.com,gateway

* Copy the `ceph.client.radosgw.keyring` and `ceph.conf` file from the gateway node to the `/etc/ceph` directory by entering:
    
		scp ceph-admin@192.x.x.x:/etc/ceph/ceph.client.radosgw.keyring .

		scp ceph-admin@192.x.x.x:/etc/ceph/ceph.conf .

* Verify that the Ceph Health is OK.

* Exercise S3 or Swift API calls as described in previous sections.
-->


<!--Binamra - we already have this as a seperate file-->
<!--
###Integrating the RADOS Gateway - Keystone Authentication

Integration of the RADOS Gateway with the Helion OpenStack Identity service (Keystone) sets up the Gateway to authorize and accept Keystone users automatically. Users are created in RADOS pools provided they have valid Keystone tokens. For details refer to [RADOS pools and Keystone tokens](http://ceph.com/docs/master/radosgw/keystone).

**Assumptions**

* The gateway node has Apache2/FastCGI without `100 continue support`.

* Two RADOS gateway nodes with two distinct users sharing same RADOSGW keyring.

**Integration Steps**

To achieve this integration, perform the following steps.

#### Admin Node ####

* On the Ceph admin node, edit the `ceph.conf` file to include the following:

		rgw keystone url = {keystone server url:keystone server admin port}
		
		rgw keystone admin token = [keystone admin token - Available in /etc/keystone/keystone.conf]
		
		rgw keystone accepted roles = {accepted user roles}
		
		rgw keystone token cache size = {number of tokens to cache}
		
		rgw keystone revocation interval = {number of seconds before checking revoked tickets}
		
		rgw s3 auth use keystone = true
		
		rgw nss db path = {path to nss db}

	For example - rgw keystone url = http://192.0.2.21:5000

		rgw keystone admin token = aa4edaa3aa219a8b8e78f937083c61d68728b654
		rgw keystone accepted roles = Member, admin, swiftoperator
		rgw keystone token cache size = 500
		rgw keystone revocation interval = 500
		rgw s3 auth use keystone = true
		rgw nss db path = /var/ceph/nss

* Re-deploy the `ceph.conf` file on all Ceph cluster nodes and the Helion Controller nodes.

#### Helion Controller Nodes ####

* On any controller node, register the RADOS gateway endpoint with Keystone. The IP address is the Helion VIP address and  the RADOS port is 10080:

	* `keystone service-create --name swift --type object-store`. Note the Swift service ID.

	* `keystone endpoint-create --service-id e001b96dc4b54a6c9f672418c21eb132` `--publicurl https://192.0.2.x:10080/swift/v1 --internalurl https://192.0.2.x:10080/swift/v1 --adminurl https://192.0.2.x:10080/swift/v1`

* To return the pointer to the Ceph store and not default to the Swift store enter:
    
    	keystone endpoint-get --service object-store
    
* Download and backup any images from Glance
* Delete the existing Swift endpoint if the above command returns a Swift store.
* Configure the Ceph store as explained in this section.
* Re-upload the Glance images and use the Ceph store for Glance images going forward.
* On all controller nodes, edit `/etc/haproxy/haproxy.cfg` as shown below. (The IP address used for bind is the Helion VIP address.)    
    
    	listen radosgw
     	bind 192.0.2.x:10080
    	balance roundrobin
    	server gateway 192.x.x.x:443 check inter 2000 rise 2 fall 5 maxconn 1500
    	server gateway1 192.x.x.x:443 check inter 2000 rise 2 fall 5 maxconn 1500
  
* Restart the HAProxy service on all controller nodes as shown below:    
    
    	/etc/init.d/haproxy restart
    
* On all controller nodes, edit `/etc/stunnel4/from-heat.conf` as shown below. (The IP address used is physical the IP address of the respective controller node.):    
    
    	[radosgw]
    	accept = 192.0.2.x:10080
    	connect = 127.0.0.1:10080
    	ciphers=ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
    
* Restart `stunnel4service` on all controller nodes as shown below:    
    
    	/etc/init.d/stunnel4 restart

* Open port 10080 on all controller nodes as shown below:    
    
    	add-rule INPUT -p tcp --dport 10080 -j ACCEPT
		iptables-save|grep 10080
    
-->

<!--

* On the Management node, delete the existing Swift endpoint and service if Swift store is no longer required. For example:

	* `keystone service-list` will list all services. Note down swift service ID

	* `keystone endpoint-list` will all endpoints. Note down swift endpoint ID

	* `keystone endpoint-delete` *`<swift endpoint ID>`*

	* `keystone service-delete` *`<swift service ID>`*

* On the Management node, configure Keystone to point to the RADOS gateway endpoint as shown below. Assumption here is that the URL for the gateway node is http://gateway.ex.com.

	* `keystone service-create` --name swift --type object-store [Note down service ID]

	* `keystone endpoint-create` --service-id <service ID from above> --public url http://gateway.ex.com/swift/v1 --internal url http://gateway.ex.com/swift/v1 --admin url http://gateway.ex.com/swift/v1

* On any controller node (for example, node 0), convert the OpenSSL certificates that Keystone uses to a NSS database format. To do this, ensure that the `certutil` package is available on all controller nodes. To do this, enter:

	* `apt-get install libnss3-tools`

	* `mkdir /var/ceph/nss`
	
	* `openssl x509 -in /mnt/state/etc/keystone/ssl/certs/ca.pem -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"`

	* `openssl x509 -in /mnt/state/etc/keystone/ssl/certs/signing_cert.pem -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"`

	* Create the `/var/ceph/nss` directory on the gateway node and copy converted certificates generated above in this path.
-->

<!--

* On all controller nodes, edit `/etc/apache2/sites-enabled/keystone_modwsgi.conf` to include `WSGIChunkedRequest` as shown below. For details, refer to [Including WSGIChunkedRequests](http://tracker.ceph.com/issues/7796).

		<VirtualHost *:35357>
			......
		<Directory /etc/keystone>
		......
		WSGIChunkedRequest On
		......
		</Directory>
		......
		</VirtualHost>

		<VirtualHost *:5000>
		......
		<Directory /etc/keystone>
		......
		WSGIChunkedRequest On
		......
		</Directory>
		......
		</VirtualHost>

* Restart the Apache2 service on all controller nodes as shown below:

		service apache2 restart

### Ceph RADOS Gateway Nodes ####

* Create the following directory on gateway node(s) by entering:    
    
    	mkdir /var/ceph/nss
    
* As root, convert OpenSSL certificates that Keystone uses to the NSS DB format. To do this, make sure the certutil package is available on the gateway nodes. 

	* Copy `ca.pem` and `signing_cert.pem` in `/mnt/state/etc/keystone/ssl/certs/` on any controller node to gateway node in `/tmp` directory. Enter:
    
    		openssl x509 -in /tmp/ca.pem -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
    		openssl x509 -in /tmp/signing_cert.pem -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"
 
* Copy following the certificates from your Helion setup to `/etc/ssl/certs`:
	- `eca` keys from the seed node 
	- `ca-certificates.crt` from the controller node
	- `from-heat` keys from Swift or the controller nodes

* On the RADOS  gateway node(s), edit `/etc/apache2/sites-available/rgw.conf` file as shown below:

    	SSLEngine on
    	SSLCertificateFile /etc/ssl/from-heat.crt
    	SSLCertificateKeyFile /etc/ssl/from-heat.key
    	SSLCACertificateFile /etc/ssl/certs/eca.crt
    	SetEnv SERVER_PORT_SECURE 443

*  Restart the Apache2 service as shown below:

		/etc/init.d/apache2 restart

* Restart the RADOSGW service as shown below:

		/etc/init.d/radosgw restart


**Validating the Configuration**

* Make sure the proxy is not set on gateway node.

* Make sure that the `radosgw` daemon is running as root and that there are no obvious errors in  the logs.

* From the Management node, make Swift v1 requests as shown below. (The asssumption here is that `s3User` is already created using the `radosgw-admin` command and that the correct credentials for `s3User` are used in making the request. Output should list the container if it is present.)

		swift -V 1.0 -A http://gateway.ex.com/auth/v1.0 -U s3User:swiftUser -K "abc" list

* From the Management node, make Swift v2 requests using Keystone. (The Ceph Object Gateway's user:subuser tuple maps to the tenant:user tuple expected by Swift. Here, admin credentials are considered. Output should list container if it is present.)

		swift -V 2.0 -A http://192.0.2.21:5000/v2.0 -U admin:admin -K "abc" list

* From the Management node, to get the admin tenant ID, enter:

		Keystone tenant-list
Output:

		+----------------------------------+---------+---------+
		| id 							   | name 	 | enabled |
		+----------------------------------+---------+---------+
		| 627770d0c17c4423b8c27a2607e60798 | admin   | True    |
		| aa70711bd69e4958ac239e2564c18054 | demo    | True    |
		| 250bf66045814455a5b3c3e6c7fb7c19 | service | True    |
		+----------------------------------+---------+---------+

* Verify if the admin user is created in the RADOS pool as shown below:
		
		rados --pool .users.uid ls
		s3User
		s3User.buckets
		627770d0c17c4423b8c27a2607e60798
		627770d0c17c4423b8c27a2607e60798.buckets

-->
## Next Steps

[Ceph RADOS Gateway]( /helion/openstack/1.1/ceph-rados-gateway/)



<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>
