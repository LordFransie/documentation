 ---
layout: default
title: "HP Helion OpenStack&#174; Ceph Integration"
permalink: /helion/openstack/1.1/services/ceph/overview-1.1/
product: commercial.ga
product-version1: HP Helion OpenStack
product-version2: HP Helion OpenStack 1.1
role1: Storage Engineer
role2: Storage Architect 
role3: Storage Administrator 
role4: Storage Engineer
role5: Service Developer 
role6: Cloud Administrator 
role7: Application Developer 
authors: Paul F

---
<!--PUBLISHED-->

<script>

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

</script>

<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/services/object/overview/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/services/overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/services/swift/deployment-scale-out/"> NEXT &#9654</a> </p>
-->

# HP Helion OpenStack&reg 1.1: Ceph Integration Overview#

The HP Helion OpenStack 1.1 Ceph Storage solution provides an unified scalable and stable storage solution for managing Helion OpenStack Glance Images, Nova Boot Volumes, Cinder persistent Volumes. This solution also supports user backup and archives workloads to the same unified Ceph storage platform.

Ceph can fulfill all the storage requirements in an OpenStack deployment; it is integrated with Glance, Cinder and Nova for block storage and can directly provide integrated, Swift-compatible, multi-tenant object storage. It also supports Keystone integration with the Ceph RADOS Gateway.

<img src="media/cephimplementation50.png">

## About Ceph ## {#about} 


Ceph is an Open Source, scalable, software-defined storage system running on HP Servers which provides block and object storage with unified management.

The main use case and integration point between HP Helion OpenStack Enterprise Edition and Ceph is block consumption of Ceph's RADOS Block Device,  described here as RBD. The secondary use case and integration point between the user application archive and backup workloads running externally or in Virtual Machines in HP Helion OpenStack, is object consumption of Ceph RADOS Gateway. The Ceph radosgw presents a REST interface with extensions offering compatibility with the Swift API. Therefore existing applications with integrations to the HP Helion OpenStack Swift API may port seamlessly from a OpenStack Swift backend storage platform and the Ceph solution.

Other supported HP storage options to consider are HP Helion OpenStack Swift for object storage, and HP VSA and HP 3PAR for block storage solutions.

### Block Storage

The Ceph Block Device is a network attached device that may be presented to the HP Helion OpenStack Controller and Compute Nodes for Glance, Nova and Cinder storage utilization or, in order to meet unique SLAs, to virtual machines in the KVM Helion OpenStack managed hypervisor.   Although Glance may be configured with either Ceph Object or Block Storage, HP has followed the best practices of the Ceph Community and is limiting support for Glance storage of Images to Block Storage.  This allows for the support of Ceph’s Clone and the Copy on Write Feature, here within described as COW.

### Object Storage
.
The Ceph RADOS Gateway is a REST API that supports the Swift API. HP supports object storage primarily for user workloads: ranging from COTS Applications running in Virtual Machines such as MySQL frequently needing to archive tar files to a reliable and resilient archive, to custom LOB solutions that require frequent and aggressive snapshots orchestrated in a consistency group across many VMs and external baremetal systems.

### File System ###
Ceph does provide a POSIX-compliant network file system that aims for high performance, large data storage, etc.  However, a file system interface is still not production ready as of the Helion OpenStack 1.1 release.  Hence HP is currently not supporting the File Interface for Production Customers.  As an alternative, HP suggests either leveraging Ceph’s RBD Block storage, Ceph’s RADOSGW Object Storage via the Swift API, or for User Archive and Backup type workloads, the Open Source Swift API compatible File System interfaces such as Duplicity.


### Ceph Cluster
The Ceph Object Storage Daemon, here within described as OSD, stores data, handles data replication, recovery, rebalancing, and provides information to Ceph monitors by checking other Ceph OSDs for a heartbeat.  It is best to maintain minimally three discrete Servers running OSDs to take into account the default three replicas.  The number of OSDs per server is part of the sizing exercise unique to each customer. For example, a stable configuration has been 9 OSD servers with 12 2TB disks per server – others can range to 9 OSD servers with 25 6TB disks per server (such as the HP SL4540).  

The Ceph Monitor maintains a master copy of the Ceph Cluster Map including: the OSD map, monitor map, placement group (PG) map, and the CRUSH map.  Ceph maintains a history called an “epoch” of each state change in the Ceph monitors, Ceph OSDs, and placement groups.  It is best to maintain in Production three Ceph Monitors (one Monitor may fail and the System will still be operational).  If additional monitors are required, upgrade in a manner such that the number of monitors results in odd numbers, such as to five Ceph monitors.  Server types range from HP XL230s to HP DL360s.


### Ceph Object Gateway

The Ceph object gateway is built on `librgw` to provide applications with a RESTful gateway to Ceph storage clusters. Ceph object storage supports two interfaces:


- Swift-compatible: Providing object storage functionality with a large subset of the OpenStack Swift API. HP Supports this interface and 3rd-party integrations with the Swift API.



- S3-compatible: Providiing object storage functionality with a large subset of the Amazon S3 RESTful API. This is not supported by HP as part of the solution, but it has passed minimal API testing.
The Ceph object gateway daemon (radosgw) is a fastCGI module for interacting with a Ceph storage cluster.  Ceph object gateway can store data in the same Ceph storage cluster used to store data from Ceph block device clients or from Ceph file system clients.  Ceph maintains its own User Authentication, and allows for extension to HP Helion OpenStack Keystone Authentication.  For User Archive and Backup workloads originating from HP Helion OpenStack Tenant Project Virtual Machines, integrating via Keystone may be preferable as a consistent authentication model.

The following is the Helion OpenStack Virtual Machine access to Ceph storage diagram

<img src="media/cephstorage.png">


## Hardware Recommendations for the Ceph Cluster {#hardware}

For a production environment, HP recommends the following hardware:

<table style="text-align: left; vertical-align: top; width:1000px;">
<tr style="background-color: #C8C8C8;">
<th>Process</th><th>Criteria</th><th>Minimum Recommended Server quantity</th></tr>
<tr><td>Ceph-OSD</td><td> 3 x 15 HP ProLiant SL4540 Gen8 Servers</td><td>5</td></tr>
<tr><td>Ceph-Mon</td><td>HP ProLiant DL360p Gen8 Servers</td><td>3</td></tr>
<tr><td>Ceph-Admin</td><td>HP ProLiant DL360p Gen8 Servers</td><td>1*</td></tr>
<tr><td>Ceph-RADOSGW</td><td>HP ProLiant DL360p Gen8 Servers</td><td>1*</td></tr>
<tr><td></td><td>10 GbE Networking running on HP 5900AF switches</td><td>1</td></tr>
<tr><td></td><td>1 GbE Networking running on an HP 2920 switch</td><td>
1</td></tr>
</table>

**Note:** If the Resiliency SLA for the user archive or backup use cases cannot tolerate downtime of the Ceph RADOS Gateway, then consider adding additional Servers with an external Load Balancer.

### Detailed Example Deployment {#deployment}

<table style="text-align: left; vertical-align: top; width:1000px;">
<tr style="background-color: #C8C8C8;">
<th>HP Helion Openstack</th><th>CEPH Monitor/GateWays</th><th>OSDs (6x15 SL4540)</th></tr>
<tr><td>12 ProLiant DL360p Gen8 with 12 cores each</td><td>4x HP ProLiant DL360p Gen8 with 12 cores</td><td>6 x HP 3xSL4540 Gen8 with 16 cores each</td></tr>
<tr><td>2 x E5-2667 (Intel Xeon 2.9GHz 15M Cache)</td><td>2 x E5-2667 (Intel Xeon 2.9GHz 15M Cache)</td><td>2 x E5-2470 (Intel Xeon 2.3GHz 20M Cache)</td></tr>
<tr><td>64 GB - 8 x HP 8GB 1Rx4 PC3-12800R</td><td>64 GB - 8 x HP 8GB 1Rx4 PC3-12800R</td><td>96 GB - 12 x HP 8GB 2Rx4 PC3L-10600R</td></tr>
<tr><td>3.6 TB - 4 x HP 900GB 6G SAS 10K 2.5in SC ENT HDD</td><td>1 TB - 2 x HP 500GB 6G SAS 10K 2.5in SC ENT HDD</td><td>45 TB - 15 x HP 3TB 6G SAS 7.2K 3.5in SC MDL HDD</td></tr>
<tr><td>1 x HP 512MB FBWC for P-Series Smart Array</td><td>1 x HP 512MB FBWC for P-Series Smart Array</td><td>1TB - 2 x HP 500GB 6G SATA 7.2K 2.5in SC MDL HDD</td></tr>
<tr><td>2 x HP dual port 10GbE</td><td>2 x HP dual port 10GbE</td><td>2 x HP 2GB P-seris Smart Array FBWC</td></tr>
<tr><td></td><td></td><td>2 x HP Smart Array P420i Mez Ctrllr FIO Kit</td></tr>
<tr><td></td><td></td><td>1 x HP 10G IO Module (2x1GbE 2x10GbE)</td></tr>
</table>

## For more information

[Ceph Architecture Reference](/helion/openstack/1.1/services/object/ceph/archref/)


<a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top &#8593; </a>

----
