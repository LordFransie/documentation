./1.0commercial.backup-restore-GA.md:You should create a backup of the overcloud servers when any Update and Extension is download to the system.
./1.0commercial.backup-restore-GA.md:If the admin user password was changed from the original password created during the installation process, you need to update the password in some files before performing the undercloud backup or restore process. If this process has been done and the files contain the correct password, you do not need to edit the files.
./1.0commercial.backup-restore-GA.md:4. Update the `UNDERCLOUD_ADMIN_PASSWORD=` line with the new password and save the file.
./1.0commercial.backup-restore-GA.md:6. Update the `undercloud` line with the new password and save the file.
./1.0commercial.backup-restore-GA.md:9. Update the `OS_PASSWORD=` line with the new password and save the file.
./1.0commercial.eula.md:- You may not download and use patches, enhancements, bug fixes, or similar updates unless you have a license to the underlying Software. However, such license doesn't automatically give you a right to receive such updates and HP reserves the right to make such updates only available to customers with support contracts.
./1.0commercial.eula.md:**l. Updates and Supplements:**  We may update or supplement the Software. If so, you may use that update or supplement with the Software, subject to any additional terms that accompany the update or supplement.
./1.0commercial.faq.md:* [How can I update the software?](#softwareupdate)
./1.0commercial.faq.md:<!-- Removed per Gary's comments I HP Helion OpenStack is the first OpenStack distribution from HP. It is free to download and will be kept updated frequently to stay current with OpenStack developments. It is the foundational technology for all HP Helion products in the future.-->
./1.0commercial.faq.md:<!-- Removed per JR's comments The final version of this free distribution enables you to evaluate OpenStack and to deploy small-scale private clouds in production (up to 30 nodes). They will be able to do this more quickly and easily with HP's simplified installation and update mechanisms.-->
./1.0commercial.faq.md:HP Helion OpenStack is one of the first distributions in the market, based on up-to-date code from the OpenStack community. It is derived from pure OpenStack and open source technologies, configured and packaged by HP. For example, TripleO, which is used for installation and update, is the official OpenStack project for deployment.-->
./1.0commercial.faq.md:The undercloud server is a basic single-node OpenStack installation running on a single physical server used to deploy, test, manage, and update the overcloud servers. There is no HA configuration for the undercloud. 
./1.0commercial.faq.md:It contains a strictly limited sub-set of OpenStack, just enough to interact with the overcloud. The services running on the undercloud are Nova, Neutron, Glance, Keystone, Ironic, Heat, Horizon & Ceilometer. This server also contains HP Helion content distribution catalog  service, which provides a mechanism to download and install content and updates for the overcloud.
./1.0commercial.faq.md:####How can I update the software?<a name="softwareupdate"></a>
./1.0commercial.faq.md:HP provides automated, live distribution of updates.
./1.0commercial.glossary.md:A catalog-based HP Helion OpenStack service that allows off-the-shelf content (like workloads, images, and patches) to be imported into your deployed cloud. To access HDN, see the Updates & Extensions panel in the  HP Helion OpenStack user interface. You can import content from the HDN portal or from a local folder.
./1.0commercial.index.md:**Note:** HP OpenStack Helion does not support an update utility to update from Helion 1.0 or Helion 1.01 to Helion 1.1. For help with updating from your current release to a newer release, contact [HP Customer Support](http://www.hpcloud.com/about/contact). 
./1.0commercial.install-add-nodes.md:3. Source the environment variables file that you updated:  
./1.0commercial.install-add-nodes.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commercial.install-add-nodes.md:9. Source the environment variables file that  you updated:  
./1.0commercial.install-add-nodes.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commercial.install-add-nodes.md:12. Source the environment variables file that  you updated:  
./1.0commercial.install-add-nodes.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-overcloud 2>&1 | tee update.log-->
./1.0commercial.install-GA-DNSaaS.md:3. Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.install-GA-DNSaaS.md:	b.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.install-GA-ESX-Proxy.md:	The new clusters will be updated in the nova-compute.conf of the VM
./1.0commercial.install-GA-LDAP.md:	<td>group_allow_update</td><td>(BoolOpt) Allow group update in LDAP backend.</td><td>False</td><td>False</td>
./1.0commercial.install-GA-LDAP.md:	<td>user_allow_update</td><td>(BoolOpt) Allow user updates in LDAP backend.</td><td>False</td><td>False</td>
./1.0commercial.install-GA-LDAP.md:2. Disable CRUD (Create/Update/Delete) operations on users for a read-only identity backend on all three overcloud controller nodes.
./1.0commercial.install-GA-LDAP.md:	                        "option": "user_allow_update",
./1.0commercial.install-GA-LDAP.md:	                        "option": "group_allow_update",
./1.0commercial.install-GA-NTP.md:		hwclock [--utc | --localtime] -s --hctosys (update system time based on hardware clock)
./1.0commercial.install-GA-NTP.md:		hwclock [--utc | --localtime] -w --systohc (update hardware clock based on system time)
./1.0commercial.install-GA-NTP.md:		hwclock [--utc | --localtime] --systz (update system time based on timezone)
./1.0commercial.install-GA-ovsvapp.md:* [Update OVSvApp](#update)
./1.0commercial.install-GA-ovsvapp.md:	Even if pyvmomi is already installed, run the command again to update the library to get major fixes.
./1.0commercial.install-GA-ovsvapp.md:## Update OVSvApp {#update}
./1.0commercial.install-GA-ovsvapp.md:To update the OVSvApp from version 1.0 to version 1.01:
./1.0commercial.install-GA-security.md:5. Update required access control lists or firewall rules to allow traffic to the Public VIP IP.
./1.0commercial.install-GA-security.md:<td>4</td><td>Container update over HTTP</td><td>Object Storage</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002</td></tr>
./1.0commercial.install-GA-security.md:<td>5</td><td>Container update over HTTP</td><td>Object Storage</td><td>Swift all in one (PACO)</td><td>6001,6002
./1.0commercial.install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Proxy-Account-Container (PAC)</td><td>Swift all in one (PACO)</td><td>6001,6002</td></tr>
./1.0commercial.install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Swift all in one (PACO)</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002
./1.0commercial.install-GA-vsa.md:3a - **Run update cloud script to provision VSA node**
./1.0commercial.install-GA-vsa.md:   * Update the *overcloud.json* file for StoreVirtual deployment and apply the configuration.
./1.0commercial.install-GA-vsa.md:   * Execute the update cloud script.
./1.0commercial.install-GA-vsa.md:5a - **Update `overcloud-config.json` file with cinder configuration**
./1.0commercial.install-GA-vsa.md:   With the advise generated from the above steps, update the overcloud-config.json file in the seed cloud.
./1.0commercial.install-GA-vsa.md:5b - **Run update cloud script to update cinder.conf**
./1.0commercial.install-GA-vsa.md:   * The cinder.conf in the overcloud should be updated after updating the overcloud-config.json file in the seed cloud.
./1.0commercial.install-GA-vsa.md:   * Execute [update cloud script](/helion/openstack/ga/undercloud/oc/config/storevirtual/) from seed cloud. -->
./1.0commercial.install-GA-vsa.md:		# apt-get update
./1.0commercial.install-GA-vsa.md:6. Edit the `/root/baremetal.csv file` in seed cloud with the details of the newly added node. <!--7. Edit the `/root/tripleo/configs/kvm-custom-ips.json` and update the value for `vsa_scale` or `vsa_ao_scale` appropriately.-->
./1.0commercial.install-GA-vsa.md:7. Edit the environment variables file used during initial installation and update the value for `vsa_scale` or `vsa_ao_scale` appropriately
./1.0commercial.install-GA-vsa.md:	<!--**Note**: Source the same file (`kvm-default.json` or `kvm-custom-ips.json`) that is used during the initial installation for specifying the environment IPs.9. Run the installer script to update the overcloud. During the installation, the number of StoreVirtual storage systems that you specified in the `overcloud-config.json`, are deployed.-->
./1.0commercial.install-GA-vsa.md:9. Run the installer script to update the overcloud. During the installation, the number of StoreVirtual storage systems that you specified in the environment variables file are deployed. 
./1.0commercial.install-GA-vsa.md: 	 	# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update.log
./1.0commercial.networking-maskedIP.md:	    | updated                              | 2014-06-26T01:41:05Z          |
./1.0commercial.release-notes.md:* If you use the Updates and Extensions tab of the Helion Dashboard for the overcloud (known as the *Sherpa UI*) to download two images that use the same name in two different projects, the second image will fail to upload to the Image Operations service (Glance). Avoid using images that use the same name. <!-- (C O D N-24) -->
./1.0commercial.release-notes.md:* After updating the undercloud to HP Helion OpenStack 1.01, existing HP StoreVirtual clusters might not work. The cluster is listed in the Helion Dashboard, but you might not be able to activate or unregister the cluster. If this occurs, you can [manually register a new cluster](/helion/openstack/undercloud/storage/storevirtual/#register-cluster) with the same name and information as the non-working cluster. You should be able to activate the cluster. <!-- (ANSUPDATE-155) -->
./1.0commercial.release-notes.md:* A user can register but cannot update a vCenter through the UI. 
./1.0commercial.sirius-cli-workflow.md:* [Reconfigure and update cloud](#reconfigure-update)
./1.0commercial.sirius-cli-workflow.md:	| updated_at | 2014-08-23 02:49:41.066921           |
./1.0commercial.sirius-cli-workflow.md:	| updated_at   | 2014-08-23 03:00:30.503439           |
./1.0commercial.sirius-cli-workflow.md:##Reconfigure and update cloud {#reconfigure-update}
./1.0commercial.sirius-cli-workflow.md:3. [Update Overcloud configuration JSON](#update-overcloud-json)
./1.0commercial.sirius-cli-workflow.md:5. [Update Overcloud](#update-overcloud) 
./1.0commercial.sirius-cli-workflow.md:The backends configured in the undercloud Sirius database will not be effective until the overcloud Cinder configuration is updated.
./1.0commercial.sirius-cli-workflow.md:### Update Overcloud configuration JSON {#update-overcloud-json}
./1.0commercial.sirius-cli-workflow.md:Update the `/root/overcloud-config.json` in the Seed node with the generated backend data. Add the StoreVirtual backend configuration as a JSON key-pair with key **vsa** and StoreServ backend configuration with key **3par** to the existing JSON.
./1.0commercial.sirius-cli-workflow.md:A sample of the file is before and after the update is give below:
./1.0commercial.sirius-cli-workflow.md:**Before the update:**
./1.0commercial.sirius-cli-workflow.md:**After the update:**
./1.0commercial.sirius-cli-workflow.md:###Update Overcloud {#update-overcloud}
./1.0commercial.sirius-cli-workflow.md:Enter the following command to update the overcloud:
./1.0commercial.sirius-cli-workflow.md:    # bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud
./1.0commercial.sirius-cli-workflow.md:When the update has completed, the Cinder service in the overcloud will be configured to have the StoreVirtual clusters and StoreServ CPG as backends.
./1.0commercial.sirius-cli.md:### Update StoreVirtual cluster ###
./1.0commercial.sirius-cli.md:You can update the StoreVirtual cluster in Sirius database. Only the CLUSTER_ID is  a mandatory argument.
./1.0commercial.sirius-cli.md:	# sirius update-storevirtual-cluster <CLUSTER_ID>  -name=<VCENTER_NAME> --ip-address=<VCENTER_IP_ADDRESS> --username=<VCENTER_USERNAME> --password=<VCENTER_PASSWORD> --port=<VCENTER_PORT> --status=<CLOUD_STATUS>
./1.0commercial.sirius-cli.md:### Update StoreServ ###
./1.0commercial.sirius-cli.md:You can update the StoreServ details in Sirius.
./1.0commercial.sirius-cli.md:	# sirius update-storeserv <STORESERV_ID>
./1.0commercial.site-index.md:* [Updating HP Helion OpenStack](#update)
./1.0commercial.site-index.md:## Updating HP Helion OpenStack ## {#update}
./1.0commercial.site-index.md:Review the following topics before you start an update:
./1.0commercial.site-index.md:* [Update Overview](/helion/openstack/update/overview/101/)
./1.0commercial.site-index.md:* [Obtaining the Patch Update](/helion/openstack/update/download/101/)
./1.0commercial.site-index.md:* [Update Prerequisites](/helion/openstack/update/prereqs/101/)
./1.0commercial.site-index.md:* [Monitoring the Update](/helion/openstack/update/monitor/101/)
./1.0commercial.site-index.md:* [Updating the Seed Cloud Host](/helion/openstack/update/seed/101/)
./1.0commercial.site-index.md:* [Updating the Undercloud](/helion/openstack/update/undercloud/101/)
./1.0commercial.site-index.md:* [Updating the Overcloud](/helion/openstack/update/overcloud/101/)
./1.0commercial.site-index.md:* [Starting and Stopping the Development-Platform Services for the Upgrade](/helion/openstack/update/devplatstop/101/)
./1.0commercial.site-index.md:* [Development Platform Commercial 1.01 Update](/helion/openstack/update/devplat/101/)
./1.0commercial.site-index.md:* [Update Troubleshooting](/helion/openstack/update/troubleshooting/101/)
./1.0commercial.site-index.md:- [Updates and Extensions](/helion/openstack/undercloud/admin/updates-and-extension/)
./1.0commercial.technical-overview.ga.md:* [Updates and extensions](#updates-and-extensions)
./1.0commercial.technical-overview.ga.md:	<td>Administrators can download content such as software patches and updates from the HP Helion CODN and apply the downloaded content to their Helion OpenStack installation. </td>
./1.0commercial.technical-overview.ga.md:	<td>The Seed cloud is deployed as a VM instance. This image contains minimal OpenStack services required to deploy and update the undercloud on a baremetal server.
./1.0commercial.technical-overview.ga.md:	 <td>A single-server deployment of a limited set of OpenStack services, called the undercloud, is used to deploy, test, manage, and update all the overcloud servers. 
./1.0commercial.technical-overview.ga.md:## Updates and extensions {#updates-and-extensions}
./1.0commercial.technical-overview.ga.md:Updates and extensions provides a mechanism to download and install the content and updates for the overcloud.
./1.0commercial.technical-overview.ga.md:[Updates and extension](/helion/openstack/undercloud/admin/updates-and-extension/).
./1.0commercial.troubleshooting.md:3. [Failure of Update overcloud](#failure-update-overcloud)
./1.0commercial.troubleshooting.md:11. [Ironic intermitently set maintenance mode to True during update](#ironic)
./1.0commercial.troubleshooting.md:### Failure of Update overcloud {#failure-update-overcloud}
./1.0commercial.troubleshooting.md:Update overcloud fails with the following error:
./1.0commercial.troubleshooting.md:2. Edit `/root/tripleo/ce_env.json `and update the right variable for build&#95;number and installed&#95;build&#95;number. <!-- (CORE-1697) -->
./1.0commercial.troubleshooting.md:3.Run the installer script to update the overcloud. 
./1.0commercial.troubleshooting.md:		# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.0commercial.troubleshooting.md:This issue can happen during the update of undercloud or overcloud nodes. The update will fail for one or more nodes. <!-- CORE-2082 -->
./1.0commercial.troubleshooting.md:If the update fails, from undercloud node:
./1.0commercial.troubleshooting.md:		`ironic node-update <id> replace maintenance=False`
./1.0commercial.troubleshooting.md:5. [Unable to update the default input json file ](#unable-update-json)
./1.0commercial.troubleshooting.md:### Unable to update the default input json file {#unable-update-json}
./1.0commercial.troubleshooting.md:Parsing the default JSON file failed. Unable to update the default input json file.
./1.0commercial.troubleshooting.md:The script will parse the configuration file and update the values based on the network and configuration files.
./1.0commercial.troubleshooting.md:* On success, the script updates the `/mnt/state/vsa/vsa_config.json` file with the updated and created time.
./1.0commercial.undercloud-admin-updates-and-extensions.md:title: "HP Helion OpenStack&#174; 1.0: Updates and Extensions"
./1.0commercial.undercloud-admin-updates-and-extensions.md:permalink: /helion/openstack/undercloud/admin/updates-and-extension/
./1.0commercial.undercloud-admin-updates-and-extensions.md:# HP Helion OpenStack&#174; 1.0: Updates and Extensions
./1.0commercial.undercloud-admin-updates-and-extensions.md:[See the Helion OpenStack 1.1 version of this page](/helion/openstack/1.1/undercloud/admin/updates-and-extension/)
./1.0commercial.undercloud-admin-updates-and-extensions.md:HP provides a simple mechanism for downloading and publishing the content and updates for the Undercloud node.
./1.0commercial.undercloud-admin-updates-and-extensions.md:You cannot access the updates from the HP Helion Horizon undercloud dashboard unless you are a registered user.
./1.0commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.undercloud-admin-updates-and-extensions.md:6.	Click **OK** to save the details.<br />The Updates and Extensions page is displayed with the list of available patches.<br />
./1.0commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.undercloud-admin-updates-and-extensions.md:5.	Click **Import**.<br />The file is imported to your local cloud and displayed in the Updates and Extensions page.
./1.0commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.0commercial.undercloud-admin-updates-and-extensions.md:Click the **More** drop-down list and select **Download** next to the package that you want to download on your local system. A Confirm Download Update dialog box is displayed.
./1.0commercial.undercloud-admin-updates-and-extensions.md:7. (Optional) To view the download status, click the **More** drop-down list and select **View Progress**. <br />The View Progress box is displayed, containing the log, updated name, updated version, task type, and the status of the task. 
./1.0commercial.undercloud-eon-cli.md:### Update vCenter<a name="update-vcenter"></a>
./1.0commercial.undercloud-eon-cli.md:You can update the exisitng vCenter to the EON database. You must enter all the arguments to add the vCenter; otherwise you will be prompted to enter them. 
./1.0commercial.undercloud-eon-cli.md:	# eon vcenter-update [--name <VCENTER_NAME>] [--ip-address <VCENTER_IP>][--username <VCENTER_USERNAME>][--password <VCENTER_PASSWORD>][--port <VCENTER_PORT>]                       <VCENTER_ID>
./1.0commercial.undercloud-eon-cli.md:###Update a cluster<a name="cluster-update"></a>
./1.0commercial.undercloud-eon-cli.md:You can update the cluster details. 
./1.0commercial.undercloud-eon-cli.md:	# eon cluster-update --vcenter-id <VCENTER_ID> --cluster-moid <CLUSTER_MOID> --state <STATE>
./1.0commercial.undercloud-oc-config-storeserv.md:* [Update Overcloud](#update-overcloud) 
./1.0commercial.undercloud-oc-config-storeserv.md:**Note**: Ensure that you allocate only those CPGs that will be used by this cloud. Changing any attributes of the CPG after allocation, may disrupt cloud functionality if the corresponding change is not updated in Sirius.
./1.0commercial.undercloud-oc-config-storeserv.md:8. Click **Update**.<br />On successful update, the number of CPGs mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.0commercial.undercloud-oc-config-storeserv.md:6. Click **Update**.
./1.0commercial.undercloud-oc-config-storeserv.md:	<br />On successful update, the number of CPGs mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.0commercial.undercloud-oc-config-storeserv.md:5. Click **OK** to download and save the file. <br />Once you download the configuration file, you can proceed to update the overcloud configuration.<br />
./1.0commercial.undercloud-oc-config-storeserv.md:### Update Overcloud {#update-overcloud}
./1.0commercial.undercloud-oc-config-storeserv.md:To update your overcloud with the changes, do the following:
./1.0commercial.undercloud-oc-config-storeserv.md: 	# cp /root/tripleo/tripleo-incubator/scripts/ee-config.json /root/overcloud-config.json	4. Edit and update the /root/overcloud-config.json and add the JSON snippet(obtained from [Generate Config](#generate-config)). Ensure the JSON file format is unbroken. A sample of the file is given below:-->
./1.0commercial.undercloud-oc-config-storeserv.md:2. Edit and update the `tripleo/configs/kvm-custom-ips.json` and add the JSON snippet(obtained from [Generate Config](#generate-config)). Ensure the JSON file format is unbroken. A sample of the file is given below:
./1.0commercial.undercloud-oc-config-storeserv.md:5.Launch install script to update the overcloud.
./1.0commercial.undercloud-oc-config-storeserv.md:		# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update-bv1.log
./1.0commercial.undercloud-oc-config-storevirtual.md:* [Update Overcloud](#update-overcloud)
./1.0commercial.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-expand-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option(opens in a new window)</a>
./1.0commercial.undercloud-oc-config-storevirtual.md:8. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.0commercial.undercloud-oc-config-storevirtual.md:	<!---<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-add-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option (opens in a new window)</a>-->
./1.0commercial.undercloud-oc-config-storevirtual.md:6. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.0commercial.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-shrink-backend1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Shrink Backend Page with Update Option (opens in a new window)</a>
./1.0commercial.undercloud-oc-config-storevirtual.md:4. Click **OK** to download and save the file.<br />Once you download the configuration file, you can proceed to update the overcloud configuration.
./1.0commercial.undercloud-oc-config-storevirtual.md:### Update Overcloud {#update-overcloud}
./1.0commercial.undercloud-oc-config-storevirtual.md:To update your overcloud with the changes, do the following:
./1.0commercial.undercloud-oc-config-storevirtual.md:	4. Edit and update the `/root/overcloud-config.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:
./1.0commercial.undercloud-oc-config-storevirtual.md:	2. Edit and update the `tripleo/configs/kvm-custom-ips.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:--> 
./1.0commercial.undercloud-oc-config-storevirtual.md:5. Launch install script to update the overcloud.
./1.0commercial.undercloud-oc-config-storevirtual.md:		# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update-bv1.log
./1.0commercial.undercloud-storage-storeserv.md:Once you register the 3PAR device, choose the CPGs for your cloud. CPG (Common Provisioning Group) is a fundamental unit that can be configured as a Cinder backend. A single HP 3PAR StoreServ may have multiple CPGs. You can choose and allocate them to the cloud as a per your requirement. <!---First register the CPG in Sirius and then configure the CPG as backend for overcloud Cinder service. You can register a few CPGs and allocate them as and when required. The Cinder configuration file is updated only during the allocation, the registeration of the CPG just updates the Sirius database.-->
./1.0commercial.undercloud-storage-storeserv.md:5. Click **Edit CPG** against the CPG that you want to edit.<br /> The Update CPG Details dialog box is displayed. 
./1.0commercial.undercloud-storage-storevirtual.md:**Note**: Ensure that you edit the StoreVirtual cluster only if there are any updates made through the CMC for the selected cluster. After editing the details, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./1.0commercial.undercloud-storage-storevirtual.md:**Note**: When you unregister a cluster,the volumes from this cluster backend will no longer be available through Cinder. Ensure that you detach all relevant volumes and remove the backend associated with the cluster before unregistering. After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information. 
./1.0commercial.undercloud-storage-storevirtual.md:**Note**: After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./1.0commercial.update-101-devplat.md:title: "HP Helion OpenStack&#174; 1.0: Development Platform Commercial 1.01 Update"
./1.0commercial.update-101-devplat.md:permalink: /helion/openstack/update/devplat/101/
./1.0commercial.update-101-devplat.md:# HP Helion OpenStack&#174; 1.0: Development Platform Commercial 1.01 Update
./1.0commercial.update-101-devplat.md:The following sections explain how to update the HP Helion Development Platform.
./1.0commercial.update-101-devplat.md:* [Update the Messaging Service](#update-messaging)
./1.0commercial.update-101-devplat.md:* [Update the Application Lifecycle Service](#update-als)
./1.0commercial.update-101-devplat.md:* [Update the Database Service](#update-database)
./1.0commercial.update-101-devplat.md:* [Update the Marketplace Service](#update-marketplace)
./1.0commercial.update-101-devplat.md:The following guide outlines the steps to update Helion Development Platform from version 1.0 to 1.01. Before executing these steps, ensure you have upgraded HP Helion OpenStack to 1.01 as described in the [HP Helion OpenStack Update Overview](/helion/openstack/update/overview/101/).
./1.0commercial.update-101-devplat.md:**Note:** If you created a new installation of 1.01, bypassing 1.0, you need to install HP Helion OpenStack Development Platform Services, and not update. See [Development Platform Install Guide](http://docs.hpcloud.com/helion/devplatform/install/).
./1.0commercial.update-101-devplat.md:## Update the Messaging Service {#update-messaging} 
./1.0commercial.update-101-devplat.md:## Update the Application Lifecycle Service (ALS) {#update-als}
./1.0commercial.update-101-devplat.md:## Update the Database Service {#update-database}
./1.0commercial.update-101-devplat.md:## Update the Marketplace Service {#update-marketplace}
./1.0commercial.update-101-devplat.md:### Download the Marketplace Service {#update-marketplace}
./1.0commercial.update-101-devplat.stopstart.md:permalink: /helion/openstack/update/devplatstop/101/
./1.0commercial.update-101-devplat.stopstart.md:If you have installed the [HP Helion Development Platform](/helion/devplatform/), you need to take the service control plane down before performing updates on the overcloud. 
./1.0commercial.update-101-devplat.stopstart.md:Services must be stopped in the correct order before the overcloud compute nodes can be updated. After the update is complete, the control plane must be restarted with a command that ensures services are restarted in order.
./1.0commercial.update-101-devplat.stopstart.md:	If this check fails, you must repair the cluster by manually logging into the target machines. Do not run any update commands on a failed cluster. Trying to update an unhealthy cluster may result in the Development Platform services becoming non-functional or non-recoverable after the overcloud update. 
./1.0commercial.update-101-devplat.stopstart.md:**Note:** Stopping the services will cause the corresponding service API to be unavailable until the services are restarted after the update on the overcloud is complete.
./1.0commercial.update-101-devplat.stopstart.md:After the overcloud update is completed, use the following command to start each service in the cluster in the correct order. By successfully starting the clusters, you will be able to bring the corresponding Dev-Platform service into a functional state and the API can be used after this. 
./1.0commercial.update-101-monitor.md:title: "HP Helion OpenStack&#174; 1.0: Monitoring the Update"
./1.0commercial.update-101-monitor.md:permalink: /helion/openstack/update/monitor/101/
./1.0commercial.update-101-monitor.md:# HP Helion OpenStack&#174; 1.0: Monitoring the Update
./1.0commercial.update-101-monitor.md:To assess the progress of HP Helion Cloud update, the progress utility provides an overview of the update status.    
./1.0commercial.update-101-monitor.md:This provides an option to get a snapshot of the current state or to monitor the update progress.
./1.0commercial.update-101-monitor.md:To get a snapshot of current status of HP Helion OpenStack update:
./1.0commercial.update-101-monitor.md:		cd /opt/stack/tripleo-ansible/update_helpers/
./1.0commercial.update-101-monitor.md:To get a snapshot of current status of HP Helion update:
./1.0commercial.update-101-monitor.md:		cd /opt/stack/tripleo-ansible/update_helpers/
./1.0commercial.update-101-obtaining.md:title: "HP Helion OpenStack&#174; 1.0: Obtaining the Patch Update Package"
./1.0commercial.update-101-obtaining.md:permalink: /helion/openstack/update/download/101/
./1.0commercial.update-101-obtaining.md:# HP Helion OpenStack&#174; 1.0: Obtaining the Patch Update Package
./1.0commercial.update-101-obtaining.md:An HP Helion OpenStack patch update is a series of images and scripts that enhance functionality or fix issues found in a previous Helion release.  
./1.0commercial.update-101-obtaining.md:Before beginning the update, obtain the patch update package using one of the two following methods:
./1.0commercial.update-101-obtaining.md:To use the Helion Distribution Network (HDN) to download the patch update from the catalog.
./1.0commercial.update-101-obtaining.md:2. Navigate to **Admin -> Updates and Extensions -> Updates and Extensions**.
./1.0commercial.update-101-obtaining.md:5. Select patch update file and click **Download** from from [HP Helion OpenStack product installation](https://helion.hpwsportal.com/catalog.html#/Home/Show)
./1.0commercial.update-101-obtaining.md:		<td>HP Helion OpenStack Upgrade</td><td>HP_Helion_OpenStack_1.0_to_1.0.1_Update.csu</td></tr>
./1.0commercial.update-101-obtaining.md:	The patch update file might take a long time to download, depending upon your connection speed.  
./1.0commercial.update-101-obtaining.md:7. When the download is complete, click this **Publish** button to extract the package contents into the undercloud repositories necessary to perform the update.
./1.0commercial.update-101-obtaining.md:If you have no internet access on the undercloud or you prefer not to use the internet, you can download directly from the download site using a browser. Then move or copy the patch update file to the undercloud node. 
./1.0commercial.update-101-obtaining.md:3. Locate the patch update file and click **Download**. You should have received and email with the patch update file name and location.
./1.0commercial.update-101-obtaining.md:	The patch update file might take a long time to download, depending upon your connection speed.  
./1.0commercial.update-101-obtaining.md:4. Use a method as appropriate for your organization to move or copy the patch update file to the undercloud.
./1.0commercial.update-101-obtaining.md:6. Navigate to **Admin -> Updates and Extensions -> Updates and Extensions**.
./1.0commercial.update-101-obtaining.md:8.	Select the CSU patch update file to download.
./1.0commercial.update-101-obtaining.md:9.	Click **Import** to deliver the patch update file for further processing.
./1.0commercial.update-101-obtaining.md:10. Once the import of the package is complete, click the **Publish** button next to the package to extract the downloaded package contents into the undercloud repositories necessary to perform the update.
./1.0commercial.update-101-obtaining.md:Review the Prerequisites for information or tasks that need to be performed before starting the update. 
./1.0commercial.update-101-obtaining.md:For prerequisites, see the [Update Prerequisites](/helion/openstack/update/prereqs/101/).
./1.0commercial.update-101-overcloud.md:permalink: /helion/openstack/update/overcloud/101/
./1.0commercial.update-101-overcloud.md:The *Readme.txt* that comes with a patch update lists the nodes that need to be updated as a result of this patch. This file is located in the directory described in the [Update Troubleshooting](/helion/openstack/update/troubleshooting/101/) of the Update Prerequisites.  
./1.0commercial.update-101-overcloud.md:If the Readme.txt does not list any overcloud nodes, the update is complete. -->
./1.0commercial.update-101-overcloud.md:* [Update the overcloud](#update)
./1.0commercial.update-101-overcloud.md:* [Validate the update](#validate)
./1.0commercial.update-101-overcloud.md:You can monitor the update process, see [Monitoring the Update](/helion/openstack/update/monitor/101/).
./1.0commercial.update-101-overcloud.md:## Prerequisites for the overcloud update ## {#pre}
./1.0commercial.update-101-overcloud.md:Before you begin the update:
./1.0commercial.update-101-overcloud.md:* If the undercloud needed updating, perform this update before updating the overcloud, as described in [Updating the Undercloud Host](/helion/openstack/update/undercloud/101/)
./1.0commercial.update-101-overcloud.md:* Review the [update prerequisites](/helion/openstack/update/prereqs/101/) and make sure all necessary tasks have been performed, including [extracting the update scripts](/helion/openstack/update/prereqs/101/#extract).
./1.0commercial.update-101-overcloud.md:* Point the install script to the overcloud. The patch update scripts are based on the Ansible platform. When patching the overcloud, because the script is launched from the seed cloud host, you need to point the script to the overcloud node.
./1.0commercial.update-101-overcloud.md:	To point the script to update the overcloud, use the following steps:
./1.0commercial.update-101-overcloud.md:	The command prompt should change to `(ansible)`. You will need to use this `(ansible)` session to perform all the update operations.
./1.0commercial.update-101-overcloud.md:Before you run the patch update on the overcloud, you must stop the HP Helion Development Platform service using a script. See [Stopping and Starting the Development Platform Services](/helion/openstack/update/devplatstop/101/). After the update is complete, you can execute another script to [restart the service](#devplatstart).
./1.0commercial.update-101-overcloud.md:## Update the overcloud ## {#update}
./1.0commercial.update-101-overcloud.md:There are two methods to update the overcloud: 
./1.0commercial.update-101-overcloud.md:You can monitor the update process, see [Monitoring the Update](/helion/openstack/update/monitor/101/).
./1.0commercial.update-101-overcloud.md:### Update the overcloud using the script ### {#upgradescript}
./1.0commercial.update-101-overcloud.md:To upgrade the overcloud using the `HelionUpdate.sh` script included in the patch update download, use the following steps:
./1.0commercial.update-101-overcloud.md:3. Run the following command to start the update:
./1.0commercial.update-101-overcloud.md:		./update-helpers/HelionUpdate.sh
./1.0commercial.update-101-overcloud.md:	This script will setup the environment, allow you to do test `ping`, and perform pre-update checking, update the nodes in a specific order.
./1.0commercial.update-101-overcloud.md:	The script allows you to update by node type. For example all the controllers are done as a group. Inside each group the individual nodes are presented for the user to update, skip or exit. If you wish to skip a whole group skip at the group level. 
./1.0commercial.update-101-overcloud.md:	You might want to skip a node if you want to test the update on a particular node or if you want to resume an update later on. To skip a node, press the `s` key until you get to a node you want to update.
./1.0commercial.update-101-overcloud.md:	**Note:** The script will detect if a node fails to update. If such a failure is detected the script will exit.  When this happens you should refer to the [Update Troubleshooting](/helion/openstack/update/troubleshooting/101/) document for known workarounds.
./1.0commercial.update-101-overcloud.md:4. When the node update is done, validate the updated node. 
./1.0commercial.update-101-overcloud.md:5. Manually reboot the compute nodes that were updated or using `nova stop` and `nova start` in the undercloud.
./1.0commercial.update-101-overcloud.md:**Note:**  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to [Redeploy Compute Proxy and OVSvAPP on ESX Host](#redeploy) after updating Controller Management node and before proceeding to controller nodes.
./1.0commercial.update-101-overcloud.md:### Update the undercloud manually ### {#upgrademanual}
./1.0commercial.update-101-overcloud.md:The manual update process allows you to upgrade each node in order. 
./1.0commercial.update-101-overcloud.md:Make sure that the updated node is back up and running before you update other nodes. This is especially important for the overcloud controllers.  
./1.0commercial.update-101-overcloud.md:The recommended update order is 
./1.0commercial.update-101-overcloud.md:#### Prerequisites for manual update ###
./1.0commercial.update-101-overcloud.md:* Determine the overcloud nodes that will need to be updated. The *Readme.txt* that comes with a patch update will tell you what nodes need to be updated as a result of this patch. It will be located in the directory described in the [Extracting the required scripts and libraries](/helion/openstack/update/prereqs/101/#extract).  
./1.0commercial.update-101-overcloud.md:	For each node, obtain the image ID and IP address. For each of these, record the ID, to get the ID, see [Gathering information needed for update](/helion/openstack/update/prereqs/101/#info).
./1.0commercial.update-101-overcloud.md:To manually update the overcloud nodes:
./1.0commercial.update-101-overcloud.md:1. Update the overcloud management controller using the following command:
./1.0commercial.update-101-overcloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True  -l <IP of controller mgmt> -e controllermgmt_rebuild_image_id=<glance Image_ID of  overcloud-control-mgmt> playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:	b. Run `progress.sh` script to validate that the image ID matches the one expected by the update by running the following command:
./1.0commercial.update-101-overcloud.md:		./update-helpers/progress.sh
./1.0commercial.update-101-overcloud.md:	**Note:**  If your cloud has ESX host and the update includes new images for ESX Proxy and ESX OVSvAPP, refer to [Redeploy Compute Proxy and OVSvAPP on ESX Host](#redeploy) after updating Controller Management node and before proceeding to controller nodes.
./1.0commercial.update-101-overcloud.md:3. Update overcloud controller0 using the following command:
./1.0commercial.update-101-overcloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True -l <IP of controller> -e controller_rebuild_image_id=<glance Image_ID of overcloud-control> playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:	b. Run `progress.sh` script to validate that the image ID matches the one expected by the update by running the following command:
./1.0commercial.update-101-overcloud.md:		cd /opt/stack/tripleo-ansible/update-helpers/
./1.0commercial.update-101-overcloud.md:4. Update overcloud controller1 using the following command:
./1.0commercial.update-101-overcloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -e single_controller=True -l <IP of controller> -e controller_rebuild_image_id=<glance Image_ID of overcloud-control> playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:	b. Run `progress.sh` script to validate that the image ID matches the one expected by the update by running the following command:
./1.0commercial.update-101-overcloud.md:		cd /opt/stack/tripleo-ansible/update-helpers/
./1.0commercial.update-101-overcloud.md:You should update the Compute nodes in such a way as to allow workloads to continue to function.  
./1.0commercial.update-101-overcloud.md:To do this, migrate the workloads to nodes that will not be down during a particular compute node set update and then migrate them back when done.  
./1.0commercial.update-101-overcloud.md:1. Use the `nova list` command in the undercloud to see the full set of compute nodes available to be updated.
./1.0commercial.update-101-overcloud.md:4.	If this update affects several maintenance cycles, you can run the progress script to see where they are in the update process.  
./1.0commercial.update-101-overcloud.md:	This command will update only one compute node, you will have to repeat it for every compute IP address.
./1.0commercial.update-101-overcloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l <IP of overcloud-compute > -e nova_compute_rebuild_image_id =<glance Image_ID of overcloud-compute > playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:	This command may update only update a subset of compute nodes, you are responsibe to update all the nodes with multiple commands if necessary.
./1.0commercial.update-101-overcloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py  -e force_rebuild=True -l IP_Compute_1:IP_Compute_2:IP_Compute_3...:IP_Compute_n -e nova_compute_rebuild_image_id=<glance Image_ID of overcloud-compute > playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:5. Manually reboot the compute nodes that were updated or using `nova stop` and `nova start` in the undercloud.
./1.0commercial.update-101-overcloud.md:For Swift, it is strongly encouraged that you update one node at a time (node by node).
./1.0commercial.update-101-overcloud.md:The following command updates only one Swift node; you will have to repeat it for every swift IP address.
./1.0commercial.update-101-overcloud.md:	ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l <IP of overcloud-swift nscale> -e swift_storage_rebuild_image_id =<glance Image_ID of overcloud-swift nscale> playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:This command updates only one VSA node; you will have to repeat it for every VSA IP address.
./1.0commercial.update-101-overcloud.md:	ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l <IP of vsa overcloud node> -e vsa_rebuild_image_id =<glance Image_ID of vsa overcloud node > playbooks/update_cloud.yml
./1.0commercial.update-101-overcloud.md:## Back up the updated overcloud ### {#backup}
./1.0commercial.update-101-overcloud.md:Once the update of overcloud node is complete, you should back up the node in case of failures.  See [Back Up and Restore](/helion/openstack/backup.restore/).
./1.0commercial.update-101-overcloud.md:If your infrastructure includes ESX proxy hosts, update the Compute Proxy and OVSvAPP after updating the overcloud controller management node. The update package contains new images for ESX. Failure to do so will prevent users from launching VMs in vCenter Hosts.
./1.0commercial.update-101-overcloud.md:After the update is complete, you can execute a script to restart the service. See [Stopping and Starting the Development Platform Services](/helion/openstack/update/devplatstop/101/). 
./1.0commercial.update-101-overview.md:title: "HP Helion OpenStack&#174; 1.0: Update Overview"
./1.0commercial.update-101-overview.md:permalink: /helion/openstack/update/overview/101/
./1.0commercial.update-101-overview.md:# HP Helion OpenStack&#174; 1.0: Update Overview
./1.0commercial.update-101-overview.md:An HP Helion OpenStack patch update is a series of images and scripts that enhance functionality or fix issues found in a  previous Helion release.  
./1.0commercial.update-101-overview.md:The update process uses an image-based update technique, where the image that gives a particular node its features is replaced with a new image. This includes the operating system and the services that make a node type unique.  
./1.0commercial.update-101-overview.md:During the process of updating, the node will be unavailable until it is updated with the new image.  When the node comes back up, the node will be using the new image and all the new services/functionality and fixes included in that updated image.  
./1.0commercial.update-101-overview.md:There are unique images for all the node types in the cloud.  Only the images that need updating will be delivered as part of a patch update.  
./1.0commercial.update-101-overview.md:There are multiple ways to update your cloud. This HP Helion OpenStack documentation will describe the recommended path and provide alternatives for some nodes in advanced situations.  Unless you are an advanced user it is strongly suggested that you follow the recommended path.  To avoid errors please read the documentation in its entirety prior to starting an update.  
./1.0commercial.update-101-overview.md:* [Overview of the Update Process](#overview)
./1.0commercial.update-101-overview.md:* [About the Update Options](#options)
./1.0commercial.update-101-overview.md:* [Update Troubleshooting](#trouble)
./1.0commercial.update-101-overview.md:The HP Helion OpenStack 1.01 update with apply to all of the nodes: seed, undercloud, overcloud controllers, overcloud Swift, overcloud VSA, and overcloud compute nodes. There is also an update for the optional HP Helion OpenStack DNSaaS. 
./1.0commercial.update-101-overview.md:## Overview of the Update Process {#overview}
./1.0commercial.update-101-overview.md:This section describes what you need to know at a high level to perform an update. 
./1.0commercial.update-101-overview.md:1. The first step of an [update is to obtain the patch update package](/helion/openstack/update/download/101/) using the Helion Horizon HDN client or the Helion catalog: 
./1.0commercial.update-101-overview.md:	- In general, patch updates can be downloaded to the undercloud using the Sherpa UI, which is the Helion Horizon HDN client. The client can be found on the Updates and Extensions tab in the Helion Dashboard on the overcloud. 
./1.0commercial.update-101-overview.md:	- For clouds that do not have Internet access, use the Helion catalog to download the update and then copy the update to the undercloud using the Sherpa import feature.  
./1.0commercial.update-101-overview.md:2. Prior to performing an update you must know your cloud infrastructure.  This is vital so that you can choose a path that helps meet your system needs.  Important things to know about your cloud infrastructure are:
./1.0commercial.update-101-overview.md:	* Which additional services and software you have installed, such as the HP Development Platform for Helion.  If you know the recommended update plans for these services, that can help you plan your overall Helion update.
./1.0commercial.update-101-overview.md:	* Any special layouts such as availability zones may affect your decision on the sequence of nodes you wish to update.  There are additional details of this in the [Compute (n-scale, not ESX)](/helion/openstack/update/overcloud/101/#compute) section of *Updating the Overcloud*. 
./1.0commercial.update-101-overview.md:3. Each patch update package will be unique, containing only the images of the node types that need to be updated.  
./1.0commercial.update-101-overview.md:	* The update package might contain images for the overcloud node, the undercloud node, or in rare cases, the seed node (delivered in a separate package). It will also, where necessary, have a set of update scripts.  
./1.0commercial.update-101-overview.md:	* So it is important to read the directions for the update that you plan to use. Instructions from previous updates will not work.  
./1.0commercial.update-101-overview.md:		In general, you should upgrade the seed VM first, and then proceed to the undercloud and the overcloud. The overcloud nodes will have suggested ordering and that is listed in the overcloud section of the update download document.  
./1.0commercial.update-101-overview.md:4. Once you know your infrastructure and the type of nodes that will be updated as a result of this update, prepare a plan for when to update so that your users can be made aware of any potential downtime or service interruption. 
./1.0commercial.update-101-overview.md:	It is important to note that it is possible for a full cloud update to happen over a series of planned maintenance cycles. Nodes are typically updatable node-by-node, but depending on the nature of the specific patch, this may not be possible. If a certain set of nodes must be updated at the same time to ensure continued functionality, this will be called out.  
./1.0commercial.update-101-overview.md:	**NOTE:** For node types that support backup and restore, it is HIGHLY recommended that you back up these nodes prior to performing any update steps.  Each section seed, undercloud, and specific overcloud will make you aware of the ability to backup and restore.  
./1.0commercial.update-101-overview.md:	Throughout the update process you may be asked for particular IP addresses or image IDs to perform a particular manual step.  Steps on how to obtain these are listed in the [Update Prerequisites](/helion/openstack/update/prereqs/101/).
./1.0commercial.update-101-overview.md:5. After the update is complete:
./1.0commercial.update-101-overview.md:## About the Update Options ## {#options}
./1.0commercial.update-101-overview.md:There are two ways to update once you have laid out your update plan: 
./1.0commercial.update-101-overview.md:* **Helper script.** There is the guided node-by-node order determined by the helper script. The helper script method is the easiest and least error-prone. However, it will limit the user to a certain node order and this may not fit with your update plan.  
./1.0commercial.update-101-overview.md:* **Manual method.** Performing the update manually gives you more control to make some modifications to the update sequence (usually not recommended, except for compute nodes).  
./1.0commercial.update-101-overview.md:Inside the update documentation, you will see sections clearly marked as helper script method or manual method to help you distinguish which technique you are using.
./1.0commercial.update-101-overview.md: Manual verification steps are provided for each node to be updated. It is recommended that you follow these steps to validate that a node was updated successfully. If you have additional services/software or needs, you can add to the manual verification steps that are performed after each node update. 
./1.0commercial.update-101-overview.md:**Note:** The seed update is different in that it has helper script components only. 
./1.0commercial.update-101-overview.md:## Update Troubleshooting {#trouble}
./1.0commercial.update-101-overview.md:If the update fails or the verification steps do not show the expected results, then recovery steps might be listed.  With each patch there will be a `troubleshooting.rst` file delivered in the `/opt/stack/tripleo-ansible` directory that will have potential issues and workarounds.  Always use the new `troubleshooting.rst` delivered with the update. If you cannot recover a node using the troubleshooting steps, use the Backup/Restore functionality to get the node back to original state.
./1.0commercial.update-101-overview.md:If you have problems during the patch update, refer to the [Update Troubleshooting](/helion/openstack/update/troubleshooting/101/) for a list of known issues and possible solutions.
./1.0commercial.update-101-overview.md:Download the software package that contains the patch update software, README file, and other information.
./1.0commercial.update-101-overview.md:For information, see [Obtaining the Patch Update Package](/helion/openstack/update/download/101/).
./1.0commercial.update-101-prereqs.md:title: "HP Helion OpenStack&#174; 1.0: Update Prerequisites"
./1.0commercial.update-101-prereqs.md:permalink: /helion/openstack/update/prereqs/101/
./1.0commercial.update-101-prereqs.md:# HP Helion OpenStack&#174; 1.0: Update Prerequisites
./1.0commercial.update-101-prereqs.md:Prior to getting a system ready for update and performing the update, there might be a few tasks that you need to complete that could affect the update outcome.  Read this document for information about:
./1.0commercial.update-101-prereqs.md:* [Gathering information needed for update](#info)
./1.0commercial.update-101-prereqs.md:1.	If you have changed your admin password for undercloud you need to make sure you update the files below, if you have not already done so. 
./1.0commercial.update-101-prereqs.md:	If you do not update the password in configuration files, the update will fail.  
./1.0commercial.update-101-prereqs.md:	b. Edit file environment variables JSON file. In the `/root/tripleo/ce_env.json` file, update the undercloud `"password":` line with the new password. for example:
./1.0commercial.update-101-prereqs.md:2.	Prior to starting update you need to verify you are running Ubuntu that is greater than 3.13.0-36 on your seed cloud host machine. 
./1.0commercial.update-101-prereqs.md:	Follow instructions for your Linux distribution to update.
./1.0commercial.update-101-prereqs.md:As mentioned in the [Update Overview](/helion/openstack/update/overview/101/), a HP Helion OpenStack patch update is composed of multiple major components.  
./1.0commercial.update-101-prereqs.md:The Overcloud node images can be one of the components. If you have [downloaded the patch update using Sherpa](/helion/openstack/update/download/101/), at this stage the overcloud node images are located in the undercloud image repository. 
./1.0commercial.update-101-prereqs.md:The other files come as TAR files and are delivered to the undercloud local filesystem, by default, the `/tmp/heat_templates` folder.  It is possible to change the location of the undercloud patch update tarballs, during or after deployment. See [Change the default](#default) below. 
./1.0commercial.update-101-prereqs.md:It is possible to change the location of the undercloud patch update TAR files, during or after deployment. The default location is the `/tmp/heat_templates` folder.   
./1.0commercial.update-101-prereqs.md:When locating the update files, use the directory set in `/etc/sherpa/sherpa.conf`.
./1.0commercial.update-101-prereqs.md:## Gather information needed for update {#info}
./1.0commercial.update-101-prereqs.md:To perform a node-by-node or group-by-group update, you will need the IPs of the nodes you are updating and the image IDs of the new images to place on the nodes.  
./1.0commercial.update-101-prereqs.md:During the update, you might be asked for various IPs and image ID's, when you are refer to this section or save off the information and use it where required.
./1.0commercial.update-101-prereqs.md:	**Note:** This command is described in [Updating the Undercloud](/helion/openstack/update/undercloud/101/). 
./1.0commercial.update-101-prereqs.md:	**Note:** This command is described in [Updating the Undercloud](/helion/openstack/update/undercloud/101/).
./1.0commercial.update-101-prereqs.md:	<img src="media/Update_UC_Image_ID.png">
./1.0commercial.update-101-prereqs.md:	**Note:** This command is described in [Updating the Overcloud](/helion/openstack/update/overcloud/101/).
./1.0commercial.update-101-prereqs.md:	<img src="media/Update_OC_IP.png">
./1.0commercial.update-101-prereqs.md:	<img src="media/Update_OC_Image_ID.png">
./1.0commercial.update-101-prereqs.md:Update the seed VM.
./1.0commercial.update-101-prereqs.md:For information, see [Updating the Seed VM](/helion/openstack/update/seed/101/).
./1.0commercial.update-101-seed.md:permalink: /helion/openstack/update/seed/101/
./1.0commercial.update-101-seed.md:The seed update instructions are different from the instructions for the undercloud and overcloud updates.  Not all updates will include seed updates, please check the [HP Helion OpenStack Release Notes](/helion/openstack/release-notes/101/) to determine if there is a seed update.
./1.0commercial.update-101-seed.md:For HP Helion OpenStack 1.01, you do not need to update the seed. However, you should perform the following steps to back up the seed VM and  download and extract the HP Helion OpenStack patch update. 
./1.0commercial.update-101-seed.md:Updating the seed node involves saving current environment and configuration settings and restoring them after the update.
./1.0commercial.update-101-seed.md:* [Update the seed](#updateseed)
./1.0commercial.update-101-seed.md:* [Download and extract the patch update](#extractpatch)
./1.0commercial.update-101-seed.md:* [Verify the update](#verify)
./1.0commercial.update-101-seed.md:Before you proceed with seed node update, you should extract the updated scripts that is delivered as part of the patch.  
./1.0commercial.update-101-seed.md:As mentioned in the [Update Overview](/helion/openstack/update/overview/101/), a HP Helion OpenStack patch update is composed of multiple major components.  
./1.0commercial.update-101-seed.md:The Overcloud node images can be one of the components. If you have [downloaded the patch update using Sherpa](/helion/openstack/update/download/101/), at this stage the overcloud node images are located in the undercloud image repository. 
./1.0commercial.update-101-seed.md:The other files come as TAR files and are delivered to the undercloud local filesystem, by default, the `/tmp/heat_templates` folder.  It is possible to change the location of the undercloud patch update tarballs, during or after deployment. See [Change the default](#default) below. 
./1.0commercial.update-101-seed.md:		scp /tmp/heat_templates/seed_update_1.0.0-1.01.tar <username>@<KVMHOST_IP>:/tmp/
./1.0commercial.update-101-seed.md:		tar xvf seed_update_1.0.0-1.01.tar
./1.0commercial.update-101-seed.md:It is possible to change the location of the undercloud patch update TAR files, during or after deployment. The default location is the `/tmp/heat_templates` folder.   
./1.0commercial.update-101-seed.md:When locating the update files, use the directory set in `/etc/sherpa/sherpa.conf`.
./1.0commercial.update-101-seed.md:###Update the seed (#updateseed)
./1.0commercial.update-101-seed.md:* Execute the `seed_update.sh` script to backup and copy the seed settings to host system:
./1.0commercial.update-101-seed.md:		./seed_update.sh --backup-seed <IP Address of Seed> <Backup Destination Folder>
./1.0commercial.update-101-seed.md:## Download and extract the patch update ## {#extractpatch}
./1.0commercial.update-101-seed.md:For HP Helion OpenStack 1.01, you do not need to update the seed. 
./1.0commercial.update-101-seed.md:1. [Download and extract the HP Helion OpenStack patch update](/helion/openstack/update/download/101/), if you have not done so already.  
./1.0commercial.update-101-seed.md:2. Update the seed node as described in the [installation instructions](/helion/openstack/install/overview/) providing details for pre-requisites as used during  the initial installation.
./1.0commercial.update-101-seed.md:On the host node where the scripts were extracted, execute the script to restore seed settings on the updated seed.
./1.0commercial.update-101-seed.md:	./seed_update.sh -??-restore-seed <Ip Address of Seed> <Backup Source Folder>
./1.0commercial.update-101-seed.md:## Verify the update ## {#verify}
./1.0commercial.update-101-seed.md:Update the undercloud. For installation instructions, see [Updating the Undercloud](/helion/openstack/update/undercloud/101/).
./1.0commercial.update-101-troubleshooting.md:title: "HP Helion OpenStack&#174; 1.0: Update Troubleshooting"
./1.0commercial.update-101-troubleshooting.md:permalink: /helion/openstack/update/troubleshooting/101/
./1.0commercial.update-101-troubleshooting.md:# HP Helion OpenStack&#174; 1.0: Update Troubleshooting
./1.0commercial.update-101-troubleshooting.md:* [MySQL fails to start upon retrying update](#mysqlfails)
./1.0commercial.update-101-troubleshooting.md:* [A lock problem occurred in Ironic during update of the undercloud using the Ansible script](#ansible)
./1.0commercial.update-101-troubleshooting.md:If the post-rebuild restart fails, it is possible that the MySQL CLI configuration file is missing. <!-- ANSUPDATE-116 -->
./1.0commercial.update-101-troubleshooting.md:2. If the file is empty, run the following command to retrieve the current metadata and update the config files on disk:
./1.0commercial.update-101-troubleshooting.md:## MySQL fails to start upon retrying update ## {#mysqlfails}
./1.0commercial.update-101-troubleshooting.md:If the update was aborted or failed during the update sequence before a single MySQL controller was operational, MySQL will fail to start.
./1.0commercial.update-101-troubleshooting.md:* Update is being re-attempted.
./1.0commercial.update-101-troubleshooting.md:* Update automatically aborts.
./1.0commercial.update-101-troubleshooting.md:Helion OpenStack is configured to store all of its state in a multi-node synchronous replication Percona XtraDB Cluster database, which uses Galera for replication. This database must be in sync and have the full complement of servers before updates can be performed safely.
./1.0commercial.update-101-troubleshooting.md:* Update fails with errors about Galera and/or MySQL being "Out of Sync".
./1.0commercial.update-101-troubleshooting.md:However, based on sequence, the current node should not be the last node.  As a result the error is thrown and update aborted.
./1.0commercial.update-101-troubleshooting.md:* Update failed with error message:
./1.0commercial.update-101-troubleshooting.md:* Ansible update attempt fails with the following error:
./1.0commercial.update-101-troubleshooting.md:	See [MySQL fails to start upon retrying update](#mysqlfails) to correct this issue.
./1.0commercial.update-101-troubleshooting.md:	The update steps, as root, are::
./1.0commercial.update-101-troubleshooting.md:		apt-get update
./1.0commercial.update-101-troubleshooting.md:	If MySQL fails to start and you have verified that MySQL is not running on any controller nodes, you will need to identify the *last* node that MySQL was stopped on and consult [MySQL fails to start upon retrying update](#mysqlfails) for guidance on restarting the cluster.
./1.0commercial.update-101-troubleshooting.md:This issue can happen during the update of undercloud or overcloud nodes. The update will fail for one or more nodes. <!-- CORE-2082 -->
./1.0commercial.update-101-troubleshooting.md:If the update fails, from undercloud node:
./1.0commercial.update-101-troubleshooting.md:		ironic node-update <id> replace maintenance=False
./1.0commercial.update-101-troubleshooting.md:When performing the upgrade to HP Helion OpenStack 1.0.1 using the Ansible-based helper script, the Ironic service cannot be restarted because of a lock situation in Ironic. The update process fails with no specific error message. 
./1.0commercial.update-101-troubleshooting.md:		mysql --defaults-file=/mnt/state/root/metadata.my.cnf --socket /var/run/mysqld/mysqld.sock ironic -e 'update nodes set reservation=NULL where reservation="hLinux";'
./1.0commercial.update-101-troubleshooting.md:4. Execute the update process again.
./1.0commercial.update-101-troubleshooting.md:## A lock problem occurred in Ironic during update of the undercloud using the Ansible script {#ansible}
./1.0commercial.update-101-troubleshooting.md:When performing the upgrade to HP Helion OpenStack 1.0.1 using the Ansible-based helper script, the Ironic service cannot be restarted because of a lock situation in Ironic. The update process fails with no specific error message. <!-- CORE 2043 -->
./1.0commercial.update-101-troubleshooting.md:		mysql --defaults-file=/mnt/state/root/metadata.my.cnf --socket /var/run/mysqld/mysqld.sock ironic -e 'update nodes set reservation=NULL where reservation="hLinux";'
./1.0commercial.update-101-troubleshooting.md:4. Execute the update process again.
./1.0commercial.update-101-undercloud.md:permalink: /helion/openstack/update/undercloud/101/
./1.0commercial.update-101-undercloud.md:The *Readme.txt* that comes with a patch update will tell you what nodes need to be updated as a result of this patch. It will be located in the directory described in the [Extract the required scripts and libraries](/helion/openstack/update/prereqs/101/#extract).  
./1.0commercial.update-101-undercloud.md:If the Readme.txt does not list undercloud nodes, skip this document and proceed to [Updating the Undercloud](/helion/openstack/update/overcloud/101/). -->
./1.0commercial.update-101-undercloud.md:* [Update the undercloud](#update)
./1.0commercial.update-101-undercloud.md:* [Validate the update](#validate)
./1.0commercial.update-101-undercloud.md:* [Backup the updated undercloud](#backup)
./1.0commercial.update-101-undercloud.md:You can monitor the update process, see [Monitoring the Update](/helion/openstack/update/monitor/101/).
./1.0commercial.update-101-undercloud.md:Before you begin the update:
./1.0commercial.update-101-undercloud.md:* If the seed VM needed updating, perform this update before updating the undercloud, as described in [Updating the Seed Cloud Host](/helion/openstack/update/seed/101/).
./1.0commercial.update-101-undercloud.md:It is possible to change the location of the undercloud patch update TAR files, during or after deployment. The default location is the `/tmp/heat_templates` folder.   
./1.0commercial.update-101-undercloud.md:When locating the update files, use the directory set in `/etc/sherpa/sherpa.conf`.
./1.0commercial.update-101-undercloud.md:* Review the [update prerequisites](/helion/openstack/update/prereqs/101/) and make sure all necessary tasks have been performed. <!-- including [extracting the update scripts](/helion/openstack/update/prereqs/101/#extract)-->.
./1.0commercial.update-101-undercloud.md:	If you are low on space and you updated the seed previously please refer to the [Cleanup section](/helion/openstack/update/seed/101/) in *Updating the Seed Cloud Host* for information on removing post-update files. The backup/restore process should remove these files. You can check to make sure the files have been removed. -->
./1.0commercial.update-101-undercloud.md:* Point the install script to the undercloud. The patch update script is based on the Ansible platform. For the undercloud, because the script is launched from the seed cloud host, you need to point the script to the seed cloud host.
./1.0commercial.update-101-undercloud.md:	To point the script to update the undercloud, use the following steps:
./1.0commercial.update-101-undercloud.md:	The command prompt should change to `(ansible)`. You must use  `(ansible)` session for executing all the update operations manually.
./1.0commercial.update-101-undercloud.md:## Update the undercloud ## {#update}
./1.0commercial.update-101-undercloud.md:There are two methods to update the undercloud: 
./1.0commercial.update-101-undercloud.md:You can monitor the update process, see [Monitoring the Update](/helion/openstack/update/monitor/101/).
./1.0commercial.update-101-undercloud.md:### Update the undercloud using the script ### {#upgradescript}
./1.0commercial.update-101-undercloud.md:To upgrade the undercloud using the `HelionUpdate.sh` script included in the patch update download, use the following steps:
./1.0commercial.update-101-undercloud.md:3. Run the following command to start the update:
./1.0commercial.update-101-undercloud.md:		./update-helpers/HelionUpdate.sh -undercloud
./1.0commercial.update-101-undercloud.md:	This script will setup the environment, allow you to do test `ping` and perform pre-update checking, and also update the node.  
./1.0commercial.update-101-undercloud.md:4. When the update is done, [validate the update](#validate). 
./1.0commercial.update-101-undercloud.md:### Update the undercloud manually ### {#upgrademanual}
./1.0commercial.update-101-undercloud.md:The manual method generally gives you more control of the update. 
./1.0commercial.update-101-undercloud.md:There are three images that you need to place into the seed glance for the Undercloud. These images will be delivered in a file called `undercloud.tar` and are located in the undercloud local filesystem after you [obtain and extract the update package](/helion/openstack/update/download/101/).  
./1.0commercial.update-101-undercloud.md:Please refer to [Extract the required scripts and libraries](/helion/openstack/update/prereqs/101/#extract) in *Update Prerequisites* for instructions on locating the directory the undercloud.tar.  
./1.0commercial.update-101-undercloud.md:To update a node, the patch image needs to be loaded into the seed VM Image Operations service (Glance).  A `qcow2` image used for image update has dependencies on `vmlinuz` and `initrd` images.  
./1.0commercial.update-101-undercloud.md:3. Run the update do the following:
./1.0commercial.update-101-undercloud.md:		glance image-update --name undercloud-old $IMAGE_ID
./1.0commercial.update-101-undercloud.md:		glance image-update --name undercloud-initrd-old $RAMDISK_ID
./1.0commercial.update-101-undercloud.md:		glance image-update --name undercloud-vmlinuz-old $KERNEL_ID
./1.0commercial.update-101-undercloud.md:		ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py -e force_rebuild=True -l <IP of undercloud> -e undercloud_rebuild_image_id=<glance Image_ID of undercloud> playbooks/update_cloud.yml
./1.0commercial.update-101-undercloud.md:7. When the update is done, [validate the update](#validate). 
./1.0commercial.update-101-undercloud.md:## Validate the update ### {#validate}
./1.0commercial.update-101-undercloud.md:### Backup the updated undercloud ### {#backup}
./1.0commercial.update-101-undercloud.md:Once the update of undercloud node is complete, you should backup the node in case of failures.  See [Back Up and Restore](/helion/openstack/backup.restore/).
./1.0commercial.update-101-undercloud.md:For installation instructions, see [Updating the Overcloud](/helion/openstack/update/overcloud/101/).
./1.0commercial.vlan-providernetwork.md:4. To update ml2.conf, edit `tripleo/hp_passthrough/overcloud_neutron_ml2_conf.json` and add tenant&#095;network&#095;type and network&#095;vlan_ranges specific to your environment. An example is given below:
./1.0commerical.flexible-control-pane-installation.md:In order to ensure that the overcloud control plane nodes land on different KVM hosts to maintain HA, modify the baremetal.csv file and update the last column as shown below such that the overcloud controller nodes represented by the 2nd, 3rd and 4th row land on different KVM hosts.
./1.0commerical.flexible-control-pane-installation.md:14.	Source the environment variables file that you updated.
./1.0commerical.flexible-control-pane-installation.md:	 	bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commerical.flexible-control-pane-installation.md:###A few VMs with two interfaces (pvt and svc) lost network plumbing on compute node (post update)
./1.0commerical.services-eve-overview.md:About "jobs" : A job is created when the user submits a desired topology to Eve and requests provisioning based on it. Eve creates the job and publishes status updates for it. Once the provisioning is finished, the job is marked as completed. 
./1.0commerical.services-focus-overview.md:- Update the contents and/or metadata of a document
./1.0commerical.services-orchestration-overview.md:- **Update a stack** &#151; Configure stacks.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:2. [Update the stack with the removed management controller node](#updatedremovemgt)
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:3. [Update the heat stack with a new management controller](#updatednewmgt)
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:### Update the stack with the removed management controller node {#updatedremovemgt}
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:2. Update the heat stack with the removed management controller node.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		heat stack-update -e ~/no-mgmt.env.json \
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:	It takes several minutes to complete the update. Once the update is completed the management controller is removed from the heat configuration.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:###Update the heat stack with a new management controller {#updatednewmgt}
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:2. Update the heat stack to provision a new management controller.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		heat stack-update -e ~/with-mgmt.env.json \
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:	It takes several minutes to complete the update. When the overcloud-ce-controller stack status reaches UPDATE_COMPLETE, the stack is ready for use, with the replaced management controller node.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:5. Generate a template with a removed controller0 and update the stack.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller stack-update -e /root/tripleo/overcloud-env.json -t 360 -f /root/tripleo/tripleo-heat-templates/overcloud-ce.yaml -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:6. Verify the update completion.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:	When the `overcloud-ce-controller` stack status reaches `UPDATE_COMPLETE`, the stack is ready for use, with the replaced management controller node.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:1. Execute the following command to increase the number of controllers and run a stack-update.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		bash tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:	When the stack reaches UPDATE_COMPLETE status, the failed node is replaced. 
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:2. After stack update is completed, execute the following command on controller0 and management controller nodes. 
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:6. Generate a template with the removed controller0 and update the heat stack.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z /root/tripleo/tripleo-heat-templates/trickle/overcloud-ce-controller stack-update -e /root/tripleo/overcloud-env.json -t 360 -f /root/tripleo/tripleo-heat-templates/overcloud-ce.yaml -P "ExtraConfig=${OVERCLOUD_EXTRA_CONFIG}" overcloud-ce-controller
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:When the stack reaches UPDATE_COMPLETE status, the replacement of the failed node is complete.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:1. Set the number of controllers to two and update the stack.
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:		tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:	When the stack reaches UPDATE_COMPLETE status, the replacement of the failed node is complete. 
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:2. Once stack update is completed, execute the following command on controller1 and management controller nodes:
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:###Controller Nodes : Heat stack updates 
./1.0commerical.services-remove-replace-failed-overcloud-nodes.md:A heat stack-update takes longer time, that is, 30 minutes or longer. 
./1.0commerical.services-sherpa-accessing-ui.md:2.	Navigate to **Admin -> Updates and Extensions -> Updates and Extensions**. 
./1.0commerical.services-sherpa-overview.md:Content may include patches, plugins, images, models, and so forth that may be used to update, improve, extend, or provide content for the cloud.
./1.0commerical.services-sherpa-overview.md:You can use the [HP Helion OpenStack Dashboard](/helion/openstack/undercloud/admin/updates-and-extension/) to work with the Sherpa service.
./1.0commerical.services-sherpa-overview.md:The Dashboard presents different options depending on the installation, location within the cloud, and the role of the Sherpa user currently logged in. Cloud Architects or Administrators, for example, can use the dashboard to update their clouds.
./1.0commerical.services-swift-deployment-add-proxy-node.md:##Update load balancer with new Proxy nodes
./1.0commerical.services-swift-deployment-provision-swift-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commerical.services-swift-deployment-provision-swift-node.md:4.Run the installer script to update the cloud.
./1.0commerical.services-swift-deployment-provision-swift-node.md:	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-overcloud |& tee update_cloud.log
./1.0commerical.services-swift-deployment-shrink-remove-proxy-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commerical.services-swift-deployment-shrink-remove-proxy-node.md:When you update the cloud the node will be skipped in all the operations.
./1.0commerical.services-swift-deployment-shrink-remove-proxy-node.md:2.	On the seed VM, update the `/root/tripleo/configs/kvm-custom-ips.json` file to reflect new scale number of swift scale-out proxy node. 
./1.0commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.0commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:When you update the cloud the node will be skipped in all the operations.
./1.0commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:2. On seed VM, update the `kvm-custom-ips.json` file to reflect number of scale-out object nodes remaining.
./1.0commerical.services-swift-deployment.md:8. [Update the Storage Policy](#update-storage-scaleout-swift) 
./1.0commerical.services-swift-deployment.md:	<td>This should be set to however long a full replication/update cycle takes. No partition is moved twice during the specified amount of time.</td>
./1.0commerical.services-swift-deployment.md:2. Update the `so_swift_storage_scale` parameter in the environment variables file, used during the initial installation, according to your storage needs.	
./1.0commerical.services-swift-deployment.md:	2. Update the `so_swift_storage_scale` parameter in  `/root/configs/kvm-custom-ips.json ` file according to your storage needs.-->
./1.0commerical.services-swift-deployment.md:4.Run the installer script to update the cloud.
./1.0commerical.services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.0commerical.services-swift-deployment.md:##Update the Storage Policy {#update-storage-scaleout-swift}
./1.0commerical.services-swift-deployment.md:4. Run the installer script to update the storage policies across the cloud.
./1.0commerical.services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.0commerical.services-tripleo-overview.md:- **undercloud** - The undercloud server is a basic single-node OpenStack installation running on a single physical server used to deploy, test, manage, and update the overcloud servers. There is no HA configuration for the undercloud. It contains a strictly limited sub-set of OpenStack services; just enough to  permit interaction with the overcloud. 
./1.0commerical.services-volume-overview.md:- **Update the metadata of a volume** &#151; Modify the metadata associated with a volume.
./1.1commercial.backup-restore-GA.md:You should create a backup of the overcloud servers when any Update and Extension is download to the system.
./1.1commercial.backup-restore-GA.md:If the admin user password was changed from the original password created during the installation process, you need to update the password in some files before performing the undercloud backup or restore process. If this process has been done and the files contain the correct password, you do not need to edit the files.
./1.1commercial.backup-restore-GA.md:4. Update the `UNDERCLOUD_ADMIN_PASSWORD=` line with the new password and save the file.
./1.1commercial.backup-restore-GA.md:6. Update the `undercloud` line with the new password and save the file.
./1.1commercial.backup-restore-GA.md:9. Update the `OS_PASSWORD=` line with the new password and save the file.
./1.1commercial.create-deploy-certified-microsoft-windows-guest-image.md:You can add useful metadata information to the image, such as operating system information or hardware details by using the glance image-update command, for example:
./1.1commercial.create-deploy-certified-microsoft-windows-guest-image.md:		glance image-update WS2012 --property os_type=windows --property hw_disk_bus=ide --property hw_vif_model=rtl8139
./1.1commercial.disaster-recovery.md:* Data in the database was not updated at all since Compute could not have anticipated the crash.
./1.1commercial.disaster-recovery.md:2.	Update the database
./1.1commercial.disaster-recovery.md:	Update the database to clean the stalled state. Use these queries to clean up the database for each volume:
./1.1commercial.disaster-recovery.md:		mysql> update volumes set mountpoint=NULL;
./1.1commercial.disaster-recovery.md:		mysql> update volumes set status="available" where status <>"error_deleting";
./1.1commercial.disaster-recovery.md:		mysql> update volumes set attach_status="detached";
./1.1commercial.disaster-recovery.md:		mysql> update volumes set instance_id=0;
./1.1commercial.disaster-recovery.md:2.	Updates the MySQL database.
./1.1commercial.disaster-recovery.md:		              updated_at: 2012-07-03 00:35:11
./1.1commercial.disaster-recovery.md:		mysql> UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';
./1.1commercial.disaster-recovery.md:2.	If you are using a hypervisor that relies on libvirt (such as KVM), it is a good idea to update the `libvirt.xml` file (found in `/var/lib/nova/instances/[instance ID]`). <br />The important changes to make are:
./1.1commercial.disaster-recovery.md:	* Update the VNC IP, if it isn't already updated, to: `0.0.0.0`.
./1.1commercial.disaster-recovery.md:The above database update and `nova reboot` command are typically all that are required to recover a VM from a failed host. However, if further problems occur, consider looking at recreating the network filter configuration using `virsh`, restarting the Compute services or updating the `vm_state` and `power_state` in the Compute database.
./1.1commercial.disaster-recovery.md:	* Execute the command `cat /mnt/state/var/lib/mysql/grastate.dat` in all controllers to find which one has the most updated database (seqno).
./1.1commercial.disaster-recovery.md:	The order that the overcloud controllers are stopped is really important, because based on that we will know the most updated database. 
./1.1commercial.eula.md:- You may not download and use patches, enhancements, bug fixes, or similar updates unless you have a license to the underlying Software. However, such license doesn't automatically give you a right to receive such updates and HP reserves the right to make such updates only available to customers with support contracts.
./1.1commercial.eula.md:**l. Updates and Supplements:**  We may update or supplement the Software. If so, you may use that update or supplement with the Software, subject to any additional terms that accompany the update or supplement.
./1.1commercial.faq.md:* [How can I update to the latest version?](#softwareupdate)
./1.1commercial.faq.md:The undercloud server is a basic single-node OpenStack installation running on a single physical server used to deploy, test, manage, and update the overcloud servers. There is no HA configuration for the undercloud. 
./1.1commercial.faq.md:The undercloud contains a strictly limited sub-set of services; just enough to interact with the overcloud. The services running on the undercloud are Nova, Neutron, Glance, Keystone, Ironic, Heat, Horizon & Ceilometer. This server also contains the HP Helion content distribution catalog service, which provides a mechanism for downloading and installing content and updates for the overcloud.
./1.1commercial.faq.md:###How can I update to the latest version? {#softwareupdate}
./1.1commercial.faq.md:HP OpenStack Helion does not support an update utility to update from previous releases. For help with updating, visit the [HP Customer Support](http://www.hpcloud.com/about/contact) page and choose your preferred method of contact.
./1.1commercial.faq.md:All of the [HP Helion OpenStack services](/helion/openstack/1.1/services/overview/#OpenStack) have been updated to [OpenStack Juno](http://www.openstack.org/software/juno/). [Learn more](http://www.hpcloud.com/learning-center) about HP's commitment to OpenStack and how we're making it even better.
./1.1commercial.glossary.md:A catalog-based HP Helion OpenStack service that allows off-the-shelf content (like workloads, images, and patches) to be imported into your deployed cloud. To access HDN, see the Updates & Extensions panel in the  HP Helion OpenStack user interface. You can import content from the HDN portal or from a local folder.
./1.1commercial.helion-updates.md:title: "HP Helion OpenStack&#174; 1.1: Update Procedure"
./1.1commercial.helion-updates.md:permalink: /helion/openstack/1.1/updates/
./1.1commercial.helion-updates.md:# HP Helion OpenStack&#174; 1.1: Update Procedure
./1.1commercial.helion-updates.md:Welcome to the Helion OpenStack 1.1 update instructions. These instructions apply to existing Helion OpenStack 1.1 installations and describe how you can update your Helion OpenStack cloud environment from 1.1 to 1.1.1. 
./1.1commercial.helion-updates.md:If you are running a version of Helion OpenStack prior to the 1.1 release, the only suppported update procedure is from Helion 1.0 to 1.01. The process of updating a Helion OpenStack 1.0 or 1.01 release to 1.1 is not supported.  
./1.1commercial.helion-updates.md:**NOTE:** This Helion update procedure is not generally available. If your Helion cloud installation needs to be updated using this procedure, then the necessary files will be provided to you. If you have any questions about Helion updates, contact your HP customer support representative. 
./1.1commercial.helion-updates.md:1. [Update the seed VM](#seed-update-process)
./1.1commercial.helion-updates.md:2. [Update the undercloud](#uc-update-process)
./1.1commercial.helion-updates.md:2. [Update the overcloud](#oc-update-process)
./1.1commercial.helion-updates.md:**NOTE:** Nodes to be updated must be in a "good" state before running the update.
./1.1commercial.helion-updates.md:## Update prerequisites ##
./1.1commercial.helion-updates.md:This section explains issues to consider before you start an update.
./1.1commercial.helion-updates.md:The update process does not automatically overwrite your Heat template. If you have modified your Orchestration (Heat) template, then you will need to update the new templates that are provided with the updater with these modifications. 
./1.1commercial.helion-updates.md:If you have not made any changes to the original  Heat template, then to update your Heat templates, run:
./1.1commercial.helion-updates.md:	`./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-undercloud` 
./1.1commercial.helion-updates.md:	`./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud` 
./1.1commercial.helion-updates.md:Check the version of the Glance image that was used to deploy the node before the update. To check this Glance image version:
./1.1commercial.helion-updates.md:Make sure the following commands are working before you start the update:
./1.1commercial.helion-updates.md:##Helion 1.1 to 1.1.X seed VM update process {#seed-update-process}
./1.1commercial.helion-updates.md:The procedure will guide you through updating the seed VM using `update_sd.sh`.
./1.1commercial.helion-updates.md:To prepare the seed VM for update, verify that the following files exist in the seed node.
./1.1commercial.helion-updates.md:To update the seed VM:
./1.1commercial.helion-updates.md:1. Remove any update folders and content from previous update attempts in the `/root` folder using the `rm` command.
./1.1commercial.helion-updates.md:1. Download the update tarball on the seed VM by entering:
./1.1commercial.helion-updates.md:		mkdir helion-update-1.1-to-1.X
./1.1commercial.helion-updates.md:		cd helion-update-1.1-to-1.X
./1.1commercial.helion-updates.md:-     /root/helion-update* 
./1.1commercial.helion-updates.md:    du -hcs /root/tripleo /mnt/state /root/helion-update* /tftpboot
./1.1commercial.helion-updates.md:1. Log into the seed VM as root and create an update directory by entering:
./1.1commercial.helion-updates.md:    	mkdir helion-update-1.1-to-1.X
./1.1commercial.helion-updates.md:1. Copy the seed update script from the downloaded tarball on the seed to the seed VM by entering:
./1.1commercial.helion-updates.md:    	scp root@<seed-ip>:/root/helion-update-1.1-to-1.X/tripleo/helion-update/seed_update/update_sd.sh .
./1.1commercial.helion-updates.md:1. Run the seed update script `update_sd.sh` from the seed VM. Make sure you run `update_sd.sh` from the update directory you created: `/root/helion-update-1.1-to-1.X)` using the seed VM's  `tripleo` root directory (`/root/tripleo`) and remote update root(extracted updated tarball directory on the seed) as arguments.
./1.1commercial.helion-updates.md:    remote_update_tripleo_root_directory = /root/helion-update-1.1-to-1.X
./1.1commercial.helion-updates.md:**NOTE:** If you use any custom variables for the initial installation of the seed, these will need to be exported again prior to executing the    `update_sd.sh` script.
./1.1commercial.helion-updates.md:1. To run the update script, enter:
./1.1commercial.helion-updates.md:    	./update_sd.sh <host_tripleo_root_directory> <remote_update_tripleo_root_directory> | tee seed_update.log
./1.1commercial.helion-updates.md:	Once the update is completed, the `ssh host_key` of the seed will have changed. 
./1.1commercial.helion-updates.md:	You will be prompted to re-add the seed VM to your known_hosts file when you SSH into the seed to update the undercloud and overcloud.
./1.1commercial.helion-updates.md:##Helion 1.1 to 1.1.X undercloud update process {#uc-update-process}
./1.1commercial.helion-updates.md:The procedure will guide you through updating the undercloud by running the `update_uc.sh` shell script.
./1.1commercial.helion-updates.md:To update your Helion OpenStack undercloud from 1.1 to a higher release:
./1.1commercial.helion-updates.md:2.	Download the update tarball onto the seed VM.
./1.1commercial.helion-updates.md:	    mkdir helion-update-1.1-to-<version>
./1.1commercial.helion-updates.md:	    cd helion-update-1.1-to-<version>
./1.1commercial.helion-updates.md:3.	Extract the tarball and run the update script.
./1.1commercial.helion-updates.md:	    cd tripleo/helion-update/undercloud_update
./1.1commercial.helion-updates.md:	    ./update_uc.sh ~/helion-update-1.1-to-<version>/tripleo
./1.1commercial.helion-updates.md:Your Helion OpenStack undercloud is now updated. Confirm that update was successful by examining the ` /var/log/ansible/ansible.log`.  You should see:
./1.1commercial.helion-updates.md:##Helion 1.1 to 1.1.X overcloud update process {#oc-update-process}
./1.1commercial.helion-updates.md:Updating the overcloud using `update_oc.sh`
./1.1commercial.helion-updates.md:2. Download the update tarball on the seed
./1.1commercial.helion-updates.md:    	mkdir helion-update-1.1-to-<version>
./1.1commercial.helion-updates.md:    	cd helion-update-1.1-to-<version>
./1.1commercial.helion-updates.md:3. Extract the tarball and run the update script
./1.1commercial.helion-updates.md:	    cd tripleo/helion-update/overcloud_update
./1.1commercial.helion-updates.md:	    ./update_oc.sh ~/helion-update-1.1-to-<version>/tripleo
./1.1commercial.helion-updates.md:Your Helion OpenStack overcloud is now updated. Confirm that update was successful by examining the ` /var/log/ansible/ansible.log`.  You should see:
./1.1commercial.helion-updates.md:This procedure allows you to update your overcloud installation from Helion OpenStack 1.1 to a higher release by executing the update commands directly and not relying on the update scripts.
./1.1commercial.helion-updates.md:To update your overcloud:
./1.1commercial.helion-updates.md:2. Download the update tarball on the seed
./1.1commercial.helion-updates.md:	 	mkdir helion-update-1.1-to-1.1.X
./1.1commercial.helion-updates.md:		cd helion-update-1.1-to-1.1.X
./1.1commercial.helion-updates.md:	    cd helion-update-1.1-to-1.1.X
./1.1commercial.helion-updates.md:	    BUILD_NO=$(cat /root/helion-update-1.1-to-1.1.1/tripleo/ce_env.json  | grep build_number | awk '{print $2}')
./1.1commercial.helion-updates.md:	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
./1.1commercial.helion-updates.md:	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-compute-$BUILD_NO.qcow2
./1.1commercial.helion-updates.md:	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
./1.1commercial.helion-updates.md:	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-control-$BUILD_NO.qcow2
./1.1commercial.helion-updates.md:	    /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/load-image \
./1.1commercial.helion-updates.md:	      -d /root/helion-update-1.1-to-1.1.X/tripleo/images/overcloud-swift-$BUILD_NO.qcow2
./1.1commercial.helion-updates.md:	      --disk-format aki < /root/helion-update-1.1-to-1.1.X/tripleo/images/deploy-ramdisk-ironic.kernel
./1.1commercial.helion-updates.md:	      --disk-format ari < /root/helion-update-1.1-to-1.1.X/tripleo/images/deploy-ramdisk-ironic.initramfs
./1.1commercial.helion-updates.md:9. Update image names and set build metadata.
./1.1commercial.helion-updates.md:	    cd /root/helion-update-1.1-to-1.1.X/tripleo/tripleo-incubator/scripts/
./1.1commercial.helion-updates.md:10. Update the triple Ansible playbook.
./1.1commercial.helion-updates.md:	    cp -r /root/helion-update-1.1-to-1.1.X/tripleo/helion-update/tripleo-ansible/ /opt/stack/
./1.1commercial.helion-updates.md:Refer to the Ansible update README for more details on running the play.
./1.1commercial.helion-updates.md:    ansible-playbook -vvvv -u heat-admin -i plugins/inventory/heat.py playbooks/update_cloud.yml
./1.1commercial.helion-updates.md:# Validating the update #
./1.1commercial.helion-updates.md:This section explains how you should validate your update.
./1.1commercial.helion-updates.md:## Validating the overcloud controller update ##
./1.1commercial.helion-updates.md:After the update, the overcloud controller should be back online without any errors. You should be able to  SSH to the Nova compute node from the seed VM.   
./1.1commercial.helion-updates.md:instances deployed before the update should still be up and accessible. It should be possible to deploy new instances and ping them and SSH to them.     
./1.1commercial.helion-updates.md:Icinga should show that all the monitors are green for the update node. To check Icinga, go to:
./1.1commercial.helion-updates.md:Make sure that the following commands are working after the update:  
./1.1commercial.helion-updates.md:Use `ifconfig` to check that all your networks still exist after the update.             
./1.1commercial.helion-updates.md:Use `ovs-vsctl show` to check that all bridges and ports still exist after the update.   
./1.1commercial.helion-updates.md:After an update, verify that instances deployed before  the update are still up and accessible.
./1.1commercial.helion-updates.md:The `os-refresh-config` command needs to be working  after  an update.
./1.1commercial.helion-updates.md:Use `ifconfig` to check that all your networks still exist after the update.             
./1.1commercial.helion-updates.md:Use `ovs-vsctl show` to check that all bridges and ports still exist after the update.   
./1.1commercial.helion-updates.md:After the update:
./1.1commercial.helion-updates.md:The `os-refresh-config` command needs to be working  after  an update.
./1.1commercial.helion-updates.md:After an update:
./1.1commercial.helion-updates.md:- Check that the version of the Glance image used to update is the same by SSHing to the undercloud node and run:
./1.1commercial.helion-updates.md:# Troubleshooting an update  
./1.1commercial.helion-updates.md:This section explains how to fix common problems  that might arise when performing an update to Helion.
./1.1commercial.helion-updates.md:    cd helion-update-1.1-to-1.X/tripleo/helion-update/seed_update
./1.1commercial.helion-updates.md:## Seed update fails noting unable to ping 192.0.2.1 ##
./1.1commercial.helion-updates.md:If your seed Update fails, your deployment will NOT utilize the 192.0.2.0/24 demo IP network range. You will see in your log:
./1.1commercial.helion-updates.md:The likely reason for this failure is the installer update of the seed VM image has failed as the configuration that was used during the install was not available or passed on to the installer to perform the seed VM update operation.  As a result, the installer has indicated that it has failed, although the seed VM has likely rebooted without issue.
./1.1commercial.helion-updates.md:	- Location of the backup folder.  It should be  at `/root/helion-update-1.1-to-1.1.X/backup*`.  For example:` /root/helion-update-1.1-to-1.1.74/backup-0/`
./1.1commercial.helion-updates.md:   		/root/helion-update-1.1-to-1.X/helion-update/seed_update/seed_update.sh --restore-seed <backup directory> --ip-address <seed IP address>
./1.1commercial.helion-updates.md:    	/root/helion-update-1.1-to-1.X/helion-update/seed_update/seed_update.sh --restore-seed /root/helion-update-1.1-to-1.X/backup-0/ --ip-address 192.2.0.1
./1.1commercial.helion-updates.md:- If the file is empty, retrieve current metadata and update  the config files on disk by running:
./1.1commercial.helion-updates.md:## MySQL fails to start after retrying the update ##
./1.1commercial.helion-updates.md:If the update was aborted or failed during the Update sequence before a single MySQL controller was operational, MySQL will fail to start upon retrying. In this case, you will see the following error messages:
./1.1commercial.helion-updates.md:    * Update automatically aborts.
./1.1commercial.helion-updates.md:complement of servers before updates can be performed safely.
./1.1commercial.helion-updates.md:The problem is update fails with errors about Galera and/or MySQL being `Out of Sync`.
./1.1commercial.helion-updates.md:error is thrown and update is aborted.
./1.1commercial.helion-updates.md:  When Ansible loses SSH connectivity causing an update attempt to fail, you will see the following output:
./1.1commercial.helion-updates.md: To fix this problem, you can generally re-run the playbook to complete the upgrade, unless SSH connectivity is lost while all MySQL nodes are down. (See 'MySQL fails to start upon retrying update' to correct this issue.)
./1.1commercial.helion-updates.md:The update steps, as root, are:
./1.1commercial.helion-updates.md:        apt-get update
./1.1commercial.helion-updates.md:The default ephemeral certificate location is `/root`. This works for normal Helion installations. If you have specified another location, when you update Helion, you must specify the certificate location as `/root`. For example, the correct environment variable specification is:
./1.1commercial.helion-updates.md:- If MySQL fails to start, and it has been verified that MySQL is not running on any controller nodes, then you will need to identify the Last node that MySQL was stopped on and consult the section "MySQL fails to start upon retrying update" for guidance on restarting the cluster.
./1.1commercial.helion-updates.md:During an update, nodes must NOT be in maintenance mode or Ironic returns an error message such as the following:
./1.1commercial.helion-updates.md:If the command `ironic node-set-maintenance` fails to properly change nodes from maintenance mode, you must either update the database directly using:
./1.1commercial.helion-updates.md: 	`mysql> update nodes set maintenance=0;`
./1.1commercial.helion-updates.md:	 `ironic node-update <ironic node-id> replace maintenance=False`.
./1.1commercial.index.md:**Note:** HP OpenStack Helion does not support an update utility to update from Helion 1.0 or Helion 1.01 to Helion 1.1. For help with updating from your current release to a newer release, contact [HP Customer Support](http://www.hpcloud.com/about/contact). 
./1.1commercial.install-add-nodes.md:3. Source the environment variables file that you updated:  
./1.1commercial.install-add-nodes.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commercial.install-add-nodes.md:4. Source the environment variables file that you updated:  
./1.1commercial.install-add-nodes.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commercial.install-add-nodes.md:When hp&#95;ced&#95;installer --update-overcloud is run again, the node will be skipped in all operations.
./1.1commercial.install-GA-DNSaaS.md:3. Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.install-GA-DNSaaS.md:	b.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.install-GA-ESX-Proxy.md:	The new clusters will be updated in the nova-compute.conf of the VM
./1.1commercial.install-GA-overview.md:	<td>The Seed cloud is deployed as a VM instance. This image contains minimal OpenStack services required to deploy and update the undercloud on a baremetal server.
./1.1commercial.install-GA-overview.md:	<td>A single-server deployment of a limited set of OpenStack services, called the undercloud, is used to deploy, test, manage, and update all the overcloud servers. 
./1.1commercial.install-GA-ovsvapp.md:* [Update OVSvApp](#update)
./1.1commercial.install-GA-ovsvapp.md:	h. Specify update information.
./1.1commercial.install-GA-ovsvapp.md:		[update]
./1.1commercial.install-GA-ovsvapp.md:		#OVSvAPP update tar file location.
./1.1commercial.install-GA-ovsvapp.md:		update_file_path=
./1.1commercial.install-GA-ovsvapp.md:## Update OVSvApp {#update}
./1.1commercial.install-GA-ovsvapp.md:The update can be run only from the system where OVSvAPP passwordless authentication is enabled. That is, from the system where id_rsa key is present for the public key (id_rsa.pub) that was used during OVSvAPP 1.0 installation.
./1.1commercial.install-GA-ovsvapp.md:To update the OVSvApp from version 1.01 to version 1.1:
./1.1commercial.install-GA-ovsvapp.md:	b. Enter the path to the `ovsvapp-patch-*.tar.gz` file in the `update_file_path` value in the [update] section.
./1.1commercial.install-GA-ovsvapp.md:	c. If you want to be able to roll back the update to the previous version, set the `do_backup` value in the [update] section to `True`.
./1.1commercial.install-GA-ovsvapp.md:	cd /hp-ovsvapp/src/update #python updater.py
./1.1commercial.install-GA-ovsvapp.md:**Note:** If you see the following error while updating the OVSvApp, it means that the system from where update is launched does not have the passwordless authentication enabled for OVSvAPPs. 
./1.1commercial.install-GA-ovsvapp.md: Re-run the update from the system where OVSvAPP passwordless authentication is enabled.
./1.1commercial.install-GA-ovsvapp.sample.md:	[update]
./1.1commercial.install-GA-ovsvapp.sample.md:	#OVSvAPP update tar file location.
./1.1commercial.install-GA-ovsvapp.sample.md:	update_file_path=/path/to/ovsvapp-patch-*.tar.gz
./1.1commercial.install-GA-prereqs.md:- Update to the latest firmware recommended by the system vendor for all system components, including the BIOS, BMC firmware, disk controller firmware, drive firmware, network adapter firmware, and so forth.
./1.1commercial.install-GA-security.md:5. Update required access control lists or firewall rules to allow traffic to the Public VIP IP.
./1.1commercial.install-GA-security.md:<td>4</td><td>Container update over HTTP</td><td>Object Storage</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002</td></tr>
./1.1commercial.install-GA-security.md:<td>5</td><td>Container update over HTTP</td><td>Object Storage</td><td>Swift all in one (PACO)</td><td>6001,6002
./1.1commercial.install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Proxy-Account-Container (PAC)</td><td>Swift all in one (PACO)</td><td>6001,6002</td></tr>
./1.1commercial.install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Swift all in one (PACO)</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002
./1.1commercial.install-GA-supportmatrix.md:All of the [HP Helion OpenStack services](/helion/openstack/1.1/services/overview/#OpenStack) have been updated to [OpenStack Juno](http://www.openstack.org/software/juno/).
./1.1commercial.install-GA-tempest-tests.md:	tempest\.api\.object_storage\.test_container_services\.ContainerTest\.test_update_container_metadata_with_delete_metadata
./1.1commercial.install-GA-tempest-tests.md:	tempest\.api\.object_storage\.test_container_services\.ContainerTest\.test_update_container_metadata_with_delete_metadata_key
./1.1commercial.install-GA-virtual-control.md:ComputeCapabilitiesFilter enabled us to overcome the above limitation. This is one of the approaches that we can follow. In this approach, we provide key:value pairs for every node.and register them in Ironic in the undercloud only. Likewise, the default flavor is update with the same key:value pair that is used for VMs and a different flavor is created for compute using the physical server key:value pair. The steps followed in this approach are:
./1.1commercial.install-GA-virtual-control.md:		undercloud-controller-xyz# ironic node-update $NODE_ID add "properties/capabilities=hw_type:vm,node_type:any"
./1.1commercial.install-GA-vsa.md:3a - **Run update cloud script to provision VSA node**
./1.1commercial.install-GA-vsa.md:   * Update the *overcloud.json* file for StoreVirtual deployment and apply the configuration.
./1.1commercial.install-GA-vsa.md:   * Execute the update cloud script.
./1.1commercial.install-GA-vsa.md:5a - **Update `overcloud-config.json` file with cinder configuration**
./1.1commercial.install-GA-vsa.md:   With the advise generated from the above steps, update the overcloud-config.json file in the seed cloud.
./1.1commercial.install-GA-vsa.md:5b - **Run update cloud script to update cinder.conf**
./1.1commercial.install-GA-vsa.md:   * The cinder.conf in the overcloud should be updated after updating the overcloud-config.json file in the seed cloud.
./1.1commercial.install-GA-vsa.md:   * Execute [update cloud script](/helion/openstack/1.1/undercloud/oc/config/storevirtual/) from seed cloud. -->
./1.1commercial.install-GA-vsa.md:		# apt-get update
./1.1commercial.install-GA-vsa.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commercial.linux.arch.md:	d. Timing: Changes are pulled 1x each week, or needed for bug/security updates
./1.1commercial.linux.arch.md:	d. Timing: Changes are pulled as needed, generally following Debian release schedule ~2x per month, or as needed for bug/security updates
./1.1commercial.networking-maskedIP.md:	    | updated                              | 2014-06-26T01:41:05Z          |
./1.1commercial.release-notes.md:**OpenStack Juno support** All of the [HP Helion OpenStack services](/helion/openstack/1.1/services/overview/#OpenStack) have been updated to [OpenStack Juno](http://www.openstack.org/software/juno/).
./1.1commercial.release-notes.md:* When a significant update operation is performed, such as adding or removing nodes, connectivity to the VM will drop until the operation is completed. This may last for up to 15 minutes.
./1.1commercial.release-notes.md:* If you use the Updates and Extensions tab of the Helion Dashboard for the overcloud (known as the *Sherpa UI*) to download two images that use the same name in two different projects, the second image will fail to upload to the Image Operations service (Glance). Avoid using images that use the same name. <!-- (CODN-24) -->
./1.1commercial.release-notes.md:**Update Issues**
./1.1commercial.release-notes.md:You may experience the following issues when performing updates in this release:
./1.1commercial.release-notes.md:- Depending on configuration, an overcloud update may take longer than was the case in earlier releases. 
./1.1commercial.release-notes.md:<!--ANSUPDATE-276/DOCS-1070 -->
./1.1commercial.release-notes.md:- After an update, the node removal procedure might not work.  This is a known issue.<!-- CORE-2940 / DOCS 1068  -->
./1.1commercial.release-notes.md:- Overcloud updates may fail because you cannot stop VSA VMs. Overcloud updates require that all VMs be stopped before an update can proceed. During an overcloud update, you may find that you are not able to stop the VSA VM (by issuing a `virsh stop` command). This causes the update to stop.   
./1.1commercial.release-notes.md:	Proceed with the update.
./1.1commercial.release-notes.md:<!-- Cinder cannot create volumes after a system update.   A resolution is under development. per DOCS 1182  -->
./1.1commercial.release-notes.md:<!-- Cinder fails to create Bootable Volumes after a system update. It appears that ISCSI authentication is failing on controller0. The workaround is to migrate the `cinder-volume` service to another node, for example, controller1, and to stop the `cinder-backup` service on controller0.
./1.1commercial.release-notes.md:- Cinder might fail after an update. To fix Cinder,  restart the `cinder-volume` service on controller0 by entering: <!-- DOCS-1047 -->
./1.1commercial.release-notes.md:-  <!--DOCS-1032--> Cinder backup and create bootable volume functions fail after an update. A workaround exists if this is an ISCSI error.  To determine if this is an ISCSI error, grep for the following string in in `/var/log/cinder/cinder-volume.log` on controller0:
./1.1commercial.release-notes.md:* A Kernel Panic error occurs on DL 360 G7 systems when a user employs SSH to log in to a virtual machine and attempts to connect to an IP address external to HP Helion OpenStack. If this occurs, update the kernel to the latest version. <!-- (EE-21) -->
./1.1commercial.release-notes.md:* A user can register but cannot update a vCenter through the UI. 
./1.1commercial.release-notes.md:- Heat templates and updating Helion: If you  are using a modified heat template, you should update the new templates that are provided with the installer to reflect what you had modified in your previous deployment. If you have not made any changes to the original template, then you can execute
./1.1commercial.release-notes.md: `./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-undercloud` 
./1.1commercial.release-notes.md: `./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-overcloud` 
./1.1commercial.release-notes.md:to update the HEAT templates.
./1.1commercial.release-notes.md:<!-- above is for patch or update -->
./1.1commercial.services-volume-fibre.md:If block storage is configured to use a Fibre Channel volume driver that supports zone manager, update `cinder.conf` to enable Fibre Channel Zone Manager.
./1.1commercial.services-volume-fibre.md:* Adhere to the JSON format (mentioned in Step 4). Otherwise, it might cause failure of update cloud.
./1.1commercial.services-volume-fibre.md:###Update cloud
./1.1commercial.services-volume-fibre.md:* Run the installer script to update the overcloud.
./1.1commercial.services-volume-fibre.md:		bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh  --update-overcloud |& tee install_update.log
./1.1commercial.services-volume-fibre.md:2.Add and configure  HP StoreServ (3PAR) and update overcloud. See [HP Helion OpenStack&#174; : Working With StoreServ Backends]( /helion/openstack/1.1/undercloud/oc/config/storeserv/) for detailed procedure..
./1.1commercial.services-volume-fibre.md:2. Add and configure  HP StoreServ (3PAR) and update overcloud. See [HP Helion OpenStack&#174; : Working With StoreVirtual Backends]( /helion/openstack/1.1/undercloud/oc/config/storevirtual/) for detailed procedure.
./1.1commercial.sirius-cli-workflow.md:* [Reconfigure and update cloud](#reconfigure-update)
./1.1commercial.sirius-cli-workflow.md:	| updated_at | 2014-08-23 02:49:41.066921           |
./1.1commercial.sirius-cli-workflow.md:	| updated_at   | 2014-08-23 03:00:30.503439           |
./1.1commercial.sirius-cli-workflow.md:##Reconfigure and update cloud {#reconfigure-update}
./1.1commercial.sirius-cli-workflow.md:3. [Update Overcloud configuration JSON](#update-overcloud-json)
./1.1commercial.sirius-cli-workflow.md:5. [Update Overcloud](#update-overcloud) 
./1.1commercial.sirius-cli-workflow.md:The backends configured in the undercloud Sirius database will not be effective until the overcloud Cinder configuration is updated.
./1.1commercial.sirius-cli-workflow.md:### Update Overcloud configuration JSON {#update-overcloud-json}
./1.1commercial.sirius-cli-workflow.md:Update the `/root/overcloud-config.json` in the Seed node with the generated backend data. Add the StoreVirtual backend configuration as a JSON key-pair with key **vsa** and StoreServ backend configuration with key **3par** to the existing JSON.
./1.1commercial.sirius-cli-workflow.md:A sample of the file is before and after the update is give below:
./1.1commercial.sirius-cli-workflow.md:**Before the update:**
./1.1commercial.sirius-cli-workflow.md:**After the update:**
./1.1commercial.sirius-cli-workflow.md:###Update Overcloud {#update-overcloud}
./1.1commercial.sirius-cli-workflow.md:Enter the following command to update the overcloud:
./1.1commercial.sirius-cli-workflow.md:    # bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud
./1.1commercial.sirius-cli-workflow.md:When the update has completed, the Cinder service in the overcloud will be configured to have the StoreVirtual clusters and StoreServ CPG as backends.
./1.1commercial.sirius-cli.md:### Update StoreVirtual cluster ###
./1.1commercial.sirius-cli.md:You can update the StoreVirtual cluster in Sirius database. Only the CLUSTER_ID is  a mandatory argument.
./1.1commercial.sirius-cli.md:	# sirius update-storevirtual-cluster <CLUSTER_ID>  -name=<VCENTER_NAME> --ip-address=<VCENTER_IP_ADDRESS> --username=<VCENTER_USERNAME> --password=<VCENTER_PASSWORD> --port=<VCENTER_PORT> --status=<CLOUD_STATUS>
./1.1commercial.sirius-cli.md:### Update StoreServ ###
./1.1commercial.sirius-cli.md:You can update the StoreServ details in Sirius.
./1.1commercial.sirius-cli.md:	# sirius update-storeserv <STORESERV_ID>
./1.1commercial.site-index.md:* [Updating HP Helion OpenStack](#update)
./1.1commercial.site-index.md:- [Updates and Extensions](/helion/openstack/1.1/undercloud/admin/updates-and-extension/)
./1.1commercial.technical-overview.ga.md:* [Updates and extensions](#updates-and-extensions)
./1.1commercial.technical-overview.ga.md:	<td>Administrators can download content such as software patches and updates from the HP Helion [HDN](https://helion.hpwsportal.com) and apply the downloaded content to their Helion OpenStack installation. </td>
./1.1commercial.technical-overview.ga.md:	<td>The Seed cloud is deployed as a VM instance. This image contains minimal OpenStack services required to deploy and update the undercloud on a baremetal server.
./1.1commercial.technical-overview.ga.md:	 <td>A single-server deployment of a limited set of OpenStack services, called the undercloud, is used to deploy, test, manage, and update all the overcloud servers. 
./1.1commercial.technical-overview.ga.md:## Updates and extensions {#updates-and-extensions}
./1.1commercial.technical-overview.ga.md:Updates and extensions provides a mechanism to download and install the content and updates for the overcloud.
./1.1commercial.technical-overview.ga.md:[Updates and extension](/helion/openstack/1.1/undercloud/admin/updates-and-extension/).
./1.1commercial.troubleshooting.controllers.md:- [Heat Stack Updates Take 30+ Minutes](#foreverupdate)
./1.1commercial.troubleshooting.controllers.md:## Heat Stack Updates Take 30+ Minutes {#foreverupdate}
./1.1commercial.troubleshooting.controllers.md:When [Replacing a Failed Overcloud Controller](/helion/openstack/1.1/removing/failedovercloud/), a Heat stack-update appears to be taking a long time (30 minutes or longer).
./1.1commercial.troubleshooting.ephemeral_partitions.md:If during an update, you receive the following error message:
./1.1commercial.troubleshooting.ephemeral_partitions.md: Once restarted, re-run the overcloud ansible playbook and not the `update_oc.sh` script
./1.1commercial.troubleshooting.install.md:The following content is the result of lessons learned from troubleshooting actual installations. As customers and HP support personnel report issues, this section will be updated.
./1.1commercial.troubleshooting.install.md:- [Overcloud update failed because of nodes in maintenance state]({#maintmode})
./1.1commercial.troubleshooting.install.md:* [Failure of update on the overcloud](#failure-update-overcloud)
./1.1commercial.troubleshooting.install.md:### Overcloud update failed because of nodes in maintenance state {#maintmode} ###
./1.1commercial.troubleshooting.install.md:Do NOT proceed with your installation/update if any of the undercloud/overcloud nodes are down or in a maintenance state.
./1.1commercial.troubleshooting.install.md:During seed update, it needs to be shut down and then started up with the updated image. If the seed is the default gateway this mean the undercloud can no longer talk to the overcloud (that is, the network is interrupted). This interruption normally lasts about five minutes. This interruption can cause Ironic running on the undercloud to put some overcloud nodes into maintenance state as Ironic can no longer contact them
./1.1commercial.troubleshooting.install.md:1. Check if any nodes are in maintenance before running the seed update.
./1.1commercial.troubleshooting.install.md:1. Run the seed update.
./1.1commercial.troubleshooting.install.md:1. After the seed update completes, check if any nodes are in maintenance state because of the seed update.
./1.1commercial.troubleshooting.install.md:2. Proceed with your undercloud and overcloud update.
./1.1commercial.troubleshooting.install.md:### Failure of update on the overcloud {#failure-update-overcloud} ###
./1.1commercial.troubleshooting.install.md:Update overcloud fails with the following error:
./1.1commercial.troubleshooting.install.md:2. Edit the */root/tripleo/ce_env.json* file and update the variables **build&#95;number** and **installed&#95;build&#95;number** to the correct value. <!-- (CORE-1697) --> They may or may not match but the value cannot be NULL.
./1.1commercial.troubleshooting.install.md:4. Run the installer script again to update the overcloud. During the installation, the build specified by build&#95;number is installed.
./1.1commercial.troubleshooting.install.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.1commercial.troubleshooting.install.md:If during an update, you receive the following error message:
./1.1commercial.troubleshooting.install.md: Once restarted, re-run the overcloud ansible playbook and not the `update_oc.sh` script
./1.1commercial.troubleshooting.install.md:     -e 'update nodes set reservation=NULL where reservation is not null;'
./1.1commercial.troubleshooting.ntp.md:		hwclock [--utc | --localtime] -s --hctosys (update system time based on hardware clock)
./1.1commercial.troubleshooting.ntp.md:		hwclock [--utc | --localtime] -w --systohc (update hardware clock based on system time)
./1.1commercial.troubleshooting.ntp.md:		hwclock [--utc | --localtime] --systz (update system time based on timezone)
./1.1commercial.troubleshooting.ovsvapp.md:	Even if pyvmomi is already installed, run the command again to update the library to get major fixes.
./1.1commercial.troubleshooting.swift.md:This issue can happen during the scale-out of the overcloud nodes. The update will fail for one or more nodes. <!-- CORE-2082 -->
./1.1commercial.troubleshooting.swift.md:If the update fails, from undercloud node:
./1.1commercial.troubleshooting.swift.md:		`ironic node-update <id> replace maintenance=False`
./1.1commercial.troubleshooting.vsa.md:* [Unable to update the default input json file ](#unable-update-json)
./1.1commercial.troubleshooting.vsa.md:* [Overcloud updates fail because VSA VMs not responding to virsh stop](#virshfail)
./1.1commercial.troubleshooting.vsa.md:* [Public interface not set and is ignored during overcloud update](#publicinterfaceset)
./1.1commercial.troubleshooting.vsa.md:### Unable to update the default input json file {#unable-update-json}
./1.1commercial.troubleshooting.vsa.md:Parsing the default JSON file failed. Unable to update the default input json file.
./1.1commercial.troubleshooting.vsa.md:The script will parse the configuration file and update the values based on the network and configuration files.
./1.1commercial.troubleshooting.vsa.md:* On success, the script updates the `/mnt/state/vsa/vsa_config.json` file with the updated and created time.
./1.1commercial.troubleshooting.vsa.md:### Overcloud updates fail because VSA VMs not responding to virsh stop {#virshfail}
./1.1commercial.troubleshooting.vsa.md:Overcloud updates require that all VMs be stopped before an update can proceed. During an overcloud update, you may find that you are not able to stop a VSA VM (by issuing a `virsh stop` command). This causes the update to stop.   
./1.1commercial.troubleshooting.vsa.md:Proceed with the update.
./1.1commercial.troubleshooting.vsa.md:### Public interface not set and is ignored during overcloud update {#publicinterfaceset}
./1.1commercial.troubleshooting.vsa.md:As of Helion OpenStack 1.1,  when you install Helion OpenStack, if you do not specify the public interface for VSA (with the `VSA_PUBLIC_INTERFACE` parameter), then updates will not take this parameter into account.
./1.1commercial.troubleshooting.vsa.md:To fix this problem, manually update `VSA_PUBLIC_INTERFACE` in the `~/tripleo/overcloud-env.json` file with the interface name, for example, `eth1`.
./1.1commercial.undercloud-admin-updates-and-extensions.md:title: "HP Helion OpenStack&#174; 1.1: Updates and Extensions"
./1.1commercial.undercloud-admin-updates-and-extensions.md:permalink: /helion/openstack/1.1/undercloud/admin/updates-and-extension/
./1.1commercial.undercloud-admin-updates-and-extensions.md:# HP Helion OpenStack&#174; 1.1: Updates and Extensions
./1.1commercial.undercloud-admin-updates-and-extensions.md:[See the Helion OpenStack 1.0 version of this page](/helion/openstack/undercloud/admin/updates-and-extension/)
./1.1commercial.undercloud-admin-updates-and-extensions.md:HP provides a simple mechanism for downloading and publishing the content and updates for the Undercloud node.
./1.1commercial.undercloud-admin-updates-and-extensions.md:You cannot access the updates from the HP Helion Horizon undercloud dashboard unless you are a registered user.
./1.1commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.undercloud-admin-updates-and-extensions.md:6.	Click **OK** to save the details.<br />The Updates and Extensions page is displayed with the list of available patches.<br />
./1.1commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.undercloud-admin-updates-and-extensions.md:5.	Click **Import**.<br />The file is imported to your local cloud and displayed in the Updates and Extensions page.
./1.1commercial.undercloud-admin-updates-and-extensions.md:2.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./1.1commercial.undercloud-admin-updates-and-extensions.md:Click the **More** drop-down list and select **Download** next to the package that you want to download on your local system. A Confirm Download Update dialog box is displayed.
./1.1commercial.undercloud-admin-updates-and-extensions.md:7. (Optional) To view the download status, click the **More** drop-down list and select **View Progress**. <br />The View Progress box is displayed, containing the log, updated name, updated version, task type, and the status of the task. 
./1.1commercial.undercloud-eon-cli.md:### Update vCenter<a name="update-vcenter"></a>
./1.1commercial.undercloud-eon-cli.md:You can update the exisitng vCenter to the EON database. You must enter all the arguments to add the vCenter; otherwise you will be prompted to enter them. 
./1.1commercial.undercloud-eon-cli.md:	# eon vcenter-update [--name <VCENTER_NAME>] [--ip-address <VCENTER_IP>][--username <VCENTER_USERNAME>][--password <VCENTER_PASSWORD>][--port <VCENTER_PORT>]                       <VCENTER_ID>
./1.1commercial.undercloud-eon-cli.md:###Update a cluster<a name="cluster-update"></a>
./1.1commercial.undercloud-eon-cli.md:You can update the cluster details. 
./1.1commercial.undercloud-eon-cli.md:	# eon cluster-update --vcenter-id <VCENTER_ID> --cluster-moid <CLUSTER_MOID> --state <STATE>
./1.1commercial.undercloud-oc-config-storeserv.md:* [Update Overcloud](#update-overcloud) 
./1.1commercial.undercloud-oc-config-storeserv.md:**Note**: Ensure that you allocate only those CPGs that will be used by this cloud. Changing any attributes of the CPG after allocation, may disrupt cloud functionality if the corresponding change is not updated in Sirius.
./1.1commercial.undercloud-oc-config-storeserv.md:8. Click **Update**.<br />On successful update, the number of CPGs mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.1commercial.undercloud-oc-config-storeserv.md:6. Click **Update**.
./1.1commercial.undercloud-oc-config-storeserv.md:	<br />On successful update, the number of CPGs mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.1commercial.undercloud-oc-config-storeserv.md:5. Click **OK** to download and save the file. <br />Once you download the configuration file, you can proceed to update the overcloud configuration.<br />
./1.1commercial.undercloud-oc-config-storeserv.md:### Update Overcloud {#update-overcloud}
./1.1commercial.undercloud-oc-config-storeserv.md:To update your overcloud with the changes, do the following:
./1.1commercial.undercloud-oc-config-storeserv.md: 	# cp /root/tripleo/tripleo-incubator/scripts/ee-config.json /root/overcloud-config.json	4. Edit and update the /root/overcloud-config.json and add the JSON snippet(obtained from [Generate Config](#generate-config)). Ensure the JSON file format is unbroken. A sample of the file is given below:-->
./1.1commercial.undercloud-oc-config-storeserv.md:2. Edit and update the `tripleo/configs/kvm-custom-ips.json` and add the JSON snippet(obtained from [Generate Config](#generate-config)). Ensure the JSON file format is unbroken. A sample of the file is given below:
./1.1commercial.undercloud-oc-config-storeserv.md:5.Launch install script to update the overcloud.
./1.1commercial.undercloud-oc-config-storeserv.md:		# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update-bv1.log
./1.1commercial.undercloud-oc-config-storevirtual.md:* [Update Overcloud](#update-overcloud) (required)
./1.1commercial.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-expand-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option(opens in a new window)</a>
./1.1commercial.undercloud-oc-config-storevirtual.md:8. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.1commercial.undercloud-oc-config-storevirtual.md:	<!---<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-add-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option (opens in a new window)</a>-->
./1.1commercial.undercloud-oc-config-storevirtual.md:6. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./1.1commercial.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-shrink-backend1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Shrink Backend Page with Update Option (opens in a new window)</a>
./1.1commercial.undercloud-oc-config-storevirtual.md:4. Click **OK** to download and save the file.<br />Once you download the configuration file, you can proceed to update the overcloud configuration.
./1.1commercial.undercloud-oc-config-storevirtual.md:### Update Overcloud {#update-overcloud}
./1.1commercial.undercloud-oc-config-storevirtual.md:To update your overcloud with the changes, do the following:
./1.1commercial.undercloud-oc-config-storevirtual.md:	4. Edit and update the `/root/overcloud-config.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:
./1.1commercial.undercloud-oc-config-storevirtual.md:	2. Edit and update the `tripleo/configs/kvm-custom-ips.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:--> 
./1.1commercial.undercloud-oc-config-storevirtual.md:5. Launch install script to update the overcloud.
./1.1commercial.undercloud-oc-config-storevirtual.md:		# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update-bv1.log
./1.1commercial.undercloud-resource-esx-manage-vm.md:   * Click **Done**. A message is displayed when the update is successful.   
./1.1commercial.undercloud-storage-storeserv.md:Once you register the 3PAR device, choose the CPGs for your cloud. CPG (Common Provisioning Group) is a fundamental unit that can be configured as a Cinder backend. A single HP 3PAR StoreServ may have multiple CPGs. You can choose and allocate them to the cloud as a per your requirement. <!---First register the CPG in Sirius and then configure the CPG as backend for overcloud Cinder service. You can register a few CPGs and allocate them as and when required. The Cinder configuration file is updated only during the allocation, the registeration of the CPG just updates the Sirius database.-->
./1.1commercial.undercloud-storage-storeserv.md:5. Click **Edit CPG** against the CPG that you want to edit.<br /> The Update CPG Details dialog box is displayed. 
./1.1commercial.undercloud-storage-storevirtual.md:**Note**: Ensure that you edit the StoreVirtual cluster only if there are any updates made through the CMC for the selected cluster. After editing the details, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./1.1commercial.undercloud-storage-storevirtual.md:**Note**: When you unregister a cluster,the volumes from this cluster backend will no longer be available through Cinder. Ensure that you detach all relevant volumes and remove the backend associated with the cluster before unregistering. After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information. 
./1.1commercial.undercloud-storage-storevirtual.md:**Note**: After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./1.1commercial.using.md:[Updates and Extensions](/helion/openstack/1.1/undercloud/admin/updates-and-extension/)
./1.1commercial.using.md:<br />Downloading and publishing content and updates.
./1.1commercial.using.md:<br />Add, expand, shrink, delete StoreServ backends, generate Cinder configuration, and update the overcloud.
./1.1commercial.using.md:<br />Add, expand, shrink, delete StoreServ backends, generate Cinder configuration, and update the overcloud.
./1.1commercial.vlan-providernetwork.md:4. To update ml2.conf, edit `tripleo/hp_passthrough/overcloud_neutron_ml2_conf.json` and add tenant&#095;network&#095;type and network&#095;vlan_ranges specific to your environment. An example is given below:
./1.1commercial.whatsnew.md:**OpenStack Juno support** <br />All of the [HP Helion OpenStack services](/helion/openstack/1.1/services/overview/#OpenStack) have been updated to [OpenStack Juno](http://www.openstack.org/software/juno/).
./1.1commercial.whatsnew.md:**Updated Command Line Client** <br />
./1.1commercial.whatsnew.md:The [ALS command line client](/helion/devplatform/1.1/als/client/reference/) (cfmgmt.exe) has been updated to provide more options.
./1.1commerical.flexible-control-pane-installation.md:	b. Update the file called '/root/baremetal.csv' in the following format:
./1.1commerical.flexible-control-pane-installation.md:###A few VMs with two interfaces (pvt and svc) lost network plumbing on compute node (post update)
./1.1commerical.high-availability-restore.md:	The controller with the highest sequence number has the most updated database and should be used to bootstrap the other controllers.
./1.1commerical.high-availability-restore.md:5. Execute the command `cat /mnt/state/var/lib/mysql/grastate.dat` in all the controller to find who has the most updated database (seqno) - Still need to add information to check if the database is corrupted or not.
./1.1commerical.high-availability.md:**Note:** Following a system update, the volume manager may attempt to start before the storage nodes have been properly initialized, and so the volume manager must be restarted after the automatic start procedure has completed.
./1.1commerical.services-eve-overview.md:About "jobs" : A job is created when the user submits a desired topology to Eve and requests provisioning based on it. Eve creates the job and publishes status updates for it. Once the provisioning is finished, the job is marked as completed. 
./1.1commerical.services-focus-overview.md:- Update the contents and/or metadata of a document
./1.1commerical.services-identity-integrate-ldap.md:	<td>group_allow_update</td><td>(BoolOpt) Allow group update in LDAP backend.</td><td>False</td><td>False</td>
./1.1commerical.services-identity-integrate-ldap.md:	<td>user_allow_update</td><td>(BoolOpt) Allow user updates in LDAP backend.</td><td>False</td><td>False</td>
./1.1commerical.services-identity-integrate-ldap.md:	user_allow_update = False
./1.1commerical.services-identity-integrate-ldap.md:	group_allow_update = False
./1.1commerical.services-identity-ldap-JSON.md:		                                "option": "user_allow_update",
./1.1commerical.services-identity-ldap-JSON.md:		                                "option": "group_allow_update",
./1.1commerical.services-identity-ldap-JSON.md:		                                "option": "user_allow_update",
./1.1commerical.services-identity-ldap-JSON.md:		                                "option": "group_allow_update",
./1.1commerical.services-identity-using.md:* Update information for a specified domain, including the description, enabled status, ID, and policy links. 
./1.1commerical.services-identity-using.md:* Update a specified project, including the description, enabled status, ID, and policy links. 
./1.1commerical.services-identity-using.md:* Update a specified user, including the description, enabled status, ID, and policy links. 
./1.1commerical.services-identity-using.md:* Update a specified group, including the description, enabled status, ID, and policy links. 
./1.1commerical.services-identity-using.md:* Update a specified credential, including the description, enabled status, ID, and policy links. 
./1.1commerical.services-identity-using.md:* Update a specified policy, including names, IDs, and the user that owns the policy. 
./1.1commerical.services-orchestration-overview.md:Because you can modify Heat templates, any updates to `openstack/tripleo-heat-templates` by design do not automatically overwrite existing templates. If you are using a modified heat template, then update the new templates provided with the installer to reflect your modifications in your previous deployment. 
./1.1commerical.services-orchestration-overview.md:If you have not made any changes to the original Heat template, then you can safely execute `./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-undercloud` and `./tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud` to update the Heat templates.
./1.1commerical.services-orchestration-overview.md:- **Update a stack** &#151; Configure stacks.
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:	Monitor status (it might take a few minutes for the status to change to `UPDATE_COMPLETE`).
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:7. Monitor the status (it might take a few minutes for the status to change to `UPDATE_COMPLETE`).
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:3. Run the installer script, passing in the update-overcloud parameter. 
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:12. Monitor the status (it might take a few minutes for the status to become `UPDATE_COMPLETE`).
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		hp_ced_install.sh --update-overcloud --skip-demo
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:12. Monitor status (it might take a few minutes for the update to complete).
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:		tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./1.1commerical.services-remove-replace-failed-overcloud-nodes.md:* If a Heat stack-update appears to be taking a long time (30 minutes or longer), see [HP Helion OpenStack&#174; Troubleshooting Controller Nodes](/helion/openstack/1.1/services/troubleshooting/controller/)
./1.1commerical.services-sherpa-accessing-ui.md:2.	Navigate to **Admin -> Updates and Extensions -> Updates and Extensions**. 
./1.1commerical.services-sherpa-overview.md:Content may include patches, plugins, images, models, and so forth that may be used to update, improve, extend, or provide content for the cloud.
./1.1commerical.services-sherpa-overview.md:You can use the [HP Helion OpenStack Dashboard](/helion/openstack/1.1/undercloud/admin/updates-and-extension/) to work with the Sherpa service.
./1.1commerical.services-sherpa-overview.md:The Dashboard presents different options depending on the installation, location within the cloud, and the role of the Sherpa user currently logged in. Cloud Architects or Administrators, for example, can use the dashboard to update their clouds.
./1.1commerical.services-swift-deployment-add-proxy-node.md:4. [Update load balancer with new proxy nodes](#update-load-balncer)
./1.1commerical.services-swift-deployment-add-proxy-node.md:##Update load balancer with new proxy nodes {#update-load-balncer}
./1.1commerical.services-swift-deployment-monitor-health-swift-services.md:* object-updater
./1.1commerical.services-swift-deployment-monitor-health-swift-services.md:* container-updater
./1.1commerical.services-swift-deployment-provision-swift-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commerical.services-swift-deployment-provision-swift-node.md:4.Run the installer script to update the cloud.
./1.1commerical.services-swift-deployment-provision-swift-node.md:	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-overcloud |& tee update_cloud.log
./1.1commerical.services-swift-deployment-provision-swift-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commerical.services-swift-deployment-shrink-remove-proxy-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commerical.services-swift-deployment-shrink-remove-proxy-node.md:When you update the cloud the node will be skipped in all the operations.
./1.1commerical.services-swift-deployment-shrink-remove-proxy-node.md:2.	On the seed VM, update the `/root/tripleo/configs/kvm-custom-ips.json` file to reflect new scale number of swift scale-out proxy node. 
./1.1commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./1.1commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:When you update the cloud the node will be skipped in all the operations.
./1.1commerical.services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:2. On seed VM, update the `kvm-custom-ips.json` file to reflect number of scale-out object nodes remaining.
./1.1commerical.services-swift-deployment.md:8. [Update the Storage Policy](#update-storage-scaleout-swift) 
./1.1commerical.services-swift-deployment.md:	<td>This should be set to however long a full replication/update cycle takes. No partition is moved twice during the specified amount of time.</td>
./1.1commerical.services-swift-deployment.md:2. Update the `so_swift_storage_scale` parameter in the environment variables file, used during the initial installation, according to your storage needs.	
./1.1commerical.services-swift-deployment.md:	2. Update the `so_swift_storage_scale` parameter in  `/root/configs/kvm-custom-ips.json ` file according to your storage needs.-->
./1.1commerical.services-swift-deployment.md:4. Run the installer script to update the cloud.
./1.1commerical.services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.1commerical.services-swift-deployment.md:##Update the Storage Policy {#update-storage-scaleout-swift}
./1.1commerical.services-swift-deployment.md:4. Run the installer script to update the storage policies across the cloud.
./1.1commerical.services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./1.1commerical.services-tripleo-overview.md:- **undercloud** - The undercloud server is a basic single-node OpenStack installation running on a single physical server used to deploy, test, manage, and update the overcloud servers. There is no HA configuration for the undercloud. It contains a strictly limited sub-set of OpenStack services; just enough to  permit interaction with the overcloud. 
./1.1commerical.services-update.md:title: "HP Helion OpenStack&#174; 1.1: Update"
./1.1commerical.services-update.md:permalink: /helion/openstack/1.1/update/overview/11/
./1.1commerical.services-update.md:# HP Helion OpenStack&#174; 1.1: Update#
./1.1commerical.services-update.md:The HP OpenStack Helion update utility supports updating Helion 1.0 to Helion 1.01. You cannot update from Helion 1.0 or 1.01 to 1.1 at this time. For help with updating from your current release to a newer release, contact Customer Support. 
./1.1commerical.services-volume-overview.md:- **Update the metadata of a volume** &#151; Modify the metadata associated with a volume.
./carrier.install-vsa-overview.md:3a - **Run update cloud script to provision VSA node**
./carrier.install-vsa-overview.md:   * Update the *overcloud.json* file for StoreVirtual deployment and apply the configuration.
./carrier.install-vsa-overview.md:   * Execute the update cloud script.
./carrier.install-vsa-overview.md:5a - **Update `overcloud-config.json` file with cinder configuration**
./carrier.install-vsa-overview.md:   With the advise generated from the above steps, update the overcloud-config.json file in the seed cloud.
./carrier.install-vsa-overview.md:5b - **Run update cloud script to update cinder.conf**
./carrier.install-vsa-overview.md:   * The cinder.conf in the overcloud should be updated after updating the overcloud-config.json file in the seed cloud.
./carrier.install-vsa-overview.md:   * Execute [update cloud script](/helion/openstack/1.1/undercloud/oc/config/storevirtual/) from seed cloud. -->
./CarrierGrade/Admin_Guide/carrier-grade-admin-hlm.md:## Update/Upgrade
./CarrierGrade/Admin_Guide/carrier-grade-admin-hlm.md:* Rolling Update to <Type> Control Plane
./CarrierGrade/Admin_Guide/carrier-grade-admin-hlm.md:* Rolling Update to <Type> Resource Single
./CarrierGrade/Admin_Guide/carrier-grade-admin-hlm.md:* Rolling Update to <Type> Resource Cluster
./CarrierGrade/Admin_Guide/carrier-grade-admin-hlm.md:G.	Apply a software update to a cloud
./CarrierGrade/Admin_Guide/carrier-grade-backup-restore-GA.md:You should create a backup of the overcloud servers when any Update and Extension is download to the system.
./CarrierGrade/Admin_Guide/carrier-grade-backup-restore-GA.md:If the admin user password was changed from the original password created during the installation process, you need to update the password in some files before performing the undercloud backup or restore process. If this process has been done and the files contain the correct password, you do not need to edit the files.
./CarrierGrade/Admin_Guide/carrier-grade-backup-restore-GA.md:4. Update the `UNDERCLOUD_ADMIN_PASSWORD=` line with the new password and save the file.
./CarrierGrade/Admin_Guide/carrier-grade-backup-restore-GA.md:6. Update the `undercloud` line with the new password and save the file.
./CarrierGrade/Admin_Guide/carrier-grade-backup-restore-GA.md:9. Update the `OS_PASSWORD=` line with the new password and save the file.
./CarrierGrade/Admin_Guide/carrier-grade-install-GA-security.md:5. Update required access control lists or firewall rules to allow traffic to the Public VIP IP.
./CarrierGrade/Admin_Guide/carrier-grade-install-GA-security.md:<td>4</td><td>Container update over HTTP</td><td>Object Storage</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002</td></tr>
./CarrierGrade/Admin_Guide/carrier-grade-install-GA-security.md:<td>5</td><td>Container update over HTTP</td><td>Object Storage</td><td>Swift all in one (PACO)</td><td>6001,6002
./CarrierGrade/Admin_Guide/carrier-grade-install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Proxy-Account-Container (PAC)</td><td>Swift all in one (PACO)</td><td>6001,6002</td></tr>
./CarrierGrade/Admin_Guide/carrier-grade-install-GA-security.md:<tr><td>6</td><td>Container update over HTTP</td><td>Swift all in one (PACO)</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002
./CarrierGrade/Admin_Guide/carrier-grade-networking-maskedIP.md:	    | updated                              | 2014-06-26T01:41:05Z          |
./CarrierGrade/Admin_Guide/carrier-grade-neutron-post-installation-configure-DNaaS.md:3. Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./CarrierGrade/Admin_Guide/carrier-grade-neutron-post-installation-configure-DNaaS.md:	b.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./CarrierGrade/Admin_Guide/carrier-grade-services-identity-integrate-ldap.md:	<td>group_allow_update</td><td>(BoolOpt) Allow group update in LDAP backend.</td><td>False</td><td>False</td>
./CarrierGrade/Admin_Guide/carrier-grade-services-identity-integrate-ldap.md:	<td>user_allow_update</td><td>(BoolOpt) Allow user updates in LDAP backend.</td><td>False</td><td>False</td>
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:	Monitor status (it might take a few minutes for the status to change to `UPDATE_COMPLETE`).
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:7. Monitor the status (it might take a few minutes for the status to change to `UPDATE_COMPLETE`).
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:3. Run the installer script, passing in the update-overcloud parameter. 
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:12. Monitor the status (it might take a few minutes for the status to become `UPDATE_COMPLETE`).
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		hp_ced_install.sh --update-overcloud --skip-demo
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		prep-for-trickle -z trickle/overcloud-ce-controller stack-update \
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:12. Monitor status (it might take a few minutes for the update to complete).
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:		tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud --skip-demo
./CarrierGrade/Admin_Guide/carrier-grade-services-remove-replace-failed-overcloud-nodes.md:* If a Heat stack-update appears to be taking a long time (30 minutes or longer), see [HP Helion OpenStack&#174; Troubleshooting Controller Nodes](/helion/openstack/carrier/services/troubleshooting/controller/)
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-add-proxy-node.md:##Update load balancer with new Proxy nodes
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-monitor-health-swift-services.md:* object-updater
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-monitor-health-swift-services.md:* container-updater
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-provision-swift-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-provision-swift-node.md:4.Run the installer script to update the cloud.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-provision-swift-node.md:	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh -??-update-overcloud |& tee update_cloud.log
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-proxy-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-proxy-node.md:When you update the cloud the node will be skipped in all the operations.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-proxy-node.md:2.	On the seed VM, update the `/root/tripleo/configs/kvm-custom-ips.json` file to reflect new scale number of swift scale-out proxy node. 
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:When you update the cloud the node will be skipped in all the operations.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment-shrink-remove-scale-out-object-storage-node.md:2. On seed VM, update the `kvm-custom-ips.json` file to reflect number of scale-out object nodes remaining.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:8. [Update the Storage Policy](#update-storage-scaleout-swift) 
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:	<td>This should be set to however long a full replication/update cycle takes. No partition is moved twice during the specified amount of time.</td>
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:2. Update the `so_swift_storage_scale` parameter in the environment variables file, used during the initial installation, according to your storage needs.	
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:	2. Update the `so_swift_storage_scale` parameter in  `/root/configs/kvm-custom-ips.json ` file according to your storage needs.-->
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:4. Run the installer script to update the cloud.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:##Update the Storage Policy {#update-storage-scaleout-swift}
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:4. Run the installer script to update the storage policies across the cloud.
./CarrierGrade/Admin_Guide/carrier-grade-services-swift-deployment.md:    	# bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update_cloud.log
./CarrierGrade/Admin_Guide/carrier-grade-services-volume-overview.md:- **Update the metadata of a volume** &#151; Modify the metadata associated with a volume.
./CarrierGrade/Admin_Guide/carrier-grade-troubleshooting.swift.md:This issue can happen during the scale-out of the overcloud nodes. The update will fail for one or more nodes. <!-- CORE-2082 -->
./CarrierGrade/Admin_Guide/carrier-grade-troubleshooting.swift.md:If the update fails, from undercloud node:
./CarrierGrade/Admin_Guide/carrier-grade-troubleshooting.swift.md:		`ironic node-update <id> replace maintenance=False`
./CarrierGrade/Admin_Guide/DCN/carrier-grade-install-bm-dcn-cloud.md:5. Update cloud template json files:
./CarrierGrade/Admin_Guide/VSA/carrier.install-GA-enroll.old.md:		bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud 2>&1 | tee update.log
./CarrierGrade/Admin_Guide/VSA/carrier.install-GA-vsa.md:	sudo apt-get update
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:* [Unable to update the default input json file ](#unable-update-json)
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:* [Overcloud updates fail because VSA VMs not responding to virsh stop](#virshfail)
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:* [Public interface not set and is ignored during overcloud update](#publicinterfaceset)
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:### Unable to update the default input json file {#unable-update-json}
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:Parsing the default JSON file failed. Unable to update the default input json file.
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:The script will parse the configuration file and update the values based on the network and configuration files.
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:* On success, the script updates the `/mnt/state/vsa/vsa_config.json` file with the updated and created time.
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:### Overcloud updates fail because VSA VMs not responding to virsh stop {#virshfail}
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:Overcloud updates require that all VMs be stopped before an update can proceed. During an overcloud update, you may find that you are not able to stop a VSA VM (by issuing a `virsh stop` command). This causes the update to stop.   
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:Proceed with the update.
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:### Public interface not set and is ignored during overcloud update {#publicinterfaceset}
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:As of Helion OpenStack Carrier Grade (Alpha):,  when you install Helion OpenStack, if you do not specify the public interface for VSA (with the `VSA_PUBLIC_INTERFACE` parameter), then updates will not take this parameter into account.
./CarrierGrade/Admin_Guide/VSA/carrier.troubleshooting.vsa.md:To fix this problem, manually update `VSA_PUBLIC_INTERFACE` in the `~/tripleo/overcloud-env.json` file with the interface name, for example, `eth1`.
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:* [Update Overcloud](#update-overcloud) (required)
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-expand-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option(opens in a new window)</a>
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:8. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:	<!---<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-add-backendoption1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Expand Backend Page with Update Option (opens in a new window)</a>-->
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:6. Click **Update**.<br />On successful update, the number of clusters mapped to the backend is updated and displays in the Backend Mapping table in the Configure Cloud page.<br />
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:	<a href="javascript:window.open('/content/documentation/media/undercloud-storevirtual-shrink-backend1.png','_blank','toolbar=no,menubar=no,resizable=yes,scrollbars=yes')">Shrink Backend Page with Update Option (opens in a new window)</a>
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:4. Click **OK** to download and save the file.<br />Once you download the configuration file, you can proceed to update the overcloud configuration.
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:### Update Overcloud {#update-overcloud}
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:To update your overcloud with the changes, do the following:
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:	4. Edit and update the `/root/overcloud-config.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:	2. Edit and update the `tripleo/configs/kvm-custom-ips.json` and add the JSON snippet obtained from [generating the configuration file](#generate-config).Ensure the JSON file format is unbroken. A sample of the file is given below:--> 
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:5. Launch install script to update the overcloud.
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-oc-config-storevirtual.md:		# bash -x /root/tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |& tee update-bv1.log
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-storage-storevirtual.md:**Note**: Ensure that you edit the StoreVirtual cluster only if there are any updates made through the CMC for the selected cluster. After editing the details, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-storage-storevirtual.md:**Note**: When you unregister a cluster,the volumes from this cluster backend will no longer be available through Cinder. Ensure that you detach all relevant volumes and remove the backend associated with the cluster before unregistering. After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information. 
./CarrierGrade/Admin_Guide/VSA/carrier.undercloud-storage-storevirtual.md:**Note**: After unregistering the cluster, the backend data should also be updated so that the Cinder configuration file has the updated cluster information.
./CarrierGrade/carrier-grade-install-bm-environment.md:		./hlm_updatepackages.sh
./CarrierGrade/carrier-grade-install-bm-hlm-cloud.md:5. Update cloud template json files:
./CarrierGrade/carrier-grade-install-pb-network-prepare.md:2. Update `/root/infra-ansib1le-playbooks/group_vars/all` file to specify the location of your `libvirt` images folder. 
./CarrierGrade/carrier-grade-install-playbook-prereqs.md:- Update to the latest firmware recommended by the system vendor for all system components, including the BIOS, BMC firmware, disk controller firmware, drive firmware, network adapter firmware, and so forth.
./CarrierGrade/carrier-grade-install-playbook-prereqs.md:	a. Update the `MaxSessions=1` to `MaxSessions=50`.
./CarrierGrade/carrier-grade-install-prereqs.md:- Update to the latest firmware recommended by the system vendor for all system components, including the BIOS, BMC firmware, disk controller firmware, drive firmware, network adapter firmware, and so forth.
./CarrierGrade/carrier-grade-install-prereqs.md:1. Update the startwm.sh file to point to the `xsession` file. The xsession program is a session manager.
./CarrierGrade/InstallVirtual/carrier-grade-install-guest-vm.md:5. Update `/etc/resolv.conf` file to add IP address of the DNS server.
./CarrierGrade/InstallVirtual/carrier-grade-install-guest-vm.md:1. Update the startwm.sh file to point to the `xsession` file. The xsession program is a session manager.
./CarrierGrade/InstallVirtual/carrier-grade-install-guest-vm.md:3. Update your sources:
./CarrierGrade/InstallVirtual/carrier-grade-install-guest-vm.md:		sudo apt-get update
./CarrierGrade/InstallVirtual/carrier-grade-install-helion.md:		./hlm_updatepackages.sh
./CarrierGrade/InstallVirtual/carrier-grade-install-helion.md:4. Update cloud template JSON files:
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm-node.md:3. Update your interfaces as below and restart network services using the following command:
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm.md:	Update the configuration files to work on the selected network.
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm.md:6. Update the `/etc/network/interfaces` file with the HLM Node interfaces specified in the configuration file.
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm.md:7. Update `/etc/resov.conf` file with the HLM Node Gateway interface
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm.md:9. Update `/etc/cobbler/settings` file with the HLM Node IP Address.
./CarrierGrade/InstallVirtual/carrier-grade-install-hlm.md:10. Update `/etc/cobbler/dhcp.template` file:
./CarrierGrade/InstallVirtual/carrier-grade-install-vsd-node.md:8. Update `/etc/resolv.conf` file to add IP address of the DNS server.
./CarrierGrade/UseCases/TasksforAdmins/carrier-grade.dashboard.default.quotas.md:4. Click **Update Defaults**.
./CarrierGrade/UseCases/TasksforAdmins/carrier-grade.dashboard.metadata.md:3. For the flavor you want to modify, click the arrow icon in the **Actions** menu and select **Update Metadata**. 
./CarrierGrade/UseCases/TasksforAdmins/carrier-grade.dashboard.metadata.md:4. In the **Update Metadata** screen, click the **+** icon for a metadata term in the **Available Metadata** column to move the term to the **Existing Metadata** column. 
./CarrierGrade/UseCases/TasksforAdmins/Security/carrier-grade.dashboard.users.password.md:4. In the **Update User** screen, enter a new password in the **Password** field and re-enter the password in the **Confirm Password** field to confirm.
./CarrierGrade/UseCases/TasksforAdmins/Security/carrier-grade.dashboard.users.password.md:5. Click **Update User**.
./CarrierGrade/UseCases/TasksforAdmins/Users/1.1commercial.dashboard.projects.primary.md:4. In the **Update User** screen, select a project from the **Primary Project** list.
./CarrierGrade/UseCases/TasksforAdmins/Users/1.1commercial.dashboard.projects.primary.md:5. Click **Update User**.
./CarrierGrade/UseCases/TasksforAdmins/Users/1.1commercial.dashboard.users.password.md:4. In the **Update User** screen, enter a new password in the **Password** field and re-enter the password in the **Confirm Password** field to confirm.
./CarrierGrade/UseCases/TasksforAdmins/Users/1.1commercial.dashboard.users.password.md:5. Click **Update User**.
./CarrierGrade/UseCases/TasksforUsers/1.1commercial.dashboard.users.md:You can create and configure volumes; create snapshots of volumes; update the metadata associated with a volume, extend the size of volumes, and attach and detach volumes from instances.
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.images.modify.md:4. Update the image information:
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.images.modify.md:5, When complete, click **Update Image**. 
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.images.public.md:4. In the **Update Image** screen, select **Public**.
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.images.public.md:5. Click **Update Image**.
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.snapshot.protect.md:4. In the **Update Image** screen, select **Protected**.
./CarrierGrade/UseCases/TasksforUsers/Images/carrier-grade.dashboard.snapshot.protect.md:5. Click **Update Image**.
./CarrierGrade/UseCases/TasksforUsers/Object_Storage/carrier-grade.dashboard.container.edit.md:8. Click **Update Object** to confirm.
./CarrierGrade/UseCases/TasksforUsers/Volumes/carrier-grade.managing.volumes.md:You can create and configure volumes; create snapshots of volumes; update the metadata associated with a volume, extend volumes, and transfer a volume between users, and attach and detach volumes from instances.
./ceilometer/1.1commerical.services-reporting-bestpractice.md:	          - "compute.instance.update"
./ceilometer/1.1commerical.services-reporting-bestpractice.md:	          - "compute.instance.update"
./ceilometer/1.1commerical.services-reporting-bestpractice.md:	          - "compute.instance.update"
./ceilometer/1.1commerical.services-reporting-metertypes.md:<tr><td>network.update</td><td>Delta</td><td>network</td><td>netw ID</td><td>notification</td><td>Update requests for this network</td></tr>
./ceph/1.1commercial-ceph-helion-openstack-nova-ceph-Storage.md:		| updated 					           | 2014-08-12T23:44:23Z 															|
./ceph/1.1commercial-ceph-hp-helion-openstack-ceph-configuration.md:6. Does an `apt-get update` and `apt-get install` of the Ceph client packages once the repository is added to the `apt sources.list` file. If `apt-get update` and `apt-get install` of the Ceph client packages executes without errors,  `dpkg` is executed to verify the installation of the correct packages.
./ceph/1.1commercial.ceph-automated-install.md:* `hp_ced_pre_update_checks.sh`
./ceph/1.1commercial.ceph-automated-install.md:**Note**: To enable login to other nodes, provide 600 permission to the `cephadmin.pem` key. Execute `chmod 600 cephadmin.pem` command to update the permission. 
./ceph/1.1commercial.ceph-automated-install.md:* Update the  `"ws_url"` with the URL of the seed node VM where  the server is running by entering:
./ceph/1.1commercial.ceph-cluster-client-node-configuration-ansible.md:		| updated 					           | 2014-08-12T23:44:23Z 															|
./ceph/1.1commercial.ceph-manual-install.md:13. Update the `ceph.conf` file as shown below:
./ceph/1.1commercial.ceph-manual-install.md:**Note**: Save the following commands to a file and save as `auth.sh`.Update the entries based on your OSDs and run `chmod +x auth.sh` and `./auth.sh`.
./ceph/1.1commercial.ceph-manual-install.md:**Note**: Save the following commands to a file and save as `crush.sh`. Update the hostname after `host=` and run `chmod +x crush.sh` and `./crush.sh`.
./ceph/1.1commercial.ceph-manual-install.md:Update the `ceph.conf` file as shown below for each of the OSD nodes. Also add the [global] section in `ceph.conf` files with file parameter `tunable` as listed in the [tunable](#ceph-tunning).
./ceph/1.1commercial.ceph-manual-install.md:A healthy Ceph cluster needs at least 3 OSD nodes. (Follow steps similar to previous section to add additional nodes to form the quorum.) Update the `ceph.conf` file like that shown below before proceeding.
./ceph/1.1commercial.ceph-manual-install.md:**Note**: Save the following commands to a file and save as `auth.sh`.Update the entries based on your OSDs and run `chmod +x auth.sh` and `./auth.sh`.
./ceph/1.1commercial.ceph-manual-install.md:**Note**: Save the following commands to a file and save as `crush.sh`. Update the hostname after `host=` and run `chmod +x crush.sh` and `./crush.sh`.
./ceph/1.1commercial.ceph-manual-install.md:Update the `ceph.conf` file as shown below for each of the OSD nodes. Also add the [global] section in `ceph.conf` files with file parameter `tunable` as listed in the [tunable](#ceph-tunning).
./ceph/1.1commercial.ceph-manual-install.md:A healthy Ceph cluster needs at least 3 OSD nodes. (Follow steps similar to previous section to add additional nodes to form the quorum.) Update the `ceph.conf` file like that shown below before proceeding.
./ceph/1.1commercial.ceph-manual-install.md:6.	Update `/etc/ceph/ceph.conf` file with the new monitor node details and push the updated `ceph.conf` file to all the nodes in the Ceph cluster.
./ceph/1.1commercial.ceph-manual-install.md:2. Update `/etc/ntp.conf` file with your local NTP server across all the nodes in the Ceph cluster.
./ceph/1.1commercial.ceph-manual-install.md:3. Once `ntp.conf` file is updated, restart ntp service by running `/etc/init.d/ntp restart6`
./ceph/1.1commercial.ceph-rados-gateway-dmz-ha-proxy.md:* Execute `update-ca-certificates`
./LDAP-domain-config.md:	user_allow_update = False
./LDAP-domain-config.md:	group_allow_update = False
./neutron/1.1commercial_neutron-post-installation-configure-DNaaS.md:3. Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./neutron/1.1commercial_neutron-post-installation-configure-DNaaS.md:	b.	Click **Updates and Extensions** and then select **Updates and Extensions** to open the Updates and Extensions page.
./neutron/1.1commercial_neutron-post-installation-configure-network.md:5. Update required access control lists or firewall rules to allow traffic to the Public VIP IP.
./neutron/1.1commercial_neutron-post-installation-configure-network.md:<td>4</td><td>Container update over HTTP</td><td>Object Storage</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002</td></tr>
./neutron/1.1commercial_neutron-post-installation-configure-network.md:<td>5</td><td>Container update over HTTP</td><td>Object Storage</td><td>Swift all in one (PACO)</td><td>6001,6002
./neutron/1.1commercial_neutron-post-installation-configure-network.md:<tr><td>6</td><td>Container update over HTTP</td><td>Proxy-Account-Container (PAC)</td><td>Swift all in one (PACO)</td><td>6001,6002</td></tr>
./neutron/1.1commercial_neutron-post-installation-configure-network.md:<tr><td>6</td><td>Container update over HTTP</td><td>Swift all in one (PACO)</td><td>Proxy-Account-Container (PAC)</td><td>6001,6002
./neutron/1.1commercial_neutron-post-installation-neutron-integration.md:	<td>group_allow_update</td><td>(BoolOpt) Allow group update in LDAP backend.</td><td>False</td><td>False</td>
./neutron/1.1commercial_neutron-post-installation-neutron-integration.md:	<td>user_allow_update</td><td>(BoolOpt) Allow user updates in LDAP backend.</td><td>False</td><td>False</td>
./neutron/1.1commercial_neutron-post-installation-neutron-integration.md:2. Disable CRUD (Create/Update/Delete) operations on users for a read-only identity backend on all three overcloud controller nodes.
./neutron/1.1commercial_neutron-post-installation-neutron-integration.md:	                        "option": "user_allow_update",
./neutron/1.1commercial_neutron-post-installation-neutron-integration.md:	                        "option": "group_allow_update",
./neutron/1.1commercial_neutron-post-installation.md:	    | updated                              | 2014-06-26T01:41:05Z          |
./neutron/1.1commercial_neutron-pre-installation.md:* Configure Gratuitous ARP (GARP). When the tenant network is created, the Neutron L3 agents are responsible for sending GARP requests to detect IP conflicts and assist in refreshing the ARP cache of other nodes. If the external gateway already exists, the L3 agent also will trigger an update on iptables to insert NAT rules on the Network Node. Using this default behavior, Floating IPs are recycled among multiple VMs. Currently there is no OpenStack configuration variable to control the delay or interval between GARP transmits. It is therefore the responsibility of the network administrator to enable GARPs to pass through the gateways and control the GARP delays in the underlying physical network so as not to trigger a timeout.
