---
layout: 1.0default-devplatform
permalink: /als/v1/admin/server/logging/
product: devplatform1.0
title: "HP Helion 1.0 Development Platform: Log Streams "
product-version1: HP Helion Development Platform
product-version2: HP Helion Development Platform 1.0
role1: Application Developer
role2: ISV Developer 
role3: Service Developer
role4: Network Administrator
role5: Systems Administrator 
role6: Security Engineer
authors: Jayme P

---
<!--PUBLISHED-->

# HP Helion 1.0 Development Platform: Log Streams {#log-streams}
[See the Helion 1.1 Development Platform version of this page](/helion/devplatform/1.1/als/admin/server/logging/)

- [Logyard](#logyard)
-   [Drains](#drains)
    -   [System Drains](#system-drains)
    -   [Log Format](#log-format)
    -   [Saving Custom Log Formats](#saving-custom-log-formats)
    -   [Custom Drains](#custom-drains)
    -   [Application Drains](#application-drains)
    -   [Drain Status](#drain-status)
    -   [Keys](#keys)
        -   [apptail](#apptail)
        -   [event](#event)
        -   [systail](#systail)
    -   [Managing the systail stream](#managing-the-systail-stream)
-   [Configuration](#configuration)
    -   [Drain Timeouts](#drain-timeouts)
    -   [User Drain Limit](#user-drain-limit)
    -   [Apptail Limits](#apptail-limits)
-   [Debugging Logyard](#debugging-logyard)

Application and system logs in Application Lifecycle Service are aggregated into streams
which can be viewed, tailed, filtered, and/or sent via drains to other
log aggregators for archiving or analysis. There are three general types
of streams:

-   **Application log streams**: application logs (plus relevant events)
    from all instances
-   **System log streams**: Application Lifecycle Service and other system logs from all
    nodes (dmesg, dea.log, auth.log, etc.)
-   **Cloud event streams**: cloud events from all nodes (see Cloud
    Events in the Management Console)

A **message** is a single log line or event in a stream.

Each message has a **key** which identifies *which* stream it belongs to
(see [*Keys*](#logging-keys) below).

Logyard[](#logyard "Permalink to this headline")
-------------------------------------------------

Log streams are handled by three processes which run on all Application Lifecycle Service
nodes:

-   **logyard**: listens for incoming log messages and forwarding them
    to a configurable list of drains
-   **systail**: sends system logs (/s/log/\*, etc.) to **logyard** to
    be in turn forwarded to drains
-   **logyard\_sieve**: listens for all system logs and extracts vital
    events back to **logyard**

**apptail** is an additional process which runs only on DEA nodes. It
sends user application logs to **logyard**, injecting relevant
application-specific events from the **logyard\_sieve** stream.

Drains[](#drains "Permalink to this headline")
-----------------------------------------------

A "drain" is a receiver for a log stream. Logyard has three kinds:

-   TCP (e.g. <tcp://10.0.11.101:12345>)
-   UDP (e.g. udp://logs.papertrailapp.com:12345)
-   Redis (e.g. redis://192.168.1.157:5000/)
-   file (e.g. <file:///s/logs/custom-drain-1.log>)

### System Drains[](#system-drains "Permalink to this headline")

Drains for system log and cloud event streams can be added by admins
with the [*kato log
drain*](/als/v1/admin/reference/kato-ref/#kato-command-ref-log-drain-add)
command. For example:

    $ kato log drain add --prefix systail.kato mydrain udp://logs.papertrailapp.com:12345

This creates a UDP drain that receives messages from **kato.log** (on
all nodes in the cluster) and forwards them to
[Papertrail](https://papertrailapp.com/) on port 12345.

The `--prefix` flag takes a [*key*](#logging-keys)
prefix as its argument.

To delete the drain:

    $ kato log drain delete mydrain

The [*kato
history*](/als/v1/admin/reference/kato-ref/#kato-command-ref-history) command
uses a built-in drain which forwards to a Redis server on the Primary
node.

The 'file' drain type will append to a local file. To overwrite the file
instead, add the 'overwrite=1' option:

    $ kato log drain add debug file:///s/logs/debug-1.log overwrite=1

### Log Format[](#log-format "Permalink to this headline")

Log drains can emit entries in a variety of formats:

-   verbatim (default): Log entries as they appear in the source log
    files (plain text).
-   json: Log entries wrapped as JSON objects, with keys identifying
    each part of the entry.
-   custom: Values of the specified JSON keys arranged in an arbitrary
    format.

For example, to add a drain with just the timestamp, application name
and message:

    $ kato log drain add -p apptail -f '{{.human_time}} - {{.app_name}}: {{.text}}' \
    > all-apps file:///s/logs/apptail-short.log

JSON keys are enclosed in double curly braces and prefixed with a
period. The spaces, hyphen, and colon here are functioning as
delimiters. The resulting entry might look like this:

    2013-01-22T16:01:14-08:00 - myenv: Application 'myenv' is now running on DEA 27da51

Different JSON keys are available in different [*log
streams*](#logging-keys):

**apptail.**:

-   text: actual log line
-   unix\_time: timestamp (seconds since 1 January 1970)
-   human\_time: formatted time
-   node\_id: DEA host IP of this app instance
-   filename: log file from which this line originated
-   source: e.g. app, staging, helion.dea, helion.stager, appstore
-   instance\_index: instance number
-   app\_guid: GUID of this app
-   app\_name: application name
-   app\_space: GUID of the space this app belongs to
-   syslog.priority: syslog priority
-   syslog.time: syslog formatted time

**event.**:

-   text: event description
-   unix\_time: timestamp
-   human\_time: formatted time
-   node\_id: Node IP from which this event originated
-   type: type of event (eg: process\_stop)
-   severity: INFO, WARN, ERROR
-   process: the process generating the event
-   info: event-specific information as JSON
-   syslog.priority: syslog priority
-   syslog.time: syslog formatted time

**systail.**:

-   text: actual log line
-   unix\_time: timestamp
-   human\_time: formatted time
-   node\_id: Node IP from which this log line originated
-   name: name of the component (eg: redis\_gateway)
-   syslog.priority: syslog priority
-   syslog.time: syslog formatted time

You can see a list of the default drain formats using [*kato config
get*](/als/v1/admin/reference/kato-ref/#kato-command-ref-config):

    $ kato config get logyard drainformats
    apptail: ! '{{.human_time}} {{.source}}.{{.instance_index}}: {{.text}}'
    event: ! '{{.type}}@{{.node_id}}: {{.desc}} -- via {{.process}}'
    systail: ! '{{.name}}@{{.node_id}}: {{.text}}'
    [...]

These default log formats are used when the corresponding prefix is used
and no format options ("-f") are specified. For example
`kato drain add -p systail.dea ...` would format the
drain using the 'systail' drain format.

### Saving Custom Log Formats[](#saving-custom-log-formats "Permalink to this headline")

Custom formats for drains can be saved as a named type in the Logyard
configuration. To do this, add the formatting string to a new key in
logyard/drainformats. For example, to save the log format used in the
'all-apps' drain example above:

    $ kato config set logyard drainformats/simplefmt "{{.human_time}} - {{.app_name}}: {{.text}}"

You can use this named format when setting up new drains. For example, a
shorter command for creating the 'all-apps' drain would be:

    $ kato log drain add -p apptail -f simplefmt all-apps file:///s/logs/apptail-short.log

A custom "systail" log stream might look like this:

    $ kato config set logyard drainformats/systail-papertrail '<13>1 - {{.human_time}} - {{.name}}@{{.node_id}} -- {{.text}}'

This could be forwarded to the Papertrail log analysis service:

    $ kato log drain add papertrail udp://logs.papertrailapp.com:45678 -f systail-papertrail

You can also change the default apptail, event, and systail drain
formats to modify the output of any drains using these prefixes (e.g.
[*helion
drain*](/als/v1/user/reference/client-ref/#command-drain-add), Cloud
Events in the Management Console, and [*kato log
tail*](/als/v1/admin/reference/kato-ref/#kato-command-ref-log-tail)
respectively).

### Custom Drains[](#custom-drains "Permalink to this headline")

You can add custom drains to Logyard to look for certain events or parse
certain log messages (e.g. tracking application push requests or user
logins). Examples of custom drains and more advanced usage of Logyard
can be found in the [Logyard Developer
Guide](https://github.com/ActiveState/logyard-devguide/blob/master/README.md)

### Application Drains[](#application-drains "Permalink to this headline")

Drains for application log streams can be added by end users with the
[*helion log drain
add*](/als/v1/user/reference/client-ref/#command-drain-add) command.
See the [*Application
Logs*](/als/v1/user/deploy/app-logs/#application-logs) section of the
User Guide for an example.

### Drain Status[](#drain-status "Permalink to this headline")

You can check the status of all drains on Application Lifecycle Service with the
`kato log drain status` subcommand. For example:

    $ kato log drain status
    appdrain.1.mine         192.168.68.5    RUNNING[53]
    appdrain.1.mydrain      192.168.68.5    RETRYING[75]  invalid port 3424252
    builtin.apptail         192.168.68.5    RUNNING[3]
    builtin.cloudevents     192.168.68.5    RUNNING[3]
    builtin.katohistory     192.168.68.5    RUNNING[3]

If the RETRYING drain hits a [*drain
timeout*](#logging-drains-timeouts), its status will change to FATAL.

### Keys[](#keys "Permalink to this headline")

Each message in a log stream is prefixed with a key, identifying what
type of message it is or to which log stream it belongs. The following
keys are available for use in defining drains using the
`--prefix` flag for [*kato log drain
add*](/als/v1/admin/reference/kato-ref/#kato-command-ref-log-drain-add)).

Systail keys are [*configurable*](#logging-systail-manage).

#### apptail[](#apptail "Permalink to this headline")

> apptail.\<app.id\>

#### event[](#event "Permalink to this headline")

-   event.\<eventname\>
    -   process\_stop
    -   process\_exit
    -   kato\_action
    -   timeline
    -   nginx\_error
    -   vcap\_error
    -   vcap\_warning
    -   service\_provision

#### systail[](#systail "Permalink to this headline")

-   systail.\<processname\>
-   systail.\<processname\>.\<nodeip\>
    -   auth
    -   dmesg
    -   dpkg
    -   kato
    -   kernel
    -   nginx\_error
    -   supervisord
    -   cc\_nginx\_error
    -   app\_mdns
    -   app\_store
    -   applog\_redis
    -   apptail
    -   avahi\_publisher
    -   cc\_nginx
    -   cloud\_controller\_ng
    -   logyard\_sieve
    -   dea\_ng
    -   dockerd
    -   aok
    -   filesystem\_gateway
    -   filesystem\_node
    -   harbor\_gateway
    -   harbor\_node
    -   harbor\_proxy\_connector
    -   harbor\_redis
    -   health\_manager
    -   logyard
    -   memcached\_gateway
    -   memcached\_node
    -   mongodb\_gateway
    -   mongodb\_node
    -   mysql
    -   mysql\_gateway
    -   mysql\_node
    -   nats\_server
    -   nginx
    -   postgresql
    -   postgresql\_gateway
    -   postgresql\_node
    -   prealloc
    -   rabbit\_gateway
    -   rabbit\_node
    -   redis\_gateway
    -   redis\_node
    -   redis\_server
    -   router
    -   router2g
    -   stager
    -   systail

### Managing the systail stream[](#managing-the-systail-stream "Permalink to this headline")

The list above shows the default systail keys. These can keys can be
modified with the [*kato
config*](/als/v1/admin/reference/kato-ref/#kato-command-ref-config) command to
add arbitrary system log files to the stream or change the log file
source for an existing key.

-   To retrieve the current list of log files being streamed:

        $ kato config get systail log_files

-   To remove a log file from the stream:

        $ kato config del systail log_files/dpkg

-   To add a new log file to the stream:

        $ kato config set systail log_files/dpkg /var/log/dpkg.log

Restart the `systail` process after adding or
removing log files:

    $ kato process restart systail

**Note**

Do not remove the default Application Lifecycle Service log stream keys (i.e. anything in the
[*systail*](#logging-keys-systail) list above) as this would affect the
output of `kato tail`.

Configuration[](#configuration "Permalink to this headline")
-------------------------------------------------------------

Application Lifecycle Service has a number of configurable limits on application log drains
to help prevent performance problems the logging subsystems. These
settings can all be viewed and set with [*kato
config*](/als/v1/admin/reference/kato-ref/#kato-command-ref-config) commands as
described below:

### Drain Timeouts[](#drain-timeouts "Permalink to this headline")

-   **logyard** **retrylimits**: If a drain gets disconnected (e.g. if
    the log aggregation service goes down), Logyard will retry the
    connection at the following intervals:

    -   once every 5 seconds for 1 to 2 minutes
    -   once every 30 seconds for 5 minutes
    -   once every 1 minute for 10 minutes
    -   once every 5 minutes until connect or destroyed

    This ensures that once connectivity is restored, the drains will
    re-establish their connections within (at most) 5 minutes.

    Application drains will retry for one day. Temporary drains (e.g.
    `kato tail`) will retry for 25 minutes. All
    other drains will retry indefinitely.

    These timeouts can be configured. To see a list of the configured
    timeouts, use [*kato config
    get*](/als/v1/admin/reference/kato-ref/#kato-command-ref-config). For
    example:

        $ kato config get logyard retrylimits
        appdrain.: 24h
        tmp.: 25m

    To set a time-out (minimum 21m), use [*kato config
    set*](/als/v1/admin/reference/kato-ref/#kato-command-ref-config). For
    example, to set the timeout limit to 10 hours on all drains named
    with the prefix "papertrail":

        $ kato config set logyard retrylimits/papertrail 10h

    These limits will take effect on new drains, deleted/re-created
    drains, or for all matching drains after
    `kato process restart logyard` has been run on
    all nodes.

### User Drain Limit[](#user-drain-limit "Permalink to this headline")

-   **cloud\_controller\_ng** **max\_drains\_per\_app** (default 2):
    limits the number of drains an application can have. Once this limit
    is reached, users will see the following notification when trying to
    add a new drain:

        Adding drain [fail] ... Error 123: Per-app drain limit (2) reached.

    To change the limit, set `max_drains_per_app` in
    the cloud\_controller\_ng configuration. For example, to change this
    limit to 5 drains:

        $ kato config set cloud_controller_ng max_drains_per_app 5

### Apptail Limits[](#apptail-limits "Permalink to this headline")

-   **apptail** **read\_limit** (default 16MB): defines the maximum
    number of bytes to read from the end of application log files. This
    is done to prevent performance problems during restart of the
    `apptail` process (or nodes running the process)
    if the log file sources have grown extremely large.

    When this limit is reached, a warning such as the following will
    appear in both the Cloud Events stream and the application's log
    stream:

        WARN -- [exampleapp] Skipping much of a large log file (stderr); size (26122040 bytes) > read_limit (15728640 bytes)

    To change the read\_limit to 100MB:

        $ kato config set apptail read_limit 100

-   **apptail** **rate\_limit** (default 400): limits the number of log
    lines per second that can be read from an application log file. The
    `apptail` process reads (at most) the specified
    number of log lines per second, after which it will wait for one
    second before resuming. A line similar to the `read_limit` warning above is inserted in the stream to explain the
    missing data.

    To change the rate\_limit to 300 lines:

        $ kato config set apptail rate_limit 300

Debugging Logyard[](#debugging-logyard "Permalink to this headline")
---------------------------------------------------------------------

Use `kato log stream debug` to monitor
Logyard-related log activity. The command tails the logyard, apptail,
systail, and logyard\_sieve streams.
